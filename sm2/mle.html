<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE0592.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script>
<style>
body {
  margin: 0;
  //background: #000;
  //color: #fff;
}


.sticky {
  position: fixed;
  top: 0;
  width: 100%;
  background: #555;
  color: #f1f1f1;
}

.cu {
  background: #ffcccc;
}

.bu {
  background: #ccccff;
}

.scrpt {
  border-left: 5px solid black;
}
</style>
<script>
window.onscroll = function() {myFunction()};
window.onload = function() {myInit()};

var header, tphldr;
function myInit() {
  header = document.getElementsByClassName("header");
  tphldr = document.getElementById("topholder");
}

function myFunction() {
  var index = -1
  for(i=0;i<header.length;i++) {
    if (window.pageYOffset > header[i].offsetTop) {
       index = i
    }
    else {
       break
    }
  }

  if(index < 0) 
    tphldr.innerHTML = "";
  else
    tphldr.innerHTML = header[index].innerHTML
}
</script><script type="text/javascript" src="https://arnabc74.github.io/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="file:///home/asu/na/v/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head>
<body>
<div class="sticky" id="topholder"> </div>
<a href="http://www.isical.ac.in/~arnabc/">[Home]</a>
<h3>Table of contents</h3>
<ul>
<li>
<a href="#Advanced MLE techniques">Advanced MLE techniques</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Technique 1: Newton-Raphson method">Technique 1: Newton-Raphson method</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Technique 2: Fisher`s scoring method">Technique 2: Fisher`s scoring method</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Advantages of Fisher`s scoring method over
Newton-Raphson">Advantages of Fisher`s scoring method over
Newton-Raphson</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#More on Fisher information">More on Fisher information</a>
</li>
</ul>
<hr/>
$
\renewcommand{\v}{\vec}
\newcommand{\do}[1]{\frac{\partial}{\partial #1}}
\newcommand{\dt}[1]{\frac{\partial^2}{\partial #1^2}}
\newcommand{\doo}[2]{\frac{\partial^2}{\partial #1\partial #2}}
$
<h1><a
name="Advanced MLE techniques">Advanced MLE techniques</a></h1>
Our textbook covers the basic idea behind the maximum likelihood
estimation technique. In some simple cases, you can find the
maximum just by inspection, while in others you have to use
differentiation. In most cases direct differentiation of the
likelihood function leads to unwieldy expressions (as the
likelihood is often the product of $n$ functions, and the
"differentiate-one-at-a-time" rule for differentiating a product is
not fun when $n$ is large). So a common technique is to
first take logarithm of the likelihood (converting the product to
a sum) and then to differentiate the log-likelihood. As log is a
strictly increasing differentiable function, the argmax does not
change upon taking log. So if $\ell(\v \theta)$ is the
log-likelihood, then we want to solve the equation:
$$
\nabla \ell(\v \theta) = \v0.
$$ 
By the way, $\nabla \ell(\cdot)$ is called the <b>score function</b>.
<p></p>

All methods to solve it analytically will be called basic MLE
techniques. Often the equation is too complicated for such basic
techniques to exist. Then we need what we shall call the advanced
MLE techniques.

<p>
<b>EXAMPLE:</b>&nbsp;
We have a random sample $X_1,...,X_n$ from the Gamma($\alpha,p$) with
pdf
$$
f(x) = \left\{\begin{array}{ll}\frac{p^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-px}&\text{if }x&gt;0\\0&\text{otherwise.}\end{array}\right..
$$
Here the parameter space for $(\alpha,p)$ is $(0,\infty)^2.$
Obtain MLE of the parameters.
<p></p>
<b>SOLUTION:</b>
The log-likelihood function is
$$
\ell(\alpha,p) = n(\alpha \log p - \log \Gamma(\alpha))
+(\alpha-1)\sum_i \log x_i -p\sum_i x_i. 
$$
It is quite easy to differentiate partially wrt $p$ and
equate it to zero: 
 $$
\do p \ell(\alpha,p) = \frac{n \alpha}{p}-\sum_i x_i. 
$$
So for each given value of  $\alpha$ we must have 
 $$
\hat p = \frac{n \alpha}{\sum_i x_i}. 
$$
Putting this 
in $\ell(\alpha,p)$ we get a function of $\alpha $ alone:
$$
\ell_1(\alpha) = \ell(\alpha,\hat p).
$$
Since this involves the $\Gamma $ function, the
equation $\ell_1`(\alpha) = 0$ may not be solved analytically.
 ///
</p>


<p>
<b>EXAMPLE:</b>&nbsp;
<b>(Probit model)</b> We have $n$ mice (random sample from a
cohort bred under uniform condition). The $i$-th mouse is
given a dose $x_i$  of a poison. $Y_i = 1$ if
the $i$-th mouse was killed by the dose, and $0$
otherwise. Our data consist of the $n$
pairs $(x_1,Y_1),...,(x_n,Y_n).$ 
<p></p>
Want to fit the model
$$
Y_i\sim Bern\left(\Phi\left(\alpha+\beta x_i\right)\right), 
$$
and $Y_i$`s independent, where $\alpha\in{\mathbb R}$
and $\beta &gt;0$ are the unknown parameters. 
<p></p>
Estimate $\alpha, \beta $ using MLE.
<p></p>
<b>SOLUTION:</b>
Here the likelihood function is 
$$
L(\alpha,\beta) = \prod_1^n \left[\Phi(\alpha+\beta x_i) \right]^{Y_i}\left[1-\Phi(\alpha+\beta x_i) \right]^{1- Y_i}.
$$
So the log-likelihood function is
$$
\ell(\alpha,\beta) = \sum [Y_i \log\Phi(\alpha+\beta x_i)]+ \sum[(1- Y_i)\log (1-\Phi(\alpha+\beta x_i))].
$$
We need to solve $\nabla\ell(\alpha,\beta)=(0,0).$

<p></p>
I hope you will agree with me that the expressions are not
particularly apetising! 
 ///
</p>

<h2><a
name="Technique 1: Newton-Raphson method">Technique 1: Newton-Raphson method</a></h2>
The Newton-Raphson method is a numerical approach to solving any
equation of the form $g(\v x) = \v0$, where $g:{\mathbb R}^n\rightarrow{\mathbb R}^n$ is
differentiable. It uses the iterative scheme
$$
\v x_{n+1} = \v x_n - J(\v x_n) ^{-1} g(\v x_n)
$$
starting from some $\v x_0.$ Here $J(\v x)$ is the
Jacobian matrix of $f$ at $\v x.$ 
<p></p>
There is no guaranty that it will converge. But for a large class
of problems it does converge quite fast if $\v x_0$ is pretty close to the
actual answer. 
<p></p>
On the face of it, it may seem that only first order derivatives
are involved. But when we are using the Newton-Raphson method for
MLE, our $g(\cdot)$ is actually the derivative of the
log-likelihood function. So the Jacobian of $g$ is actually
the second derivative matrix (called the Hessian) of the
log-likelihood. The iterative scheme is:
$$
\v \theta_{n+1} = \v \theta_n - H(\v \theta_n) ^{-1}  \nabla \ell (\v \theta_n),
$$
where $H_{k\times k}$ is the Hessian matrix with $(i,j)$-th entry 
$$
\frac{\partial^2}{\partial \theta_i\partial \theta_j} \ell(\v \theta).
$$
Here $k$ is the number of parameter (<i>i.e.</i>, the number of
 components in $\v \theta$).
<p></p>
 As you might guess, the expressions often turn
out to be quite hairy, limiting the effectiveness of the
technique. 
<p>
<b>EXAMPLE:</b>&nbsp;<b>(Newton-Raphson for probit)</b>
We had already seen that the log-likelihood equation was pretty
nasty looking. But we can notationally simplify things
by introducing two functions: 
$f(x) = \phi(x)/\Phi(x)$ and $g(x) =
\phi(x)/(1-\Phi(x)).$
  
<p></p>

It is easy to see that $f'(x) = -f(x)(x+f(x))$ and $g'(x) = -g(x)(x-g(x)).$
<p></p>
With these definitions, we can now write
$$\begin{eqnarray*}
\do \alpha \ell &amp; = &amp; \sum h_i(\alpha,\beta)\\
\do \beta  \ell &amp; = &amp; \sum x_i h_i(\alpha,\beta),
\end{eqnarray*}$$ 
where 
$$
h_i(\alpha,\beta) = Y_i f(\alpha + \beta x_i) - (1-Y_i) g(\alpha + \beta x_i),
$$
and hence
$$\begin{eqnarray*}
\do \alpha h_i(\alpha,\beta) &amp; = &amp; Y_i f'(\alpha + \beta x_i) - (1-Y_i) g'(\alpha + \beta x_i),\\
\do \beta h_i(\alpha,\beta) &amp; = &amp; x_i \do \alpha h_i(\alpha,\beta).
\end{eqnarray*}$$


So 
$$\begin{eqnarray*}
\dt \alpha \ell &amp; = &amp; \sum \do \alpha h_i(\alpha,\beta)\\
\dt \beta \ell &amp; = &amp; \sum x_i^2\do \alpha  h_i(\alpha,\beta)\\
\doo \alpha \beta  \ell &amp; = &amp; \sum x_i \do \alpha h_i(\alpha,\beta).
\end{eqnarray*}$$ 
Then the Newton-Raphson iteration is
$$
\left[\begin{array}{ccccccccccc}\alpha_{k+1} \\ \beta_{k+1} 
\end{array}\right] = \left[\begin{array}{ccccccccccc}\alpha_k \\ \beta_k 
\end{array}\right]
- \left[\begin{array}{ccccccccccc}\dt \alpha \ell &amp; \doo \alpha \beta \ell\\ \doo \alpha
\beta \ell &amp; \dt \beta \ell
\end{array}\right]^{-1} \left[\begin{array}{ccccccccccc}\do \alpha \ell\\\do \beta \ell
\end{array}\right].
$$
Here the RHS is evaluated at $(\alpha_k,\beta_k).$
 ///
</p>

<h2><a
name="Technique 2: Fisher`s scoring method">Technique 2: Fisher`s scoring method</a></h2>
This is actually a variant of the Newton-Raphson method, but
tailored to finding MLE. Here we replace the Hessian matrix by
its expectation. So the Fisher`s scoring iteration is:
$$
\left[\begin{array}{ccccccccccc}\alpha_{k+1} \\ \beta_{k+1} 
\end{array}\right] = \left[\begin{array}{ccccccccccc}\alpha_k \\ \beta_k 
\end{array}\right]
+ \left(-E\left[\begin{array}{ccccccccccc}\dt \alpha \ell &amp; \doo \alpha \beta \ell\\ \doo \alpha
\beta \ell &amp; \dt \beta \ell
\end{array}\right]\right)^{-1} \left[\begin{array}{ccccccccccc}\do \alpha \ell\\\do \beta \ell
\end{array}\right].
$$
The matrix whose inverse we are taking (<i>i.e.</i>, the negative of
the expected Hessian) is called the <b>Fisher information
matrix</b> and denoted by $I(\v \theta).$ If $\v
\theta$ just consists of a single scalar parameter,
then $I(\v \theta)$ is just a number, called the <b>Fisher
information</b>. 

<p></p>

While the Fisher`s scoring method  might appear even hairier than
the Newton-Raphson method, usually the expectation
often simplifies things by averaging over all possible
data. Note that $I(\v \theta)$, unlike the Hessian 
is <i>not</i> a function of the random sample.

<p></p>

Notice that $\nabla\ell(\v \theta)$ is a <i>row</i>
vector. So the quantity inside the expectation is a $k\times
k$ matrix, where $k$ is the number of components in $\theta.$

<h3><a
name="Advantages of Fisher`s scoring method over
Newton-Raphson">Advantages of Fisher`s scoring method over
Newton-Raphson</a></h3>
There are three advantages:
<ol type="">

<li>The expressions involved in the iterative scheme is analytically more tractable. So it is easier to write
the code.</li>

<li>As we are averaging out over all possible samples, the
undulations specific to the given random sample are ironed out
to some extent (they are still there in the score function,
though). So possibly less chance of the iterative method getting stuck at
a local max. Never seen any demonstration of this.</li>

<li>Under fairly general conditions,  the inverse
of the information matrix is
the asymptotic covariance matrix of the MLE. This is automatically
computed as a byproduct.</li>

</ol>


<p>
<b>EXAMPLE:</b>&nbsp;<b>(Fisher`s scoring method for probit)</b>
For Fisher`s scoring method we need the expectation of the Hessian
matrix. This involves 
$$
E\left(\do \alpha h_i(\alpha,\beta) \right)  =  E(Y_i) f'(\alpha + \beta x_i)
- (1-E(Y_i)) g'(\alpha + \beta x_i).
$$
Since $E(Y_i) = \Phi(\alpha + \beta x_i),$ we get some
simplification:
$$
E(Y_i) f'(\alpha + \beta x_i) = -\phi(\alpha + \beta x_i) (x + f(x))
$$
and 
$$
(1-E(Y_i)) g'(\alpha + \beta x_i) = -\phi(\alpha + \beta x_i) (x - g(x))
$$
 ///
</p>


<h2><a
name="More on Fisher information">More on Fisher information</a></h2>
What does it have to do with information? Consider the following
two cases for the log-likelihood function:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/loglikinfo.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Which one pinpoints the MLE better? Surely the first one, because
the peak is more prominent. So we shall say that the first one
has more information about the parameter than the second has. You
might like to compare them both with the extreme example:
estimating $\theta$ from a random sample from $N(0,1)$
(no $\theta $ in the distribution at all). Here the
log-likelihood function is just flat, meaning that there is no
information at all in the data about $\theta.$
<p></p>
The second derivative of the log-likelihood function is negative
at the peak. The more prominent the peak, the more negative it
is. So its negative should be a good indication of  the amount of
information the data contain about the parameter.
<p></p>
The Fisher information matrix occurs in a number of theorems in
statistics, one of which is relevant here:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Under some regularity conditions, 
$$
I(\v \theta)^{1/2} (\hat \v\theta_{mle}- \v\theta)\rightarrow N_k(\v0,I),
$$
where $k$ is the dimension of $\v \theta.$
</fieldset>
It is more commonly (though less rigourously) remembered as 
<blockquote>"For
large enuogh $n$, the sampling distribution of $\hat
\v\theta_{mle}$ is roughly $N_k(\v\theta, I(\v\theta)^{-1}).$"
</blockquote>
<hr/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/>
</body>
</html>
