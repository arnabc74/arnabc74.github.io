<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE-BAN-OTF-new.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v19.0" nonce="Q7jTbrCq"></script>

<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else {
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script>
<style>
body,table {
  margin: 0;
  font-size: 40;
  //background: #000;
  //color: #fff;
}

.ans {
  display:none;
  background: #ccffcc;
}

.sticky {
  position: fixed;
  top: 0;
  width: 100%;
  background: #555;
  color: #f1f1f1;
}

.cu {
  background: #ffcccc;
}

.bu {
  background: #ccccff;
}

.scrpt {
  margin:10px;
  border-left: 5px solid black;
}

.box {
  background-color: yellow; 
  //border: 2px solid black;
  display: inline-block;
}

.hl {
  list-style-type: upper-alpha;
}
</style>
<script>
window.onscroll = function() {myFunction()};
window.onload = function() {myInit()};

var header, tphldr;
function myInit() {
  header = document.getElementsByClassName("header");
  tphldr = document.getElementById("topholder");
}

function myFunction() {
  var index = -1
  for(i=0;i<header.length;i++) {
    if (window.pageYOffset > header[i].offsetTop) {
       index = i
    }
    else {
       break
    }
  }

  if(index < 0) 
    tphldr.innerHTML = "";
  else
    tphldr.innerHTML = header[index].innerHTML
}
</script><script type="text/javascript" src="https://arnabc74.github.io/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="file:///home/asu/na/v/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head><body>
<div class="sticky" id="topholder"> </div>
<a href="http://web.isical.ac.in/~arnabc/">[Home]</a>
<h3>Distributions with density</h3>
<ul>
<li>
<a href="#The concept of density">The concept of density</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Properties of PDF">Properties of PDF</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 1">Problem set 1</a>
</li>
<li>
<a href="#More about PDF">More about PDF</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 2">Problem set 2</a>
</li>
<li>
<a href="#Proving the theorem (part 1)">Proving the theorem (part 1)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 3">Problem set 3</a>
</li>
<li>
<a href="#Proving the theorem (part 2)">Proving the theorem (part 2)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#From CDF to PDF">From CDF to PDF</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 4">Problem set 4</a>
</li>
<li>
<a href="#Nonuniqueness of density">Nonuniqueness of density</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 5">Problem set 5</a>
</li>
<li>
<a href="#Physical interpretation of density">Physical interpretation of density</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 6">Problem set 6</a>
</li>
<li>
<a href="#Are continuous distributions counterintuitive?">Are continuous distributions counterintuitive?</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 7">Problem set 7</a>
</li>
<li>
<a href="#Expectation and moments">Expectation and moments</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 8">Problem set 8</a>
</li>
<li>
<a href="#Hazard rate">Hazard rate</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 9">Problem set 9</a>
</li>
<li>
<a href="#Uniform distribution">Uniform distribution</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 10">Problem set 10</a>
</li>
<li>
<a href="#Miscellaneous">Miscellaneous</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 11">Problem set 11</a>
</li>
</ul>
<hr/>

<title xmlns="">Distributions with density</title>

<h1><a
name="The concept of density">The concept of density</a></h1>
<a href="https://youtu.be/AwB5Xw7Hedw">Video for this section</a>
<p></p>
We already know the following two definitions.
<fieldset>
<legend><b>Definition: Cumulative distribution function (CDF)</b></legend>
Let $X$  be a random variable. By its <b><font color="red" size="40">cumulative distribution function (CDF)</font></b> we mean the function $F:{\mathbb R}\rightarrow[0,1]$ 
 given by
$$F(x) = P(X\leq x),\quad x\in{\mathbb R}.$$
</fieldset>

<p></p>

<fieldset>
<legend><b>Definition: Continuous distribution</b></legend>A random variable $X$  is said to be  <b><font color="red" size="40">continuous</font></b>  or to have a
 <b><font color="red" size="40">continuous distribution</font></b>  if its CDF is continuous.</fieldset>
A special class of such random variables is given in the following definition:
<fieldset>
<legend><b>Definition: </b></legend>A random variable $X$  is said to have <b><font color="red" size="40">probability density function (PDF)</font></b> 
(or, more simply, a <b><font color="red" size="40">density</font></b>) $f:{\mathbb R}\rightarrow[0,\infty)$
if for every $a\leq b\in{\mathbb R}$  we have 
$$P(X\in[a,b]) = \int_a^b f(x)\, dx.$$
</fieldset>


<p></p>

<h2><a
name="Properties of PDF">Properties of PDF</a></h2>
The following facts all come from continuity of probability. 
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X$  have a density $f(x).$  Then 
<ol type="">

<li>$\forall a\in{\mathbb R}~~P(X=a) = 0.$</li>

<li>$\forall a&lt;b\in{\mathbb R}~~P(a &lt; X &lt; b) =P(a &lt; X \leq b) =P(a \leq X &lt; b) =P(a \leq X \leq b) = \int_a^b f(x)\, dx.$</li>

<li>$\forall a\in{\mathbb R}~~P(a &lt; X )  = P(a\leq X)=\int_a^\infty f(x)\, dx.$</li>

<li>$\forall a\in{\mathbb R}~~P(X &lt; a)  = P(X\leq a)=\int_ \infty^a f(x)\, dx.$</li>

<li>$\int_ \infty^ \infty f(x)\, dx = 1.$</li>

</ol>

</fieldset>

<p></p>

<h2><a
name="Problem set 1">Problem set 1</a></h2>
::<p>
<b>EXERCISE 1:</b>&nbsp;<font size="-2">[rosspdf31.png]</font><img width="" src="image/rosspdf31.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 2:</b>&nbsp;<font size="-2">[rosspdf32.png]</font><img width="" src="image/rosspdf32.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 3:</b>&nbsp;<font size="-2">[rosspdf2.png]</font><img width="" src="image/rosspdf2.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 4:</b>&nbsp;<font size="-2">[rosspdf3.png]</font><img width="" src="image/rosspdf3.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 5:</b>&nbsp;<font size="-2">[rosspdf5.png]</font><img width="" src="image/rosspdf5.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 6:</b>&nbsp;<font size="-2">[rosspdf21.png]</font><img width="" src="image/rosspdf21.png" style="vertical-align:text-top;">
<p><a
href="javascript:hideShow('lab1')"><b>[Hint]</b></a><div
class="ans" id="lab1">Use integration by parts (twice). Also you need to know this fact: $\int_0^\infty e^{-x^2/2}\, dx = \sqrt{\frac \pi2}.$</div></p>

</p>

<p></p>
::<p>
<b>EXERCISE 7:</b>&nbsp;<font size="-2">[hpspdf13.png]</font><img width="" src="image/hpspdf13.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 8:</b>&nbsp;<font size="-2">[hpspdf22.png]</font><img width="" src="image/hpspdf22.png" style="vertical-align:text-top;"></p>

<p></p>

<h1><a
name="More about PDF">More about PDF</a></h1>
<a href="https://youtu.be/gNzIXFDJ5C8">Video for this section</a>
<p></p>
A PDF can look surprisingly complicated. We shall explore the general form later when we shall
 learn about something called the Lebesgue integral.  Fortunately, for all practical applications the PDF is quite simple:
 it is positive and Riemann integrable (in fact, usually continuous) on some interval, and vanishes outside it. We
 shall exclusively work with such PDFs.  There
 is a simple characterisation of such densities:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $I$  be any interval in ${\mathbb R}.$  Let $f:{\mathbb R}\rightarrow[0,\infty)$  vanish outside $I.$  Then $f$  is
 a density of some random variable if and only if $\int_{-\infty}^\infty f(x)\, dx = 1.$
</fieldset>
We shall prove it soon. But first some examples.
<p></p>

<p>
<b>EXAMPLE 1:</b>&nbsp;
$f(x) = \left\{\begin{array}{ll}3x^2&\text{if }x\in (0,1)\\ 0&\text{otherwise.}\end{array}\right.$
is one density.
 ■
</p>
The supporting interval may be unbounded. 
<p>
<b>EXAMPLE 2:</b>&nbsp;
$f(x) = \left\{\begin{array}{ll}2e^{-2x}&\text{if }x\in (0,\infty)\\ 0&\text{otherwise.}\end{array}\right.$
is a density.
 ■
</p>
The density may be unbounded. 
<p>
<b>EXAMPLE 3:</b>&nbsp;
$f(x) = \left\{\begin{array}{ll}\frac{1}{\pi\sqrt{x(1-x)}}&\text{if }x\in (0,1)\\ 0&\text{otherwise.}\end{array}\right.$
is a density.
 ■
</p>
Both the density and the supporting interval may be unbounded.
<p>
<b>EXAMPLE 4:</b>&nbsp;
$f(x) = \left\{\begin{array}{ll}cx^{-\frac 12} e^{-x/2}&\text{if }x\in (0,\infty)\\ 0&\text{otherwise.}\end{array}\right.$
is a density.
 ■
</p>

<p></p>

<h2><a
name="Problem set 2">Problem set 2</a></h2>

<p></p>

<p>
<b>EXERCISE 9:</b>&nbsp;Is $f(x) = \frac{1}{\pi(1+x^2)}$  a density?</p>

<p></p>

<p>
<b>EXERCISE 10:</b>&nbsp;Is it possible to find $c\in{\mathbb R}$  such that $f(x)=ce^{-|x|}$  is a density? Sketch the graph of $f(x).$</p>

<p></p>

<p>
<b>EXERCISE 11:</b>&nbsp;Is it possible to find $c\in{\mathbb R}$  such that $f(x)=\left\{\begin{array}{ll}\frac{c}{x}&\text{if }x&gt;0\\ 0&\text{otherwise.}\end{array}\right.$  is a density?</p>

<p></p>

<h1><a
name="Proving the theorem (part 1)">Proving the theorem (part 1)</a></h1>
<a href="https://youtu.be/RKzORDfHLSY">Video for this section</a>
<p></p>
We know that the CDF of a random variable must satisfy the following conditions:
<ol type="">
<li>$F(x)$  must be non-decreasing.</li>

<li>$\lim_{x\rightarrow-\infty} F(x) = 0.$</li>

<li>$\lim_{x\rightarrow\infty} F(x) = 1.$</li>

<li>$F$  must be right continuous.</li>

</ol>
Conversely, the <b><font color="red" size="40">fundamental theorem of probability</font></b>  guarantees that any $F:{\mathbb R}\rightarrow[0,1]$  with these properties must
 be the CDF of some random variable (<i>i.e.</i>, there must exist some probability space and a random variable defined on it with
$F$  as the CDF.
<p></p>
We have a similar characterisation for densities as well. This is given in the next theorem. 
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
A function $f:{\mathbb R}\rightarrow[0,\infty)$  is a density of some random variable if and only if  $\int_ {-\infty}
 ^\infty f(x)\, dx = 1.$ 
</fieldset>
Let us start proving the theorem.
<p></p>
Let $F(x) = \int_{- \infty}^x f(t)\, dt.$  
<p></p>
Then 
<ol type="">
<li> $F(x)$  is non-decreasing, because $f$  is non-negative.
</li>
<li>
$\lim_{x\rightarrow\infty} F(x) = 1$, since $\int_{-\infty}^\infty f(x)\, dx = 1.$  
</li>
<li>
$\lim_{x\rightarrow-\infty} F(x) = 0$.
<blockquote><a
href="javascript:hideShow('reason1')"><b>[Because...]</b></a><div
class="ans" id="reason1">
$ F(x) =\int_{-\infty}^x f(x)\, dx = \int_{-\infty}^0 f(x)\, dx-\int_x^0 f(x)\, dx.$
<p></p>
So 
$$\begin{eqnarray*}
\lim_{x\rightarrow-\infty} F(x) 
&amp; = &amp; \lim_{x\rightarrow-\infty} \left[ \int_{-\infty}^0 f(x)\, dx-\int_x^0 f(x)\, dx\right]\\
&amp; = &amp; \int_{-\infty}^0 f(x)\, dx-\lim_{x\rightarrow-\infty} \int_x^0 f(x)\,dx\\
 = \int_{-\infty}^0 f(x)\, dx-\int_{-\infty}^0 f(x)\, dx = 0.
\end{eqnarray*}$$
</div></blockquote>
</li>

<li>
Also $F$  is continuous (and so right continuous) everywhere.
<blockquote><a
href="javascript:hideShow('reason2')"><b>[Because...]</b></a><div
class="ans" id="reason2">
Take any $a\in{\mathbb R}.$
<u>Case 1: $f$  is bounded by some $B&gt;0$  in a neighbourhood of $a$</u>:  
Then for any $x$  in that neighbourhood, we have
$|F(x)-F(a)| = \left|\int_a^{x} f(x)\, dx\right|\leq B|x-a|\rightarrow 0$
as $x\rightarrow a.$
<p></p>

<u>Case 2: $f$  is unbounded in every neighbourhood of $a$</u>: Here the result will follow from property of 
improper Riemann integrals. We shall explore this in the exercies below.
</div></blockquote>

</li>
</ol>
The proof is not yet over. But let us solve some problems to digest the argument so far.
<h2><a
name="Problem set 3">Problem set 3</a></h2>

<p>
<b>EXERCISE 12:</b>&nbsp;
Consider a density with a graph as shown below.
<center>
<table width="100%">
<tr>
<th><img width="" src="image/lunb.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Find $F(a).$  Then express $F(x)-F(a)$  as the difference of two integrals (you may use
 $b$  for this). Finally show that $\lim_{x\rightarrow a+} F(x)-F(a) = 0.$ 
</p>

<p>
<b>EXERCISE 13:</b>&nbsp;
Consider the same density as in the last exercise.
Show (trivially) that $\lim_{y\rightarrow a-} F(y)-F(a) = 0.$ 
</p>

<p></p>

<h1><a
name="Proving the theorem (part 2)">Proving the theorem (part 2)</a></h1>
<a href="https://youtu.be/lSVLrhkst1Y">Video for this section</a>
<p></p>

<p></p>
So by the fundamental theorem of probability, there exists a random variable $X$  with CDF  $F(x).$
<p></p>
To complete the proof we need to show that $X$  has PDF $f(x),$  <i>i.e.</i>, 
<p></p>

<div class="box">Target</div>$\forall a \leq b~~P(a \leq X \leq b) = \int_a^b f(t)\, dt.$
<p></p>

<div class="box">$\forall a,b$</div> Take any $a\leq b.$
<p></p>
Here $F$  is a continuous function. So $P(X=a) = 0.$
<p></p>
Hence 
$$P(a\leq X\leq b) = P(a &lt; X\leq b) = P(X\leq b)-P(X\leq a) = F(b)-F(a) = \int_{-\infty}^b f(t)\, dt-\int_{-\infty}^a f(t)\, dt = \int_a^b f(t)\, dt,$$
completing the proof.
<h2><a
name="From CDF to PDF">From CDF to PDF</a></h2>
It is possible to recover a density from the CDF as follows. Let $F$  be the CDF having a density. Then
$F$ must be continuous.  Define $f$  as follows;
$$f(x) = \left\{\begin{array}{ll}F'(x)&\text{if }\mbox{it exists}\\ 0&\text{otherwise.}\end{array}\right. $$
This $f$  must be a PDF for our CDF. For most applications, $F$  will fail to be differentiable at only finitely
 many points.
<p></p>
For this technique to work $F$  must actually have a density. Otherwise, the
function  $f(x)$  obtained by this procedure will not be a density. 
For example, for a
 discrete distribution, the CDF is a step function, and so the above prescription would lead to $f(x)\equiv 0.$
<p></p>

<img src="image/alert.png">If you are given a CDF $F$, and you do not know if it has a density, then you could still define $f$  as above,
 and actually check if $\forall x\in{\mathbb R}~~F(x) = \int_{-\infty}^x f(t)\, dt.$  
<p></p>

<h2><a
name="Problem set 4">Problem set 4</a></h2>
::<p>
<b>EXERCISE 14:</b>&nbsp;<font size="-2">[rosspdf1.png]</font><img width="" src="image/rosspdf1.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 15:</b>&nbsp;<font size="-2">[rosspdf4.png]</font><img width="" src="image/rosspdf4.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 16:</b>&nbsp;<font size="-2">[hpspdf1.png]</font><img width="" src="image/hpspdf1.png" style="vertical-align:text-top;">
The answer should not involve any limit of $F_X.$
</p>

<p></p>
::<p>
<b>EXERCISE 17:</b>&nbsp;<font size="-2">[hpspdf8.png]</font><img width="" src="image/hpspdf8.png" style="vertical-align:text-top;">
<p></p>
Here is Equation (3) mentioned above:
<p></p>

<center>
<fieldset>
<legend><b>Equation 3</b></legend><img width="80%" src="image/hpspdf9.png"></fieldset>
</center>
</p>
::<p>
<b>EXERCISE 18:</b>&nbsp;<font size="-2">[hpspdf10.png]</font><img width="" src="image/hpspdf10.png" style="vertical-align:text-top;">
<p></p>
Equation (3) is as given in the exercise above.
</p>
::<p>
<b>EXERCISE 19:</b>&nbsp;<font size="-2">[hpspdf14.png]</font><img width="" src="image/hpspdf14.png" style="vertical-align:text-top;"></p>

<h1><a
name="Nonuniqueness of density">Nonuniqueness of density</a></h1>
<a href="https://youtu.be/e7osUxY0sUU">Video for this section</a>
<p></p>
It is  a somewhat disconcerting fact that density of a distribution is not unique. For intance, 
 changing a density at any countable number of points to arbitrary non-negative values would still
 keep it a density for the same distribution.
<p></p>

<p>
<b>EXAMPLE 5:</b>&nbsp;
$f(x)=\left\{\begin{array}{ll}2e^{-2x}&\text{if }x&gt;0\\ 0&\text{otherwise.}\end{array}\right.$   and
$f(x)=\left\{\begin{array}{ll}2e^{-2x}&\text{if }x&gt;0,\,x\neq3\\ 10000&\text{if }x=3\\ 0&\text{otherwise.}\end{array}\right.$  
are both densities corresponding to the same distribution.
 ■
</p>
 However, the following result provides some relief.
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $f(x)$  and $g(x)$  are both densities for the same distribution and both are continuous at some $a\in{\mathbb R},$ 
 then $f(a)=g(a).$
</fieldset>

<p>
<b><i>Proof:</i></b>
Let, if possible, $f(a)\neq g(a).$  Say $f(a)&gt; g(a).$  
<p></p>
Let $h(x) = f(x)-g(x).$  Then $h(x)$  is continuous at $x=a$  and $h(a)&gt;0.$  
<p></p>
So $\exists\delta&gt;0~~\forall x\in (a-\delta,a+\delta)~~h(x)&gt;\frac{h(a)}{2}.$
<p></p>
Hence $\int_{a-\delta}^{a+\delta} h(x)\, dx \geq \delta h(a) &gt; 0.$
<p></p>
But this is impossible, since $\int_{a-\delta}^{a+\delta} f(x)\, dx =\int_{a-\delta}^{a+\delta} g(x)\, dx, $  since
 both are equal to $P(a-\delta&lt; X &lt; a + \delta).$
<b><i>[QED]</i></b>
</p>

<p></p>
Indeed, we shall mostly work with random variables $X,$ for which there will be an density which will
 be continuous over an interval
 $I,$  for which $P(X\in I)=1.$
<h2><a
name="Problem set 5">Problem set 5</a></h2>

<p>
<b>EXERCISE 20:</b>&nbsp;
If $f,g:{\mathbb R}\rightarrow{\mathbb R}$  are two functions such that $\forall a\leq b~~\int_a^b f(x)\, dx = \int_a^b g(x)\, dx,$  and
 they are both continuous at $x=c,$   then show that $f(c)=g(c).$  
<p><a
href="javascript:hideShow('lab2')"><b>[Hint]</b></a><div
class="ans" id="lab2">
Let, if possible,  $f(c)\neq g(c).$  Without loss of generality $f(c)&lt; g(c),$  say. 
<p></p>
Consider $h(x) = g(x)-h(x).$
<p></p>
Then $h$  is continuous at $c$  and $h(c)&gt;0.$  
<p></p>
Note that it is enough to show that
$$\exists \delta&gt;0~~\int_{a-\delta}^{a+\delta} h(x)\, dx&gt;0.\hspace{1in} \mbox{(*)}$$
<blockquote><a
href="javascript:hideShow('reason3')"><b>[Because...]</b></a><div
class="ans" id="reason3"> 
 This will complete the
 contradiction, because then $\int_{a-\delta}^{a+\delta} g(x)\, dx&gt;\int_{a-\delta}^{a+\delta} f(x)\, dx,$  
contradicting the given condition. 
</div></blockquote>
Since $h(x)$  is continuous at $x=c,$  we have
$$\forall \epsilon&gt;0~~\exists \delta&gt;0~~\forall x\in(c-\delta,c+\delta)~~h(x)\in(h(c)-\epsilon,h(c)+\epsilon).$$
Choose $\epsilon= \frac{h(c)}{2}&gt;0.$
<p></p>
Then  
$$\exists \delta&gt;0~~\forall x\in(c-\delta,c+\delta)~~h(x)\in(h(c)-\epsilon,h(c)+\epsilon).$$
For any $x\in(c-\delta,c+\delta)$  we have $h(x)&gt; h(c)-\epsilon = 2 \epsilon-\epsilon = \epsilon&gt;0.$
<p></p>
So $\int_{a-\delta}^{a+\delta} h(x)\, dx\geq 2\delta \epsilon&gt;0.$
<p></p>
Hence we have shown (*), completing the proof.
</div></p>

</p>

<p>
<b>EXERCISE 21:</b>&nbsp;
State true or false: if $f(x)$  is a density, and $g(x)$  is obtained by changing $f(x)$  at only countably
 many points, then $g(x)$  is also a density corresponding to the same distribution. (Assume that $g(x)$  is non-negative).
</p>

<p></p>

<p>
<b>EXERCISE 22:</b>&nbsp;State true or false: If a distribution has a density, then it also has a continuous density.</p>

<p></p>

<p>
<b>EXERCISE 23:</b>&nbsp;In this problem we shall say that a density  $f$ "sits on"  an interval
 $I$  if all the three conditions hold:
<ul>
<li> $\forall
 x\in I~~f(x)&gt;0$</li>

<li>$f(x)$  vanishes outside $I.$</li>

<li>$f$  is continuous on $I$.</li>
</ul>
  Let $f,g$  be two densities
 for the same distribution. If $f$  "sits on"an interval  $I$  and $g$  "sits on"
 an interval $J$, then show that the closures of $I$  and $J$   must match. (This closure is called the
<b><font color="red" size="40">support</font></b>  of the distribution).
  </p>

<h1><a
name="Physical interpretation of density">Physical interpretation of density</a></h1>
<a href="https://youtu.be/SDGTjfYfOZ4">Video for this section</a>
<p></p>
If $X$  has a density $f(x)$  which is continuous at $x=a$  then $f(a)$  measures
 "something like $P(X\approx a)$". However, there are not even approximately equal. In
 particular, $f(a)$  may very well exceed $1.$  The precise statement is 
$$f(a) = \lim_{\epsilon\rightarrow 0+} \frac{P(X\in (a-\epsilon, a+\epsilon))}{2 \epsilon}.$$
Thus, if we have a density like the following, then $X$  is more likely to take values near $B$  than near $A$ 
 or $C.$
 <center>
<table width="100%">
<tr>
<th><img width="" src="image/pdfpeaks.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Just like everything else in probability theory, densities are also born out of statistical
 regularity. You might remember that probability came out of limit of proportions, and hence PMF
 was born out of barplots of relative frequencies. Similarly, densities are born of histograms, a graphical device to show
 relative frequency densities. Suppose that we have a random variable $X$  that can assume all values in an interval
 $[a,b].$  If we repeat the underlying random experiment a large number of times independently, then we shall get many
 values of $X$, that we can show along a number line:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/hist1.png"></th>
</tr>
<tr>
<th>Random values of $X$  along the number line.</th>
</tr>
</table>
</center>
This plot, however, is too clutterred as many points get plotted on top of each other. A more lucid plot is the histogram:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/hist2.png"></th>
</tr>
<tr>
<th>A histogram of the same random values of $X$.</th>
</tr>
</table>
</center>
 Here we have partitioned the range of $X$  into a number of subintervals (or
 <b><font color="red" size="40">bin</font></b> s) and have computed the
<b><font color="red" size="40">relative frequency density</font></b>
 for each bin as 
$$\mbox{rel. freq density of a bin} = \frac{\mbox{number of points in the bin}}{\mbox{sample size}\times
 \mbox{bin width}}.$$
Then we have erected a rectangle on each bin with height equal to the relative frequency density for that bin. As sample
 size tends to infinity, the shape of the histogram stabilises due to statistical regularity. If we now make the bins finer
 and finer, then the limiting shape of the histogram approaches a certain shape, which is the PDF. 
<p></p>


<h2><a
name="Problem set 6">Problem set 6</a></h2>

<p>
<b>EXERCISE 24:</b>&nbsp;Get/download a large data set on some continuous variable (height, weight, blood pressure
 etc). Make a histogram of the first half using R:
<font color="red">
<pre>
hist(dat[1:(n/2)], prob=T)
</pre>
</font>
Here <font size="+1" color="red"><code>dat</code></font>  is the array storing the values. The length of the array is <font size="+1" color="red"><code>n</code></font>. Notice the shape.
Now make a histogram of the rest of the data:
<font color="red">
<pre>
hist(dat[(n/2):n], prob=T)
</pre>
</font>
If <font size="+1" color="red"><code>n</code></font>  is large enough (say at least 5000), then you should see striking similarity between the shapes of these
 two histograms.
</p>

<p></p>

<p>
<b>EXERCISE 25:</b>&nbsp;This is a more specific version of the last exercise, where we shall work with audio data.
 Record some audio data of ambitent noise (<i>e.g.</i>, sound of fan, AC, crowded place) using your laptop
 or mobile. (You should get a
 file with <font size="+1" color="red"><code>.wav</code></font>
  or <font size="+1" color="red"><code>.mp3</code></font>  extension).  If you cannot record yourself, you may use this hardly audible audio file <a href="test.wav">test.wav</a>  of me rubbing my hands together. Download it your machine. Load it using R:
<font color="red">
<pre>
install.packages('tuneR') #You need to do this only once (with net connection)
library(tuneR)
dat = readWave('test.wav') #You may need to give the entire path of the file
hist(dat@left,probability=TRUE)
hist(dat@left,breaks=200,probability=TRUE) #finer histogram
</pre>
</font>

</p>

<p>
<b>EXERCISE 26:</b>&nbsp;
Let $X_1,X_2,...$  be IID with some common density $f(x).$  Fix any $a&lt;b.$  Let $Y_n$  be the proportion
 of the $X_1,...,X_n$  landing in $[a,b].$  (<i>i.e.</i>, you count the number of $X_i$'s inside $[a,b]$  for
 $i=1,...,n,$  and divide that number by $n$). Show that $Y_n$  converges $\int_a^b f(x)\, dx.$  [Hint:
 Use the weak law of large numbers that you learned last semester.]
</p>

<h1><a
name="Are continuous distributions counterintuitive?">Are continuous distributions counterintuitive?</a></h1>
<a href="https://youtu.be/JMx3Wcx37M4">Video for this section</a>
<p></p>
It may seem counterintuitive that  $\forall a\in{\mathbb R}~~P(X=a)=0$, and yet $P(X\in{\mathbb R})=1.$  There are two ways to
 think about it:
<ol type="">
<li>A straight line segment consists of individual points, each of which has lenth zero, yet
 the segment does have strictly positive length. The intuituion that lengths of constituents add
 up to the length of thw whole works only for the countable case.</li>

<li>But when we carry out the random experiment behind $X$  we do get some value $a.$ 
 If $P(X=a)=0,$  then shouldn't $\{X=a\}$  be an impossible event? Actually, when we say
 $\{X=a\}$  we are making a measurement, and all measurements have finite precision. If the
 precision of the measurement is $\epsilon,$  then the satement ''$X$  is found to be
 $a$'' actually means $X\in\left[a-\frac \epsilon2, a+\frac \epsilon2\right),$  which indeed has some positive probability.</li>

</ol>

<p></p>

<h2><a
name="Problem set 7">Problem set 7</a></h2>
::<p>
<b>EXERCISE 27:</b>&nbsp;
If $X$  has density $e^{-x}$  for $x&gt;0$  (and 0 else), then show that 
$$\lim_{\epsilon\rightarrow0+}\frac{P(1-\epsilon,1+\epsilon)}{2 \epsilon} = \frac 1e.$$
</p>
::<p>
<b>EXERCISE 28:</b>&nbsp;
If $X$  has density $e^{-x}$  for $x&gt;0$  (and 0 else), then show that 
$$\lim_{\epsilon\rightarrow0+}\frac{P[2,2+\epsilon)}{\epsilon} = e^{-2}.$$
</p>

<p></p>

<p>
<b>EXERCISE 29:</b>&nbsp;
If $X$  has density $f(x)$  which is continuous over a neighbourhood of $x=a,$  then prove that
$$\lim_{\epsilon\rightarrow0+}\frac{P(a-\epsilon,a+\epsilon)}{2 \epsilon} = f(a).$$
Can you drop the continuity assumption?
</p>

<p></p>

<p>
<b>EXERCISE 30:</b>&nbsp;
Let $X$  have density $f(x)=\left\{\begin{array}{ll}1&\text{if }x\in(0,1)\\ 0&\text{otherwise.}\end{array}\right.$  Find 
$$\lim_{\epsilon\rightarrow0+}\frac{P(1-\epsilon,1+\epsilon)}{2 \epsilon}.$$  
</p>

<h1><a
name="Expectation and moments">Expectation and moments</a></h1>
<a href="https://youtu.be/NbjZeS88QkE">Video for this section</a>
<p></p>

<p></p>
We had already defined expectation of a random variable in general. That definition reduces to the following.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X$  be a non-negative random variable with density $f.$ 
Then $E(X)=\int_0^\infty x f(x)\, dx$ (which may be $\infty$).
</fieldset>
We shall prove this near the end of this course. Until then we shall treat this as the definition
 of $E(X)$  when $X$  has density $f.$
<p></p>
If $X$  can take negative values, then we again use the $X_+$, $X_-$  technique.
<p></p>
The following familiar properties of expectation (which are actually consequences of the general
 definition) can be easily shown to hold in this special case:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X, Y$  are jointly distributed  random variables with expectations defined (may be $\infty$ 
 or $-\infty$), and $X\leq Y,$ then $E(X)\leq E(Y).$
</fieldset> 

<p></p>

<fieldset>
<legend><b><i>Linearlity of expectation</i></b></legend>
If $X,Y$  are jointly distributed random variables with finite expectations and
 $a,b\in{\mathbb R}$, then $aX+bY$  also has finite expectation, $E(aX+bY) = aE(X)+bE(Y).$
</fieldset>
Both these properties follow directly from properties of integration.
<p></p>
The following theorem is also of a familiar form:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X$  be a random variable with density $f.$  Let $h:{\mathbb R}\rightarrow{\mathbb R}$  be a any function such that $h(X)$ 
 is  a random variable. Then $E(h(X))=\int_{-\infty}^\infty h(x) f(x)\, dx$  if $\int_{-\infty}^\infty |h(x) f(x)|\, dx &lt; \infty.$ 
 If $\int_{-\infty}^\infty |h(x) f(x)|\, dx = \infty,$  then $E(h(X))$  does not exist finitely (may be $\infty$ 
 or $-\infty$  or non-existent).
</fieldset>
While we shall not prove this here, you should be aware of one subtle implication. Suppose that
 $X$  has density $f$  and we are trying to find $E(X^2).$  Then here are two ways of proceeding:
<ol type="">
<li>Work out the distribution of $Y=X^2$  and find $E(Y)$  using the general
 definition, this may be very compicated to do in practice, but at least one can do this in principle.</li>

<li>Use the above theorem.</li>

</ol> 
 Both the approaches should produce the same result. This is the main implication of the theorem. The usefulness of the theorem
lies in the fact that the second approach is usually much easier than the first. 
<p></p>
We define $V(X)$  and the other moments as usual. Their
properties (<i>e.g.</i>, Chebyshev inequality) are intact.
<p></p>

<h2><a
name="Problem set 8">Problem set 8</a></h2>
::<p>
<b>EXERCISE 31:</b>&nbsp;<font size="-2">[rosspdf6.png]</font><img width="" src="image/rosspdf6.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 32:</b>&nbsp;<font size="-2">[rosspdf7.png]</font><img width="" src="image/rosspdf7.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 33:</b>&nbsp;<font size="-2">[rosspdf8.png]</font><img width="" src="image/rosspdf8.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 34:</b>&nbsp;<font size="-2">[rosspdf26.png]</font><img width="" src="image/rosspdf26.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 35:</b>&nbsp;<font size="-2">[rosspdf27.png]</font><img width="" src="image/rosspdf27.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 36:</b>&nbsp;<font size="-2">[rosspdf33.png]</font><img width="" src="image/rosspdf33.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 37:</b>&nbsp;<font size="-2">[rosspdf34.png]</font><img width="" src="image/rosspdf34.png" style="vertical-align:text-top;"></p>

<h1><a
name="Hazard rate">Hazard rate</a></h1>
An important class of distributions are called <b><font color="red" size="40">life span distributions</font></b>. Imagine a
 huge population of perishable objects that have a well-defined life span. (<i>e.g.</i>, animals,
 electronic components, but not vegetables, as "death" is not well-defined there). 
We pick one element at random and follow it until it dies.  Let $X$  be the age at death. Clearly, $X$  is a non-negative
 random variable. Typically we consider it as having a density. Then a quantity of interest is what is the chance of death
 at a given age. This gives rise to the following definition.
<fieldset>
<legend><b>Definition: Hazard rate</b></legend>
If a non-negative $X$  has density $f$  and CDF $F,$  then its its <b><font color="red" size="40">hazard rate</font></b>  at time $t\leq 0$ 
 is 
$$h(t) = \frac{f(t)}{1-F(t)},$$
when $F(t) &lt; 1.$  Here we are assuming that $f$  is continuous at all values of $t$  where we are computing
 the hazard rate.
</fieldset>
We can think of it as 
$$\lim_{\delta t\rightarrow 0+} \frac{P(X\leq t+\delta t | X &gt; t)}{\delta t} =\lim_{\delta t\rightarrow 0+} \frac{P(X\in
 [t,t+\delta t] }{ P(X &gt; t) \delta t}.$$
<p></p>

<p>
<b>EXAMPLE 6:</b>&nbsp;Let $X$  have density 
$$f(x) = \left\{\begin{array}{ll}e^{-x}&\text{if }x&gt;0\\ 0&\text{otherwise.}\end{array}\right.. $$
Find its hazard rate function.
<p></p>
<b>SOLUTION:</b>
$F(x) = \left\{\begin{array}{ll}0&\text{if }x\leq 0\\1-e^{-x}&\text{if }x&gt; 0\\\end{array}\right..$  So 
$$h(t) = \frac{e^{-t}}{1-(1-e^{-t})} = 1.$$
 ■
</p>

<h2><a
name="Problem set 9">Problem set 9</a></h2>
::<p>
<b>EXERCISE 38:</b>&nbsp;<font size="-2">[rosspdf28.png]</font><img width="" src="image/rosspdf28.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 39:</b>&nbsp;<font size="-2">[rosspdf29.png]</font><img width="" src="image/rosspdf29.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 40:</b>&nbsp;<font size="-2">[rosspdf39.png]</font><img width="" src="image/rosspdf39.png" style="vertical-align:text-top;">
<p></p>
You may like to use the result $\int_0^\infty e^{-x^2/2}, dx = \sqrt{\frac \pi2}.$
</p>
For the next three problems you need to work out the distribution from a given hazard function. Use $f = F'$  to turn
 the definition of hazard function into a differential equation:
$$\frac{F'(x)}{1-F(x)} = h(x),$$
and then solve it. Find the value for the constant of integration by using the fact that $F(x)\rightarrow 0$  as $x\rightarrow -\infty.$
<p></p>
::<p>
<b>EXERCISE 41:</b>&nbsp;<font size="-2">[rosspdf17.png]</font><img width="" src="image/rosspdf17.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 42:</b>&nbsp;<font size="-2">[rosspdf18.png]</font><img width="" src="image/rosspdf18.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 43:</b>&nbsp;<font size="-2">[rosspdf40.png]</font><img width="" src="image/rosspdf40.png" style="vertical-align:text-top;"></p>

<h1><a
name="Uniform distribution">Uniform distribution</a></h1>
<a href="https://youtu.be/PKqSKFayRZ0">Video for this section</a>
<p></p>
Last semester we often worked wih the discrete uniform distribution, <i>e.g.</i>, fair coin toss, fair die roll, drawing a card
 from a well-shuffled deck, picking a ball at random, etc. For the continuous case, the analog is
 the <b><font color="red" size="40">uniform distribution</font></b>. 
<fieldset>
<legend><b>Definition: Uniform distribution</b></legend>
Let $A\subseteq {\mathbb R}$ (${\mathbb R}^2$  or ${\mathbb R}^3$) have finite length (area or volume), $|A|.$   We say
 that $X$  has uniform
 distribution over $A$  if for any $B\subseteq A$  with length (area or volume) $|B|$  we have
$$P(X\in B) = \frac{ |B| }{ |A| }.$$
</fieldset> 

<p>
<b>EXAMPLE 7:</b>&nbsp;Let $Q$  be chosen randomly from the unit square in ${\mathbb R}^2.$  If
 $Q\equiv(X,Y)$, then find $P(X&lt; Y).$<p></p>
<b>SOLUTION:</b>
Look at the following diagram. 
<center>
<table width="100%">
<tr>
<th><img width="" src="image/unifsq.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Here $|A| = 1$  and $|B| = \frac 12.$  So $P(X &lt; Y) = P( (X,Y)\in B) = \frac{ |B| }{ |A| } = \frac 12.$
 ■
</p>

<p></p>

<p>
<b>EXAMPLE 8:</b>&nbsp;Let $X$  be uniformly distirbuted over $[0,1].$  Then find 
<ol>

<li>$P(X\leq -1)$</li>

<li>$P(X\leq 0)$</li>

<li>$P\left(X\leq \frac 12\right)$</li>

<li>$P(X\leq 1)$</li>

<li>$P(X\leq 2)$</li>

</ol>
Sketch the CDF of $X.$  Does $X$  have a PDF?
<p></p>
<b>SOLUTION:</b>

<ol>

<li>$P(X\leq -1)=0,$</li>

<li>$P(X\leq 0)=0,$</li>

<li>$P\left(X\leq \frac 12\right)=\frac 12,$</li>

<li>$P(X\leq 1)=1,$</li>

<li>$P(X\leq 2)=1.$</li>

</ol>

<center>
<table width="100%">
<tr>
<th><img width="" src="image/unifcdf.png"></th>
</tr>
<tr>
<th>Sketch of the CDF</th>
</tr>
</table>
</center>
Yes, by differentiating it, we get a PDF 
$$f(x) = \left\{\begin{array}{ll}1&\text{if }x\in(0,1)\\ 0&\text{otherwise.}\end{array}\right.. $$
We could replace $(0,1)$  by $[0,1]$  or $(0,1]$  or $[1,0).$
 ■
</p>

<h2><a
name="Problem set 10">Problem set 10</a></h2>
::<p>
<b>EXERCISE 44:</b>&nbsp;<font size="-2">[hpspdf2.png]</font><img width="" src="image/hpspdf2.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 45:</b>&nbsp;<font size="-2">[hpspdf3.png]</font><img width="" src="image/hpspdf3.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 46:</b>&nbsp;<font size="-2">[hpspdf4.png]</font><img width="" src="image/hpspdf4.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 47:</b>&nbsp;<font size="-2">[hpspdf5.png]</font><img width="" src="image/hpspdf5.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 48:</b>&nbsp;<font size="-2">[hpspdf6.png]</font><img width="" src="image/hpspdf6.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 49:</b>&nbsp;<font size="-2">[hpspdf12.png]</font><img width="" src="image/hpspdf12.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 50:</b>&nbsp;<font size="-2">[hpspdf26.png]</font><img width="" src="image/hpspdf26.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 51:</b>&nbsp;<font size="-2">[rosspdf37.png]</font><img width="" src="image/rosspdf37.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 52:</b>&nbsp;<font size="-2">[rosspdf38.png]</font><img width="" src="image/rosspdf38.png" style="vertical-align:text-top;"></p>

<p></p>
::<p>
<b>EXERCISE 53:</b>&nbsp;<font size="-2">[rosspdf11.png]</font><img width="" src="image/rosspdf11.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 54:</b>&nbsp;<font size="-2">[rosspdf12.png]</font><img width="" src="image/rosspdf12.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 55:</b>&nbsp;<font size="-2">[rosspdf13.png]</font><img width="" src="image/rosspdf13.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 56:</b>&nbsp;<font size="-2">[rosspdf14.png]</font><img width="" src="image/rosspdf14.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 57:</b>&nbsp;<font size="-2">[rosspdf16.png]</font><img width="" src="image/rosspdf16.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 58:</b>&nbsp;<font size="-2">[rosspdf20.png]</font><img width="" src="image/rosspdf20.png" style="vertical-align:text-top;"></p>

<p></p>

<h1><a
name="Miscellaneous">Miscellaneous</a></h1>

<h2><a
name="Problem set 11">Problem set 11</a></h2>

<p></p>
::<p>
<b>EXERCISE 59:</b>&nbsp;<font size="-2">[rosspdf22.png]</font><img width="" src="image/rosspdf22.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 60:</b>&nbsp;<font size="-2">[rosspdf25.png]</font><img width="" src="image/rosspdf25.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 61:</b>&nbsp;<font size="-2">[rosspdf37.png]</font><img width="" src="image/rosspdf37.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 62:</b>&nbsp;<font size="-2">[rosspdf38.png]</font><img width="" src="image/rosspdf38.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 63:</b>&nbsp;<font size="-2">[rosspdf36.png]</font><img width="" src="image/rosspdf36.png" style="vertical-align:text-top;"></p>

<p></p>
<hr xmlns="http://www.w3.org/1999/xhtml"/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/></body></html>
