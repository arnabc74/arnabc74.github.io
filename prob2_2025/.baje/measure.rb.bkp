 @{<NOTE>
<M>
\newcommand{\calF}{{\mathcal F}}
\newcommand{\calB}{{\mathcal B}}
\newcommand{\ind}{{\mathbb 1}}
</M>
<HEAD1>Uncountable sample space</HEAD1>
We have already seen last semester that for an uncountable <M>\Omega</M>  we may not always be able
 to define a function <M>P:{\mathcal P}(\Omega)\to[0,1]</M> 
 satisfying all the probability axioms.  Please see <LINK to="../prob1_2024/vitali.html">this page
 from the last semester</LINK>  to brush up on this. 

Hence we defined <M>\sigma</M>-algebra. While the simplest <M>\sigma</M>-algebras are the trivial one and the entore power
 set, the most commonly used is the Borel <M>\sigma</M>-algebra. 

Closely related to this is the idea of a measurable function.
<HEAD2>Measurable function</HEAD2>
Let <M>(\Omega_1,\calF_1)</M>  and <M>(\Omega_2,\calF_2)</M>  be measurable spaces. Then a function <M>f:\Omega_1\to\Omega_2</M> 
 is called <TERM>measurable</TERM>  if 
<D>\forall B\in\calF_2~~f ^{-1} (B)\in \calF_1.</D>
The most common application of this our course is when <M>(\Omega,\calF,P)</M>  is a probability
 space (i.e., a random experiment) and <M>X:\Omega\to\rr</M>  is a random variable. Here we take
 <M>(\Omega_1,\calF_1) = (\Omega,\calF)</M>  and <M>(\Omega_2,\calF_2) = (\rr,\calB),</M>  where
 <M>\calB</M>  is the Borel sigma-field on <M>\rr.</M>

<DEFN>
By a <TERM>random variable</TERM>  on a probability space <M>(\Omega,\calF,P)</M>  we mean a measurable function <M>X</M> 
 from <M>(\Omega,\calF)</M>  to <M>(\rr,\calB).</M>  Note that <M>P</M>  plays no role in the definition. 
</DEFN> 
We need the measurability condition on <M>X</M>  so that we can talk about <M>P(X\in (a,b)).</M>  For this we need 
<M>\{w\in\Omega~:~X(w)\in (a,b)\}\equiv X ^{-1} (a,b)\in \calF.</M>
<HEAD2>Simple funtion to approx measurable function</HEAD2>
While defining <M>E(X)</M>  we had proceeded in three steps: simple, non-negative and general. We took a supremum in the
 second step. This is motivated by the following result. 

<THM>
If <M>f:\Omega\to[0,\infty)</M>  is any function, then there is a non-decreasing sequence <M>(f_n)</M>  where each
 <M>f_n:\Omega\to [0,\infty)</M>  and 
<D>\forall \omega\in\Omega~~f_n(\omega) \uparrow f(\omega).</D> 
</THM>
<PF>
For <M>n\in\nn</M>  and <M>\omega\in\Omega</M>  we define <M>f_n</M>  as follows. First partition
 <M>[0,\infty)</M>  into <M>2</M>  intervals <M>[0,n)</M>  and <M>[n,\infty)</M>  and then
 subdivide the first into equal subintervals of length <M>2^{-n}.</M>  So you get <M>N=n2^n+1</M>  subintervals in all. Call
 these <M>[a_1,b_1),...,[a_N,b_N).</M>  These constitute a partition of <M>[0,\infty).</M>

Now  set <M>f_n(\omega) = a_k</M>  if <M>f_n(\omega) \in[ a_k,b_k).</M>  

The following picture shows this process for <M>n=1</M>  and <M>n=2.</M>
<CIMG web="subdiv.png">Notice how the subdivisions for <M>n=2</M>  fit into those for <M>n=1.</M></CIMG>     

For each <M>\omega\in\Omega</M>  and for each <M>n\in\nn</M>  we have <M>f_n(\omega)\leq f_{n+1}(\omega).</M>
<BECAUSE>
If <M>f_n(\omega) = a</M>   and <M>f_{n+1}(\omega) = b,</M>  then <M>f(\omega)\in[a+2^{-n})</M>  and also <M>f(\omega)\in[b+2^{-n-1}).</M> 
 
So, by the contruction of the partitions, <M>[b+2^{-n-1})\seq[a,2^{-n}).</M>

Thus, <M>a\leq b,</M>  as required.
</BECAUSE>

Again, for each <M>\omega\in\Omega</M>  we have <M>f_n(\omega)\to f(\omega).</M>  
<BECAUSE>
To show:

<TGT>\forall \omega\in\Omega~~\forall \epsilon>0~~\exists M\in\nn~~\forall n\geq M ~~|f(\omega)-f_n(\omega)| < \epsilon.</TGT>

<FLL>\omega</FLL> Take any <M>\omega\in\Omega.</M>

<FLL>\epsilon</FLL> Take any <M>\epsilon>0.</M>

<EXS>M</EXS> Choose <M>M\in\nn</M>  such that <M>M> f(\omega)</M>  and <M>2^{-M} < \epsilon.</M>  (Possible since <M>\nn</M> 
 is unbounded above and <M>2^{-n}\to 0</M>  as <M>n\to \infty.</M>

<FLL>n</FLL> Take any <M>n\geq M.</M>

<CHK/>Since <M>f(\omega) < M\leq n,</M>  hence <M>f_n(\omega) < n.</M>  

Thus, <M>f(\omega) \in [f_n(\omega),f_n(\omega)+2^{-n}).</M>
</BECAUSE>
This completes the proof.
</PF>

<EXR>
Show that the convergence is uniform if <M>f</M>  is bounded.
</EXR>

<EXR>
Show that if, in the theorem above,  <M>f</M>  is measurable (w.r.t. any given <M>\sigma</M>-field <M>\calF</M>
 over <M>\Omega</M>  and the Borel <M>\sigma</M>-field over <M>\rr</M>), then so must be each <M>f_n.</M> 
<ANS>
Since <M>f_n</M>  can take only finitely many value, it is enough (why?) to show that for any <M>a\in\rr</M>  the set <M>f_n
 ^{-1}\{a\}\in\calF.</M>   

Express <M>f_n ^{-1}\{a\}</M>  as <M>f ^{-1}</M>(some set). 
</ANS>
</EXR>
<THM>
If <M>X</M>  is a non-negative random variable, and <M>X_n</M>'s are simple random variables with <M>X_n\uparrow X,</M> 
 then <M>E(X_n)\uparrow E(X).</M>
</THM>
<PF>
Shall show

<TGT>\forall \epsilon>0~~\exists N\in\nn~~\forall n\geq N~~ E(X_n)> E(X)-\epsilon.</TGT>

This will complete the proof, since anyway <M>(E(X_n))</M>  is a non-decreasing sequence bounded from above by <M>E(X)</M> 
  (The case <M>E(X)=\infty</M>  is trivially
 included in it). 

Since <M>E(X) = \sup\{E(Z)~:~  Z\leq X,~~Z \mbox{ simple}\},</M>

hence <M>\exists</M> simple <M> Z\leq X</M>  with <M>E(Z) > E(X)-\epsilon.</M> 


Fix some <M>\delta>0.</M>

Let <M>A_n =\{X_n > Z-\delta\}.</M>

Then <M>P(A_n)\to 1.</M>
<BECAUSE>
Since <M>X_n</M>'s are non-decreasing, hence <M>A_1\seq A_2\seq A_3\seq\cdots.</M>  

Also since <M>\forall\omega\in\Omega~~X_n(w)\uparrow X(w),</M>  hence <M>\cup_n A_n=\Omega.</M>  
</BECAUSE>  

So <M>E(X_n)\geq E(X_n\ind_{A_n}) \geq E(Z\ind_{A_n}) \geq E(Z)-MP(A_n^c)-\delta,</M>
where <M>M = \max Z.</M>  

Taking limit <M>\lim E(X_n) \geq Z- \delta.</M>  

Since <M>\delta>0</M>  is arbitrary, we have <M>\lim E(X_n) \geq E(Z).</M>  
</PF>
<HEAD2>Lebesgue integral</HEAD2>

<HEAD3>Additivity</HEAD3>
<HEAD3>MCT</HEAD3>
<THM>
Let <M>X_n</M> 's be non-negative simple random variables with <M>X_n\uparrow X</M>  for some random variable <M>X.</M>
 Then <M>E(X_n)\uparrow E(X).</M>
</THM>
<PF>
Enough to show simple random variables <M>Y_n</M>  such that <M>Y_n\uparrow X </M> and <M>Y_n\leq X_n.</M>
<BECAUSE>
We already know <M>E(Y_n)\uparrow E(X).</M>  But <M>E(X_n)</M>  is sandwiched between <M>E(Y_n)</M>  and <M>E(X).</M>
</BECAUSE>
Let <M>(Z_{n,k})_k</M>  be the simplification of <M>X_n.</M>  

Let <M>Y_n = \max\{Z_{1,n},...,Z_{n,n}\}.</M>

Then <M>Y_1\leq Y_2\leq\cdots</M>
<BECAUSE>
<MULTILINE>
Y_{n+1} & = & \max\{Z_{1,n+1},...,Z_{n+1,n+1}\}\\
& \geq & \max\{Z_{1,n+1},...,Z_{n,n+1}\}<SINCE><M>\because</M> superset cannot have smaller max</SINCE>\\
& \geq & \max\{Z_{1,n},...,Z_{n,n}\},
</MULTILINE>
by non-decreasing property of <M>Z_{n,k}</M>  w.r.t. <M>k.</M>
</BECAUSE>
Also <M>Y_n\leq X_n.</M>
<BECAUSE>
<M>Z_{k,n}\leq X_k\leq X_n.</M>
</BECAUSE>
Finally, <M>Y_n\uparrow X.</M>
<BECAUSE>
We have <M>Z_{n,k} \leq Y_k.</M>  

Taking limit as <M>k\to \infty,</M>  we have <M>X_n\leq \lim_k Y_k.</M>

Now taking limit as <M>n\to \infty,</M>  we have <M>X\leq \lim_k Y_k.</M>  

Also we have <M>Y_n\leq X_n\leq X.</M>  So <M>\lim_k Y_k\leq X.</M>  

Hence <M>\lim_k Y_k= X.</M>
</BECAUSE>
This completes the proof.
</PF>
<HEAD3>BCT and DCT</HEAD3>

<HEAD2>Special cases</HEAD2>
We have defined <M>E(X)</M>  in three steps: simple, non-negative and general. But we have given a computational formula
 only in case of simple random variables. If <M>X</M>  takes countably infinite values, <M>x_1,x_2,...</M>  with probabilities
 <M>p_1,p_2,...,</M>  respectively, then we have mentioned that <M>E(X) = \sum_n x_n p_n</M>  if this sum is absolutely convergent.
 This formula actually follows from the general definition, as we now show.

<HEAD3>Countable case</HEAD3>
<THM>If <M>X</M>  takes the nonnegative values <M>x_1<x_2<\cdots</M>   with probabilities
 <M>p_1,p_2,...</M>  where <M>\sum p_i = 1,</M>  then 
<D>E(X) = \sum p_i x_i.</D>
</THM>
<PF>
To show 
<D>\sum p_i x_i = \sup\{E(U)~:~U\mbox{ simple, }U\leq X\}.</D>
Let <M>L= \sum_i p_i x_i,</M>  and <M>{\mathcal D}=\{E(U)~:~U\mbox{ simple, }U\leq X\}.</M>

This requires showing two things: 
<UL><LI><M>L</M>  is an upper bound of <M>{\mathcal D},</M></LI>
<LI>no number less than <M>L</M>  is an upper bound of <M>{\mathcal D}.</M></LI></UL>

<B>Step 1:</B>  To show

<D>\forall U\in{\mathcal D}~~E(U)\leq L.</D>

Take any  <M>U\in{\mathcal D}</M>  be any simple random variable. 

Let <M>U</M>  take only the values <M>u_1,...,u_k.</M>  

Let <M>p_{ij} = P(X=x_i, U=u_j).</M>

Then <M>E(U) =\sum_j (u_j \sum_i p_{ij}) = \sum_j\sum_i u_j p_{ij}.</M>  

Also <M>L = \sum_i x_i \sum_j
 p_{ij}=\sum_i  \sum_j x_i p_{ij}=\sum_j \sum_i x_i p_{ij}.</M>
<BECAUSE>
A finite sum can always be interchanged with an infinite sum, when the summands are all nonnegative. For example,
<D>\sum (a_n+b_n) = \sum a_n + \sum b_n.</D>
If we write <M>c_{1,n}=a_n</M>  and <M> c_{2,n}=b_n</M>  then this is 
<D>\sum_n \sum_i c_{i,n} = \sum_i \sum_n c_{i,n}.</D>  
</BECAUSE>
Now <M>p_{ij}>0\implies u_j\leq x_i.</M>  

Hence <M>\sum_i   u_j p_{ij}\leq \sum_i   x_i p_{ij},</M>  and so <M>\sum_j\sum_i   u_j p_{ij}\leq \sum_j\sum_i   x_i p_{ij}.</M>

Thus, <M>E(U)\leq L,</M>  as required.

<B>Step 2:</B>  Shall show that no <M>L'< L</M>  is an upper bound of <M>{\mathcal D},</M>  i.e.,

<D>\forall L'< L~~\exists U\in{\mathcal D}~~E(U)> L'.</D>  

Let <M>U_n</M>  be the random variable 
<D>
U_n =<CASES>X<IF>X=x_1,...,x_n</IF> 0<ELSE/></CASES>.
</D>  
Then <M>U_n</M>  is a simple random variable such that <M>U_n\leq X.</M> 

So <M>U_n\in{\mathcal D}.</M>
 
Also <M>E(U_n)
 =\sum_{i=1}^n p_i x_i\to L.</M>  

Hence <M>\exists N\in\nn~~E(U_N) > L'.</M>  

Choose this <M>U_N</M>  as our <M>U\in{\mathcal D}.</M>

Since <M>E(U) > L',</M> this completes the proof.
 </PF>


::<EXR>If <M>X</M>  takes the  values <M>x_1,x_2,...</M>  (not necessarily all nonnegative) with probabilities
 <M>p_1,p_2,...</M>  where <M>\sum p_i = 1</M> and <M>\sum |p_i x_i|<\infty,</M> then 
<D>E(X) = \sum p_i x_i.</D>
</EXR>
::<EXR>If <M>X</M>  takes the  values <M>x_1,x_2,...</M>  (not necessarily all nonnegative) with probabilities
 <M>p_1,p_2,...</M>  where <M>\sum p_i = 1</M> and <M>\sum |p_i x_i|=\infty,</M> then what are the possibilities for <M>E(X):</M> 
 finite, <M>\infty</M>, <M>-\infty</M>  or undefined? Give one example of each of the possibilities. Prove the impossibility
 of the other(s).
</EXR>

<HEAD3>Riemann vs Lebesgue</HEAD3>
If a function is Riemann integrable (proper or improper), then it is also Lebesgue integrable, and the two integrals match. Indeed,
 that is why we could use Riemann integraion to compute expectation in the absolutely continuous case.

<HEAD2>Existence of <M>Unif(0,1)</M></HEAD2>
We had talked about the fundamental theorem of probability in the last semester: for any CDF there is a random variable with
 that CDF. Indeed, this is the therem that powers every statement of the form "Let <M>X</M>  be a random variable with a
 given distribution." 

We had given a partial proof earlier in this course: We assumed the existence of a random variable with the <M>Unif(0,1)</M> 
 distribution, and provided a way to manufacture random variables following any given distribution using this.

So the only thing that remains to be checked is the existence of <M>Unif(0,1)</M>  random variables. 
For this we take <M>\Omega = (0,1)</M>  and <M>X:\Omega\to\rr</M>  as <M>X(\omega)=\omega.</M>  We take the Borel <M>\sigma</M>-field
 on <M>\Omega.</M>  Need to show the existence of <M>P:\calB\to[0,1]</M>  such that <M>\forall a<b\in(0,1)~~P(a,b) = b-a.</M>

This proof turns out to be surprisingly tricky, and will be not be dealt with in this course.

</NOTE>@}
