 @{<NOTE>
<TITLE>Some more standard densities</TITLE>

<HEAD1>Beta function</HEAD1>
The next distribution that we shall discuss is the Beta distribution. Just as we needed to know about the Gamma function
 in order to talk about the Gamma distribution, we need to know about the Beta function before we can get into Beta distribution.

<DEFN name="Beta function">
The Beta function is the function <M>B:(0,\infty)\times(0,\infty)\to\rr</M>  defined as
<D>B(a,b) = \int_0^1 x^{a-1}(1-x)^{b-1}\, dx\mbox{ for } a,b>0.</D>
</DEFN>
Notice that if <M>a,b\geq 1,</M>  then the integrand is continuous, and so integrable over <M>[0,1].</M>  If, however, <M>a</M> 
 or <M>b</M>  lies in <M>(0,1)</M>, then the integral is am improper one, and hence we have to worry about its existence.
 Fortunately, it is easy to establish convergence by comparison with <M>\int_0^1 x^\alpha\, dx</M>  for <M>\alpha>-1.</M> 

Most manipulations with the Beta function uses the following theorem to reduce the Beta function to the Gamma function:
<THM>
For <M>a,b>0</M>  we have 
<D>B(a,b) = [[\Gamma(a)\Gamma(b)][\Gamma(a+b)]].</D>
</THM> 
<PF>The proof is quite easy using a bivariate change of variable. However, since that result will
 be proved in Analysis III, we shall omit the proof of this theorem here.</PF>

<HEAD2>Problem set <PS/></HEAD2>
<EXR>Find <M>B(4,5).</M>  Remember that <M>\Gamma(n) = (n-1)!</M>  for <M>n\in\nn.</M></EXR>

<EXR>Writing the factorials in terms of the Gamma function, express <M>\binom{10}{6}</M>  in terms of the Beta function</EXR>

<HEAD1>Beta distribution</HEAD1>
Now that we see that for <M>a,b>0</M>  the function <M>x^{a-1}(1-x)^{b-1}</M>  is  nonnegative and integrable 
over  <M>(0,1)</M>  with integral <M>B(a,b),</M> 
 we can immediately manufacture a density out of it:
<DEFN name="Beta distribution"> 
The distribution with density
<D>f(x) = <CASES>[[1][B(a,b)]] x^{a-1}(1-x)^{b-1}<IF>x\in(0,1)</IF> 0<ELSE/></CASES></D>
for <M>a,b>0</M>  is called the <TERM>Beta distribution with parameters</TERM>  <M>a,b.</M>  
</DEFN>


The beta densities show a wide variety of shapes. 
<CIMG web="betadens.png">A variety of shapes from the Beta family</CIMG>
<COMMENT>
svg("image/betadensraw.svg")
x = seq(0,1,len=100)
y1 = dbeta(x,4,5)
y2 = dbeta(x,0.5,5)
y3 = dbeta(x,4,0.5)
y4 = dbeta(x,4,1)
bareplot(x,y1,t='l',ylim=range(-0.1,5),xlim=c(-0.1,1.1))
lines(x,y2)
lines(x,y3)
lines(x,y4)
abline(h=0,v=0:1)
dev.off()
</COMMENT>

<THM>
If <M>X\sim Beta(a,b)</M>  then <M>E(X) = [[a][a+b]].</M>
</THM>
<PF>
<D>E(X) = [[1][B(a,b)]]\int_0^1 x\times x^{a-1} (1-x)^{b-1}\, dx = [[1][B(a,b)]]\int_0^1  x^{(a+1)-1} (1-x)^{b-1}\, dx = [[B(a+1,b)][B(a,b)]].</D>
Now we shall express the Beta functions in terms of the Gamma function to get
<D>[[B(a+1,b)][B(a,b)]] = [[\Gamma(a+1)\Gamma(b)][\Gamma(a+b+1)]]\times[[\Gamma(a+b)][\Gamma(a)\Gamma(b)]].</D>
We know that <M>\forall \alpha >0~~\Gamma(\alpha+1) = \alpha \Gamma(\alpha).</M>
Hence <M>[[\Gamma(a+1)][\Gamma(a)]]= a</M>  and <M>[[\Gamma(a+b)][\Gamma(a+b+1)]]=[[1][a+b]].</M>
Hence the result.
</PF>

<HEAD2>Problem set <PS/></HEAD2>
<EXR>For particular values of <M>a,b</M>  we get the <M>Unif(0,1)</M>  distribution. Which values?</EXR>
<EXR>
If <M>X\sim Beta(a,b)</M>, then exactly one of the two statements is correct in general. Which one?
<HL>
<LI><M>E(X) = [[a][a+b]].</M></LI>
<LI><M>E(X) = [[b][a+b]].</M></LI>
</HL>
Answer by thinking about the density. Now prove it mathematically.
</EXR>

<EXR>If <M>X\sim Beta(a,b)</M>, then find <M>V(X).</M></EXR>

<EXR>If <M>X\sim Beta(a,b)</M>  then show that <M>1-X\sim Beta(b,a).</M></EXR>
<HEAD1>Bayesian machine learning</HEAD1>
The main use of the beta distribution in statistics is as a "distribution of probability". To
 understand this, consider the large collection of coins, both biased and unbiased. If you pick
 one of them at random and let <M>\Pi</M>  denote its <M>P(head),</M>  then <M>\Pi</M>  is a
 random variable, just as the height of a random person is considered a random variable. Now
 <M>\Pi</M>, being a probability, must have a distribution supported  on <M>[0,1].</M>  Also,
 since <M>\Pi</M>  depends on various physical properties of the coin, it is natural to expect
 that it will have a density. The members of the Beta family   are suitable choices. 
This was a toy example. More often we have some event and we ask for expert opinion about its probability. Then <M>\Pi</M> 
 denotes the opinion of a randomly selected expert. If there is a peak in its density, then that's the value most experts
 vote for. In this sense, <M>Beta(1,1)\equiv Unif(0,1)</M>  denotes absolute ignorance.

As a warm up towards the use of Beta distribution plays a role in Bayesian machine
 learning, consider the simple set up from last semester. We have a box containing 3 white and 2 black balls. An SRSWOR of
 size 3 is drawn from it, and transferred to a new box. Now an SRSWR of size 5 is drawn from the second box. If all the balls
 in the SRSWR are found to be black, then what is the (conditional) probability that all the balls
 in the first SRSWOR were black?
 I hope you remember the following diagrammatic representation for this:
<CIMG web="bayesdiag.png">Diagram for the above problem</CIMG>
There are four paths. The <M>i</M>-th path has probability <M>p_iq_i</M>  for <M>i=0,1,2,3.</M>  The required probability
 is <M>[[p_3q_3][\sum_i p_iq_i]].</M>

You may think of the first stage of this experiment as "constructing a random coin" whose <M>P(head)</M>  is <M>0</M>  or
 <M>[[13]]</M>  or <M>[[23]]</M>  or <M>1.</M>  These have probabilities <M>p_0,...,p_3,</M>  respectively. The second stage
 is tossing the coin 5 times. Given that we get 5 heads, we want to find the probability that our coin has <M>P(head)=1.</M>

In this toy example, our "random coin" could have only finitely many possible <M>P(head)</M>'s. Now let's make it more realistic.
 We shall start by picking a random coin from th population of all coins. Let <M>\Pi</M>  denote its <M>P(head).</M>  Just
 as the height of a randomly selected person is a random variable, similarly this <M>\Pi</M>  is a random variable. 
It can takes values in <M>[0,1].</M>  Let us assume that <M>\Pi\sim Beta(1,1)\equiv Unif(0,1).</M>  This means we consider
 any value of <M>\Pi</M>  in <M>[0,1]</M>  equally likely (this is one way to formalising "absolute ignorance"). 
We shall
now toss this coin 5 times. Given that all the tosses result in heads, what is the conditional distribution
 of <M>\Pi?</M>  We had started with a <M>Beta(1,1)</M>  distribution. This is often called the <TERM>prior</TERM>  distribution,
 as it represented the state of our knowledge about <M>\Pi</M>  <I>prior</I>  to collection of data. In this sense, the conditional
 distribution we are seeking is the <TERM>posterior</TERM>  distribution, which represents our state of knowledge after seeing
the data (the outcomes of the coin tosses). We plan to draw a diagram similar to the earlier one for this problem. But here
 the random coin can have any <M>P(head)</M>  in <M>[0,1].</M>  So we have a contnuum of arrows, which is hard to draw:
<CIMG web="bayesdiag2.png">Diagram with continuous sprays of arrows</CIMG>
The 1 labelling the left hand arrow is the density of the prior <M>Beta(1,1).</M>  The <M>p^5</M>  labelling the right hand
 arrow is the (binomial) probability of obtaining <M>5</M>  heads out of 5 tosses of a coin with <M>P(head)=p.</M>  
Following the Bayes theorem idea as before we get the posterior density 
<D>[[1\times p^5][\int_0^1 1\times p^5\, dp]]=6p^5,</D>
which is the <M>Beta(6,1)</M>  density. 

It is instructive to draw the two densities side by side to understand how our state of belief has been updated in light
 of the data:
<CIMG web="priopost1.png">After seeing the data our belief is concentrated more near 1.</CIMG>
In this way, whenever the prior is from the Beta family, our data is <M>X|\Pi=p\sim Binom(n,p)</M>,  the
 posterior will also be from the
 same family. We express this by saying:
<Q>
Beta is the conjugate prior family for
</Q>
An exercise below
 asks you to prove this. For now let us toss the same coin 5 more times. Suppose now we get 4 tails and 1 head. If we carry
 out the same exercise again (but this time with <M>Beta(6,1)</M>  playing the role of the prior, and "1 head, 4 tails"
 as our data), we again get a Beta posterior that we plot below:
<CIMG web="priopost2.png">Our belief is now peaked closer to the centre.</CIMG>
In this way the Beta family can express our ever-changing state of belief as more and more data stream in.

<HEAD2>Problem set <PS/></HEAD2>
<EXR>Find the posterior for <M>\Pi</M>  if the prior is <M>Beta(6,1)</M>  and the data consist of
 1 head and 4 tails out of 5
 independent tosses of the coin. The answer should be a Beta distribution with parameters that you are determine.</EXR> 

<EXR>Show that if  <M>\Pi</M>  has prior <M>Beta(a,b)</M>  and our data consist of exactly
 <M>X</M>  heads out of <M>n</M>  tosses, then the posterior is again a Beta distribution. What are its parameters?</EXR>

<EXR>Suppose that we have coin with <M>P(head)</M>  having prior <M>Unif(0,1).</M>  We toss the
 coin <M>n</M>  times independently and obtain exactly <M>X</M>  heads. Let <M>f(p)</M>  be the
 (continuous) density of the posterior. It is natural to estimate <M>p</M>  using the value where
 <M>f</M>  is the maximum. This is called the (maximum a posteriori) MAP estimator. Derive its
 formula in terms of <M>n</M>  and <M>X.</M> Is it the same as the "usual" estimator <M>[[Xn]]?</M></EXR>
<HEAD1>Cauchy distribution</HEAD1>
<DEFN name="Cauchy distribution">
By <TERM>Cauchy distribution</TERM>  we mean the distribution with density
<D>f(x) = [[1][\pi(1+x^2)]],\quad x\in\rr.</D>
Sometimes we also talk about <M>Cauchy(\mu,\sigma)</M>  distribution which is the distribution of <M>\mu+\sigma X</M>, where
 <M>X</M>  has the above density. In this notation, the density corresponds to the <M>Cauchy(0,1)</M>  distribution. 
</DEFN>

The most important reason for including this distribution in our discussion is that it has one notoriously bad property. It
 does not have any finite moment!  In particular, do not think that <M>Cauchy(\mu,\sigma)</M>  has
 mean <M>\mu</M>  and variance <M>\sigma^2.</M>   

<THM>
If <M>X</M>  has <M>Cauchy</M>  distribution, then <M>E(X)</M>  does not exist. As a result <M>E(X^n)</M>  does not exist
 for any <M>n\in\nn.</M>  
</THM>
<PF>
<M>\int_0^\infty [[x][1+x^2]]dx\sim \int_0^\infty [[1x]]dx = \infty.</M>
</PF>

<THM>
The characteristic function of the Cauchy distribution is <M>e^{-|t|}.</M>
</THM>
<PF>Needs techniques (complex contour integration/differentiation under intergal) beyond the present level.</PF>
<HEAD2>Cauchy defines the laws of large numbers</HEAD2>
The following R snippet will plot the running means of 10000 IID Cauchy random variables. The plot does not converge.
<R>
n = 10000
x= rcauchy(n)
y=cumsum(x)/(1:n)
plot(y,ty='l')
</R>
This demonstration is theoretically justified in the exercise below.
<HEAD2>Problem set <PS/></HEAD2>
<EXR>Show that if <M>X</M>  and <M>Y</M>  are independent Cauchy random variables, then for any <M>a\in[0,1]</M>  
 <M>aX+(1-a)Y</M>  is also a Cauchy variate. Hence conclude that if <M>X_1,...,X_n</M>  are IID
 Cauchy, then <M>\overline X</M>  is also Cauchy.</EXR>

<EXR>How can you generate a Cauchy random variable from a <M>Unif(0,1)</M>  random variable?</EXR>

<EXR>Consider the unit semicircle shown below.
<CIMG web="caupt.png"></CIMG>
 We pick a point at random on it, and extend the ray
  through it from the origin until it hits the <M>x=1</M>  line at some <M>(1,Y).</M>  Find the distribution of <M>Y.</M></EXR>

<EXR>If <M>X</M>  is a Cauchy random variable, then show that <M>[[1X]]</M>  is also a Cauchy random variable.</EXR>

<HEAD1>Normal distribution</HEAD1>
This is the most commonly used distribution in statistics.

<DEFN name="Normal distribution">
We say that <M>X\sim N(\mu,\sigma^2)</M>  to mean <M>X</M>  has density 
<D>\phi(x) = [[1][\sqrt{2\pi\sigma^2}]] \exp(*(-[[(x-\mu)^2][\sigma^2]] )*),\quad x\in\rr.</D>
</DEFN>
The density looks like the following: 
<CIMG web="normdens.png"><M>\mu</M>  controls centre, <M>\sigma</M>  controls spread</CIMG>
<COMMENT>
svg("image/normdensraw.svg")
x = seq(-5,5,len=1001)
y1 = dnorm(x,0,1)
y2 = dnorm(x,2,1)
y3 = dnorm(x,0,0.5)
bareplot(x,y1,t='l',ylim=range(-0.1,y1,y2,y3))
lines(x,y2)
lines(x,y3)
abline(h=0,v=c(0,2))
dev.off()
</COMMENT>
Proving that this is indeed a density is not entirely straightforward. Howeer, we may reduce
 the problem to a Gamma integral and use the following theorem.
<THM>
<M>\Gamma(*([[12]])*)= \sqrt\pi.</M>
</THM>
<PF>Omitted.</PF>
To use this result in order to show that the total integral of the <M>N(\mu,\sigma^2)</M>  density is indeed 1, we proceed
 as follows.
<D>\int_0^\infty e^{-x^2/2}\, dx= [[1][\sqrt2]]\int_0^\infty t^{-1/2} e^{-t} \, dt</D>
by using <M>t = x^2/2.</M>  This new integral is just 
<D>\int_0^\infty t^{[[12]]-1} e^{-t} \, dt = \Gamma(*([[12]])*) = \sqrt\pi.</D>
So
<D>\int_{-\infty}^\infty e^{-x^2/2}\, dx=2\int_0^\infty e^{-x^2/2}\, dx= \sqrt{2\pi}.</D>
Hence we have shown that the <M>N(0,1)</M>  density integrates up to 1. To prove for any general <M>N(\mu,\sigma^2)</M> 
 we simply use the substitution <M>y = [[x-\mu][\sigma]]</M>  to reduce it to the <M>N(0,1)</M>  case.

The letter lower case phi, <M>\phi</M>, is generally used for the <M>N(0,1)</M>  density, while its capital version <M>\Phi</M> 
 is reserved for the CDF.
<D>\Phi(x) = \int_{-\infty}^x \phi(t)\, dt</D>
Louiville showed that <M>\Phi(x)</M>  cannot be expressed in terms of elementary functions (trigonometric, exponential, logarithmic,
 square root, cube root etc).  However, its value may be computed numerically for any given <M>x.</M>  

<THM>
If <M>X\sim N(\mu,\sigma^2),</M>  then for any <M>a\in\rr</M>  and <M>b\neq 0</M>  we have
 <M>a+bX\sim N(a+b\mu,b^2 \sigma^2).</M>
</THM>
<PF>
Directly from Jacobian formula.
</PF>

A corollary is the following theorem.

<THM>
If <M>X\sim N(\mu,\sigma^2),</M>  then <M>[[X-\mu][\sigma]]\sim N(0,1).</M>
</THM>
The transformation from <M>X</M>  to <M>[[X-\mu][\sigma]]</M>  is called <TERM>standardisation</TERM>.
<THM>If <M>X\sim N(\mu,\sigma^2),</M>  then <M>E(X)=\mu</M>  and <M>V(X)=\sigma^2.</M></THM>
<PF>Easy, and left as an exercise. Just one reminder. As a first step you should substitute
 <M>y=[[x-\mu][\sigma]]</M>  to arrive at <M>N(0,1).</M>  Now the expectation is given by the integral 
<D>[[1][2\sqrt\pi]]\int_{-\infty}^\infty x e^{-x^2/2}\, dx.</D>
Don't rush to the conclusion that this must be zero, because the integrand is an odd function. Here you are working with
 an improper integral. So you need to make sure that <M>\int_0^\infty x e^{-x^2/2}\, dx</M>  is finite before you can use
 the odd function argument.
</PF>

<THM>
The characteristic function of <M>N(0,1)</M>  is <M>e^{-t^2/2}</M>  for <M>t\in\rr.</M>
</THM>
<PF>
As you have not formally done complex integration yet, all our characteristic function derivations are heuristic. 

Here we can show directly that for any  <M>s\in\rr</M>  we have <M>E(e^{sX}) = e^{s^2/2},</M>
 where <M>X\sim N(0,1).</M>   This is easily shown using a simple  substitution.  
 
Now, if you replace <M>s</M>  with <M>it,</M>  you get the result. This replacement is justified
 using arguments from complex analysis beyond the present scope. 
</PF>
<HEAD2>Problem set <PS/></HEAD2>
<EXR>If <M>X\sim N(0,1),</M>  then express the following probabilities in terms of <M>\Phi(\cdot).</M>  
<OL><LI><M>P(X<1)</M></LI>
<LI><M>P(|X|<1)</M></LI>
<LI><M>P(|X|>2)</M></LI>
</OL>
</EXR>
<EXR>If <M>X\sim N(2,3^2),</M>  then express the following probabilities in terms of <M>\Phi(\cdot).</M>  
<FL><LI><M>P(X<1)</M></LI>
<LI><M>P(|X|<1)</M></LI>
<LI><M>P(|X|>2)</M></LI>
</FL>
</EXR>

<EXR>If <M>\Phi ^{-1}(0.95)=1.64</M>, then find <M>c\in\rr</M>  such that <M>P(|X-1|>c) = 0.1 </M>  where <M>X\sim N(1,1^2).</M></EXR>

<HEAD1>Maxwell's derivation of the normal distribution</HEAD1>
Generally, Gauss is credited with the "discovery" of the normal distribution, which he derived via his famous central limit
 theorem. However, a lesser known derivation is due to James Clerk Maxwell, which we shall discuss now. 

Imagine a gas without any overall flow. We know that the molecules are moving tis way and that
 randomly. Maxwell was interested in
 finding the distribution of velocities. Of course, he had no way of grabbing a molecule and measuring its velocity. So he
 did a brilliant logical argument starting with little more than common sense notions available to anybody. 

His first step was to set up three axes <M>x</M> , <M>y</M>  and <M>z.</M>  Then the velocity could be expressed as <M>(V_1,V_2,V_3).</M> 
Since there is no overall flow and we have no reason to favour any one direction over the other hence <M>V_1,V_2,V_3</M>  must
 be IID. 

Now we make our first not-so-common-sense assumption: each <M>V_i</M>   has a density, say <M>f.</M> 
Then the joint density of <M>(V_1,V_2,V_3)</M>  is  <M>f(v_1)f(v_2)f(v_3).</M>  Surely this cannot
 depend on the direction of <M>\vec v = (v_1,v_2,v_3).</M> 
 So we must have 
<D>f(v_1)f(v_2)f(v_3) = g(v_1^2+v_2^2+v_3^2)</D>
for some function <M>g(\cdot).</M>
 
Now we make our second technical assumption: <M>f</M>  is differentiable. Then differentiating both sides of the above equality
partially           wrt <M>v_i</M>  we get
<MULTILINE>
f'(v_1)f(v_2)f(v_3) & = & 2v_1g'(v_1^2+v_2^2+v_3^2),\\
f(v_1)f'(v_2)f(v_3) & = & 2v_2g'(v_1^2+v_2^2+v_3^2),\\
f(v_1)f(v_2)f'(v_3) & = & 2v_3g'(v_1^2+v_2^2+v_3^2).
 </MULTILINE>
So 
<D>[[f'(v_1)f(v_2)f(v_3)][v_1]]=[[f(v_1)f'(v_2)f(v_3)][v_2]],</D>
or
<D>[[f'(v_1)][v_1f(v_1)]]=[[f'(v_2)][v_2f(v_2)]].</D>
Since <M>v_1,v_2</M>  are arbitrary, hence this means 
<D>[[f'(x)][xf(x)]]=k</D>
for some constant <M>k.</M>   Solving the differerntial equation we get 
<M>\log f(x) = [[k x^2][2]]+</M> constant,   or 
<D>f(x) \propto e^{k x^2/2}.</D>
Since <M>f</M>  is a density, hence its total intergral must be 1. Hence <M>k < 0</M>  and we get
 the density of <M>N(*(0,-[[1][2k]])*).</M>

<HEAD2>Problem set <PS/></HEAD2>
<EXR>If <M>X</M>  is a random variable with density proportional to <M>\exp((1-x)(3+4x))</M>  for
 <M>x\in\rr</M>,  then find the distribution of <M>X.</M> </EXR>

<EXR>Let <M>\vec V = (V_1,V_2,V_3)</M>  have the joint distribution as in Maxwell's derivation.
 Consider <M>\vec U = [[\vec V][\|\vec V\|]]</M>, the unit vector along <M>\vec V.</M>  Describe
 the distribution of <M>\vec U.</M></EXR>

<EXR>If <M>X</M>  has density proportional to <M>e^{ax^2+bx+c}</M>  for <M>x\in\rr</M>, for some
 constants <M>a,b,c</M>, then find <M>E(X)</M>  and <M>V(X).</M></EXR>

<HEAD1>Problems for practice</HEAD1>
::<EXR><EIMG web="rossdistrib1.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib2.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib3.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib4.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib5.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib6.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib7.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib8.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib9.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib10.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib11.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib12.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib13.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib14.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib15.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib16.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib17.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib18.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib19.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib20.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib21.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib22.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib23.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib24.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib25.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib26.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib27.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib28.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib29.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib30.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib31.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib32.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib33.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib34.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib35.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib36.png"></EIMG></EXR>
::<EXR><EIMG web="rossdistrib37.png"></EIMG></EXR>
::<EXR><EIMG web="rosspdf9.png"/>

Here is Example 5b that is refered above:
<IMG web="rosspdf10.png">Example 5b</IMG>
</EXR>

::<EXR><EIMG web="rosspdf15.png"/></EXR>
::<EXR><EIMG web="hpspdf21.png"/></EXR>
::<EXR><EIMG web="hpspdf23.png"/></EXR>
::<EXR><EIMG web="hpspdf25.png"/></EXR>
::<EXR><EIMG web="hpspdf27.png"/></EXR>
---
::<EXR><EIMG web="hpspdf28.png"/></EXR>
::<EXR><EIMG web="hpspdf29.png"/></EXR>
::<EXR><EIMG web="hpspdf30.png"/></EXR>
::<EXR><EIMG web="hpspdf31.png"/></EXR>
::<EXR><EIMG web="hpspdf32.png"/></EXR>
::<EXR><EIMG web="hpspdf33.png"/></EXR>
::<EXR><EIMG web="hpspdf34.png"/></EXR>
::<EXR><EIMG web="hpspdf35.png"/></EXR>
::<EXR><EIMG web="hpspdf36.png"/></EXR>
::<EXR><EIMG web="hpspdf37.png"/></EXR>
::<EXR><EIMG web="hpspdf38.png"/></EXR>
::<EXR><EIMG web="hpspdf39.png"/></EXR>
---
::<EXR><EIMG web="hpspdf41.png"/></EXR>
::<EXR><EIMG web="hpspdf42.png"/></EXR>
::<EXR><EIMG web="hpspdf43.png"/></EXR>
::<EXR><EIMG web="hpspdf44.png"/></EXR>
----
::<EXR><EIMG web="hpstrans3.png"/></EXR>
::<EXR><EIMG web="hpstrans6.png"/></EXR>
::<EXR><EIMG web="hpstrans10.png"/></EXR>
::<EXR><EIMG web="hpstrans11.png"/></EXR>
::<EXR><EIMG web="hpstrans13.png"/></EXR>
::<EXR><EIMG web="hpstrans15.png"/></EXR>
::<EXR><EIMG web="hpstrans16.png"/></EXR>
::<EXR><EIMG web="hpstrans17.png"/></EXR>
::<EXR><EIMG web="hpstrans18.png"/></EXR>
::<EXR><EIMG web="hpstrans20.png"/></EXR>
::<EXR><EIMG web="hpstrans22.png"/></EXR>
::<EXR><EIMG web="hpstrans25.png"/></EXR>
::<EXR><EIMG web="hpstrans26.png"/></EXR>

</NOTE>@}
