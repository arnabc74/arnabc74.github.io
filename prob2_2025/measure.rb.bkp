 @{<NOTE>
<TITLE>Notions from measure theory</TITLE><M>
\newcommand{\calF}{{\mathcal F}}
\newcommand{\calP}{{\mathcal P}}
\newcommand{\calB}{{\mathcal B}}
\newcommand{\calD}{{\mathcal D}}
\newcommand{\ind}{{\mathbb 1}}
\newcommand{\area}{\mathrm {area}}
</M>
<HEAD1>Measure</HEAD1>
We often talk about the "size" of different sets. There are various ways to measure the size, length, area, volume, mass,
 cardinality. All these measures share some common properties: they are defined over subsets of
 some universal set (e.g., length for subsets of <M>\rr,</M>  while area for subsets of
 <M>\rr^2</M>), they are always nonnegative
 and they add up over disjoint sets. 

Measure theory is a unified approach to deal with all these mathematically. 
Let <M>\Omega</M>  be our universal set. We denote the measure of some <M>A\seq\Omega</M>  by
 <M>\mu(A).</M>  

<EXM>If we are talking about "length" of subsetsof <M>\rr,</M>  then we take <M>\Omega=\rr.</M>  For
 <M>A = (0,3)</M>  we have <M>\mu(A) = 3.</M></EXM>

<EXM>If we are talking about "area" of subsets of <M>\rr^2</M>,  then we take <M>\Omega=\rr^2.</M>  For
 <M>A = [0,3]\times[-1,1]</M>  we have <M>\mu(A) = 3\times2=6.</M></EXM>

<EXM>If we are talking about "cardinality" of subsets of <M>\{1,2,...,10\}</M>,  then we take <M>\Omega=\{1,2,...,10\}.</M>  For
 <M>A = \{2,4,6,8,10\}</M>  we have <M>\mu(A) = 5.</M></EXM>

In general, for <M>A\seq\Omega</M>, we require <M>\mu(A)</M>  to be a nonnegative number, and for any disjoint countable
 collection of subsets 
 <M>A_1,A_2,...\seq\Omega</M>  we want <M>\mu(\cup A_i) = \sum \mu(A_i).</M>

This may sound simple, but there are a couple of problems.
<UL><LI>To see the first problem,  let's consider the example of "length"
 of subsets of <M>\rr.</M>  Here we would like to say that  <M>\mu(A)= \infty</M>  if <M>A</M>  is
 <M>\rr</M>  or <M>(0,\infty)</M>  or <M>[4,\infty)</M>  etc. So we have consider the symbol
 <M>\infty</M>  as a nonnegative number.
This will not cause any problem with the countable
 additivity condition, since we are not working with both <M>\infty</M>  and <M>-\infty.</M>  So
 addition is still well-defined, as <M>\forall a\in\rr~~a+\infty = \infty+a = \infty</M>  and
 <M>\infty + \infty= \infty.</M>  
</LI>
<LI>The second problem is more subtle. Sometimes we want the measure to satisfy additional "nice" conditions. 
Often we find that no measure with those "nice" conditions exists.
See <LINK to="vitali.html">this discussion</LINK>  for one such example. 
 If, however, we define it only for a subcollection of
 subsets of <M>\Omega,</M>  then the problem goes away. The subcollection must of course be rich enough to allow interesting
mathematical  manipulations. So we want a nonempty subcollection that is closed under countable
 union, intrsection and complementation.
 Any such subcollection is called a <TERM><M>\sigma</M>-algebra</TERM>  on <M>\Omega.</M>
</LI></UL>

<EXM>Let <M>\Omega = \{1,2,...,10\}.</M>  Is <M>{#{ \{1,2,3\}, \{2,3,4\} }#}</M>  a <M>\sigma</M>-algebra over <M>\Omega</M>?
<SOLN/>
No. Here <M>\{1,2,3\}</M>  and <M>\{2,3,4\}</M>  are both in the collection, but their intersection <M>\{2,3\}</M>  is not.
</EXM>

<EXM>Which subsets of <M>\Omega</M>  do we need to  the above collection in order to turn it into a <M>\sigma</M>-algebra?
<SOLN/>
Let <M>A=\{1,2,3\}</M>  and <M>B=\{2,3,4\}.</M>  Drawing them as a Venn diagram we get 4 basic sets. 
<CIMG web="basic.png">The 4 basic sets</CIMG>
The smallest <M>\sigma</M>-algebra consists of all possible unions of these. There are <M>2^4=16</M>  sets in the <M>\sigma</M>-algebra.
</EXM>
The idea behind this example is important. We often start with some collection of subsets, and
 turn it into a simple by adding as few extra subsets to it as possible. The resulting <M>\sigma</M>-algebra is the smallest
 <M>\sigma</M>-algebra containing the given collection. We call it the <M>\sigma</M>-algebra <TERM>generated</TERM>  by the
 collection. 

If the initial collection is finite, then we can always proceed as in the above example. But more often the initial collection
 is infinite. Then there is no simple way to describe the <M>\sigma</M>-algebra generated by it. The most prominent example
 is given by the <TERM>Borel <M>\sigma</M>-algebra</TERM>  defined below.
<DEFN name="Borel $\sigma$-algebra">
Let <M>\Oemga=\rr.</M>  The <M>\sigma</M>-algebra generated by the collection of all open intervals is called the <TERM>Borel <M>\sigma</M>-algebra</TERM> 
 on <M>\rr.</M>  It is commonly denoted by <M>\calB.</M>
</DEFN>

Combining all these, we get the  definition of measure.
<DEFN name="Measure">
Let <M>\Omega</M>  be any non-empty set. Let <M>\calF</M>  be any <M>\sigma</M>-algebra on
 <M>\Omega.</M>  Then by a <TERM>measure</TERM>  we understand a function <M>\mu:\calF\to[0,\infty]</M>  such that 
<D>\forall \mbox{disjoint }A_1,A_2,...\in\calF~~\mu(\cup A_i) = \sum\mu(A_i)</D>
</DEFN>
Here are a couple of related terms: the pair  <M>(\Omega,\calF)</M> is called a <TERM>measurable space</TERM>,  and
the triple <M>(\Omega,\calF,\mu)</M>  is called a <TERM>measure space</TERM>. Note that if <M>A\not\in\calF</M>,  then 
 <M>\mu(A)</M>  is undefined.

Notice that the conditions in this definition are much like the probability axioms, except that there we had one extra conditions
 <M>P(\Omega)=1.</M>  This shows that probability is a measure. However, a measure <M>\mu</M>  need not have <M>\mu(\Omega)</M> 
 equal to <M>1.</M>  It could be any finite positive number or even <M>\infty.</M>  If <M>\mu(\Omega)< \infty,</M>  we naturally
 call <M>\mu</M>  a <TERM>finite measure</TERM>. These are generally easier to work with. Infinite measres are harder to
 deal. But there is an useful intermediate type of measure that allows <M>\mu(\Omega)=\infty</M>  and yet retains much of
 the advantages of a finite measure. These are what we work with most of the time. Th definition goes like this.

<DEFN name="$\sigma$-finite measure">
A measure <M>\mu</M>  on <M>(\Omega,\calB)</M>  is called <TERM><M>\sigma</M>-finite</TERM>  if there are disjoints <M>B_1,B_2,...\in\calB</M> 
 such that <M>\Omega = \cup_i B_i</M>  and <M>\forall i~~\mu(B_i)<\infty.</M>
</DEFN>
One example is "length" or <M>\rr.</M>  Entire <M>\rr</M>  has infinite length, but <M>\rr</M>  may be split up into <M>[n,n+1)</M> 
 for <M>n\in\zz</M>  and each of these has finite length.


<HEAD2>Problem set <PS/></HEAD2>

<EXR>Show that any <M>\sigma</M>-algebra on <M>\Omega</M>  must contain <M>\phi</M>  and <M>\Omega.</M></EXR>
<EXR>Is <M>\calP(\Omega)</M>  a <M>\sigma</M>-algebra on <M>\Omega?</M>
<ANS>
Yes. It is the largest <M>\sigma</M>-algebra on <M>\Omega.</M>
</ANS></EXR>
<EXR>Is <M>\{\phi,\Omega\}</M>  a <M>\sigma</M>-algebra on <M>\Omega?</M>
<ANS>Yes, it is the smallest <M>\sigma</M>-algebra on <M>\Omega.</M></ANS></EXR>

<EXR>Show that for any measure <M>\mu(\phi)=0.</M></EXR>  

<EXR>If <M>\mu(A) = 3</M>  and <M>\mu(B) = 5.5</M>  and <M>\mu(A\cap B) = 2</M>, then what is <M>\mu(A\cup B)?</M></EXR>

<HEAD1>Constructing a measure from intuition</HEAD1>
 How do we construct a measure? Typically we start with some subsets of <M>\Omega</M>  for which the value is obvious. Then
 we use the countable additivity to define it for less obvious sets. Let's do it for "length". We all agree that the length
 of <M>(a,b)</M>  is <M>b-a</M>  for <M>a<b.</M>  Then what should the length of <M>(0,1)\cup
 (3,5)</M>  be? The answer is <M>1+2=3.</M>  Even less obvious is the length of <M>\qq.</M>  It
 turns out to be <M>0+0+\cdots=0</M>  and so  the length of <M>\qq^c</M>  must be <M>\infty.</M>  

Can we do this for all subsets? Or may we run into some difficulty? The answer is very heartening:
 If you have enough "obvious" cases, and there is no inconsistency
among them, then you may extend them uniquely. This is formalised as a famous theorem:

<THM name=" Caratheodory extension">
Let <M>\Omega</M>  be a nonempty set and <M>\calB</M>  be a <M>\sigma</M>-algebra on it. We want to specify a measure <M>\mu</M> 
 on it. For this it it enough to specify <M>\mu</M>  on a subcollection <M>\calF</M>   collection of subsets if  
<OL><LI><M>\mu</M>  is nonnegative and countably additive,</LI>
<LI><M>\calB</M>  is generated by <M>\calF</M></LI>
<LI><M>\calF</M>  is closed under finite intersection,</LI>
<LI><M>\forall A\in\calF~~\exists \mbox{disjoint }B_1,...,B_n\in\calF~~A^c = \cup_1^n B_i.</M></LI>
</OL>
</THM>
We shall not prove this in this course. But let us understand the significance of the conditions. The first condition is
 clearly necessary. If <M>\mu</M>  already fails to be nonnegative or countably additive on <M>\calF</M>  how can we expect
 it to be a measure when extended to  <M>\calB?</M>  If we specify <M>\mu</M>  for each and every set in <M>\calB,</M>  then
 of course it would suffice. But that woluld be too much work. Our aim is to specify <M>\mu</M>  for only a smaller number
 of the sets. Clearly, we cannot fall below a generating set. So that condition puts a lower bound on how low we can go.
 But that lower bound is actually too low, as not all generating collection would do. It must be "rich" enough. If you specify
 <M>\mu</M>  on <M>A</M>  and <M>B,</M>  then you should also specify it for <M>A\cap B,</M>  to help determine how much
 <M>A,B</M>  overlap each other. Also, for each <M>A</M>  we should be able to express <M>A^c</M>  as a finite union of sets
 for which <M>\mu</M>  has been specified. This will help us to place <M>A</M>  in the background of <M>\Omega.</M>  

<HEAD2>Problem set <PS/></HEAD2>
The following exercises will apply this concept to define "length" for all
 subsets in <M>\calB.</M>  This measure is called <TERM>Lebesgue measure</TERM>  on <M>(\rr,\calB).</M>

Let  <M>\calF</M>  be the collection of all intervals (bounded, unbounded, open, closed,
 semi-open) in <M>\rr.</M>  Also we include <M>\phi</M>  in <M>\calF.</M>  
We can define the length of an interval by intuition: <M>(a,b), [a,b], [a,b)</M>  and <M>(a,b]</M>  all should have length
 <M>b-a.</M>  Unbounded intervals should have length <M>\infty.</M>

<EXR>
Show that
 <M>\forall A,B\in\calF~~A\cap B\in\calF.</M>  
</EXR>

<EXR>Is it true that <M>\forall A\in\calF~~\exists \mbox{disjoint }B_1,...,B_n\in\calF~~A^c = \cup B_i?</M>  
Let <M>A = (0,1).</M>  How many <M>B_i</M>'s would you need? What if <M>B  = [0,1]?</M>  or <M>[a, \infty)?</M>  or <M>(-\infty,5)?</M> 
</EXR>
<EXR>
In each case below we give some sequence <M>(A_n)</M> of sets in <M>\calF</M>.  check if the sets are disjoint,
 and whether their union
 is again in <M>\calF.</M>  If so check if  <M>\sum_n\mu(A_n) = \mu(*(\cup_n A_n)*).</M>
<OL>
<LI><M>A_n = [0,n))</M>  for <M>n\in\nn.</M></LI>
<LI><M>A_n = [n,n+1]</M>  for <M>n\in\nn.</M></LI>
<LI><M>A_n = [n,n+1)</M>  for <M>n\in\nn.</M></LI>
<LI><M>A_n = [*[ 2^{-n}, 2^{-n+1})*))</M>  for <M>n\in\nn.</M></LI>
<LI>Let <M>(a_n)</M>  be a positive sequence with <M>\sum a_n < \infty.</M>  Let <M>S_n = \sum_1^n a_i.</M>  Let <M>A_n = (2+S_{n-1}, 2+S_n].</M></LI>
</OL></EXR>

<EXR>Is our<M>\mu</M>  here <M>\sigma</M>-finite on <M>\calF?</M> </EXR>

<EXR>
Show that there is a unique measure <M>\mu</M>  on <M>(\rr,\calB)</M>  such that
 <M>\mu(\phi)=0</M>  and <M>\forall a < b~~\mu(#( (a,b) )#) = b-a</M>  and unbounded intervals having length <M>\infty</M>
</EXR>

The next exercise asks you to prove the existence of <M>Unif(0,1),</M>  thus completing the
 proof of the fundamental theorem. 

<EXR>Follow the same procedure as in the exiercises above to show that there is a measure 
 on <M>\Omega = (0,1)</M>  such that for each <M>0 \leq a \leq b\leq 1</M>  we have
 <M>\mu((a,b))=\mu((a,b]) =\mu([a,b)) =\mu([a,b])=b-a.</M>  Show that this <M>\mu</M>  is a
 probability. This is called <M>Unif(0,1).</M>    </EXR>
<COMMENT>
<THEAD1>Uncountable sample space</THEAD1>
We have already seen last semester that for an uncountable <M>\Omega</M>  we may not always be able
 to define a function <M>P:{\mathcal P}(\Omega)\to[0,1]</M> 
 satisfying all the probability axioms.  Please see <LINK to="../prob1_2024/vitali.html">this page
 from the last semester</LINK>  to brush up on this. 

Hence we defined <M>\sigma</M>-algebra. While the simplest <M>\sigma</M>-algebras are the trivial one and the entire power
 set, the most commonly used is the Borel <M>\sigma</M>-algebra. 

Closely related to this is the idea of a measurable function.
</COMMENT>
<HEAD1>Measurable function</HEAD1>
We have talked about random variables as real-valued functions defined on <M>(\Omega,\calF).</M>   Thus, <M>X:\Omega\to\rr.</M> 
Suppose that we want to compute  <M>P(X\in B)</M> for some <M>B\seq \rr.</M>   Now, the event <M>\{X\in B\}</M>  is actually
<D>A=\{w\in\Omega~:~X(w)\in (a,b)\}\equiv X ^{-1} (a,b).</D> 
 In order to be able to compute probability of this,
 we need to have <M>X ^{-1}(B)\in\calF.</M>  Typically we want to be able to compute <M>P(X\in B)</M>  for
 each <M>B\in\calB.</M>  So we need <M>\forall B\in\calB~~X ^{-1}(B)\in\calF.</M>

This motivates the following definition.
<DEFN name="Measurable function">
Let <M>(\Omega_1,\calF_1)</M>  and <M>(\Omega_2,\calF_2)</M>  be measurable spaces. Then a function <M>f:\Omega_1\to\Omega_2</M> 
 is called <TERM>measurable</TERM>  if 
<D>\forall B\in\calF_2~~f ^{-1} (B)\in \calF_1.</D>
</DEFN>

<THM>
Countable operations with measurable functions produce measurable functions. 
</THM>
The following two definitions express the familiar concepts of a random variable and its distribution in terms of measure
 theory.

<DEFN>
By a <TERM>random variable</TERM>  on a probability space <M>(\Omega,\calF,P)</M>  we mean a measurable function <M>X</M> 
 from <M>(\Omega,\calF)</M>  to <M>(\rr,\calB).</M>  Note that <M>P</M>  plays no role in the definition. 
</DEFN> 

<DEFN name="Distribution">
Let <M>(\Omega,\calF,P)</M>  be a probability space. Let <M>X:\Omega\to\rr</M>  be a random variable. By the
 <TERM>distribution</TERM> of 
 <M>X</M>  we mean the function
 <M>P_X:\calB\to[0,1]</M>  given by 
<D>P_X(B) = P(X ^{-1}(B))\mbox{ for } B\in\calB.</D>
</DEFN>
<HEAD2>Problem set <PS/></HEAD2>
<EXR>Characterise measurable functions from <M>(\rr,\calp(\rr))</M>  to <M>(\rr,\{\phi,\rr\}).</M> 
</EXR>
<EXR>Characterise measurable functions from  <M>(\rr,\{\phi,\rr\})</M>  to <M>(\rr,\calp(\rr))</M>. 
</EXR>


<HEAD1>Lebesgue integration: the idea</HEAD1>
<HEAD2>Problem with Riemann integration</HEAD2>
We all know about
 Riemann integration. We shall illustrate the idea with a positive, bounded function
 <M>f:[a,b]\to\rr</M>. The idea is to measure
 the area under its graph by  approximating it with steps
 functions with finitely many steps. We do this from both above and below. For this we partition the domain of the function
 into finitely many intervals and raise rectangles on them as follows.
<CIMG web="rul.png"></CIMG>
The intuition is that if we take finer and finer partitions and raise the red rectangles as much as we can under the graph,
 we shall come arbitrarily close to the area under the graph. If we do the same from above  the graph using the blue rectangles,
 then also we should come arbitrarily close to the same area. So our intuition dictates that 
<Q>
sup (red area) = inf(blue area),
</Q>
and we plan to use this common value as the area under the curve. This brilliant intuition has just one loop hole, for many
 functions the sup does not equal the inf! We call such functions non-Riemann integrable, and try
 to avoid them at all costs.  However, these
 bad functions cannot be completely avoided, as they crop up naturally from time to time, usually as the limit of Riemann integrable
 functions. 
<HEAD2>Lebesgue's solution</HEAD2>
Lebesgue had a solution for this. Instead of partitioning the domain is plan was to partition the codomain. So he also
 got red rectangles below the graph and blue rectangles above as follows.
<CIMG web="lul.png"></CIMG>
 And like Riemann he also hoped that
<Q>
sup (red area) = inf(blue area),
</Q>
and he wanted to call this the area under the curve. 

<HEAD2>Problem set <PS/></HEAD2>
<EXR>Consider the following graph of a bounded positive function. Finitely many values are marked
 on the <M>y</M>-axis. Draw horizontal lines through them, and obtain the red and blue areas. </EXR>

<EXR>Repeat for the following function.</EXR>

<HEAD1>Why Lebesgue's solution works</HEAD1>
Just based on these diagrams, you may think that Lebesgue's idea is no different from Riemann's idea. Actually, Lebesgue's
approximations are more flexible than Riemann's. To understand this look at the graph below, where
 we have shown the lower Lebesgue approximation using just 4 points in the codomain.
<CIMG web="lul2.png">Just three heights, but so many rectangles!</CIMG>
Each value in the codomain, can give birth to many rectangles, depending on the ups and downs of the curve. 

Indeed, a single height can give rise to infinitely many "rectangles"! 
For instance, the function
<D>f(x) =<CASES>1<IF>x\in\qq\cap[0,1]</IF> 0<ELSE/></CASES> </D> 
takes only two values, 0 and 1. Yet each value is taken infinitely often. 
So you can now feel why Lebesgues approximations are more flexible than Riemann's:
<Q>
Riemann's approximations are special cases of Lebesgue's approximations, but
 not <I>vice versa</I>. 
</Q>

As a result here
 the sup(red) and inf(blue)
 match for a more general class
 of functions. In fact, they match for all measurable functions. 
This  also shows that if Riemann's sup(red) and inf(blue) areas meet, then so must Lebesgue's, and the meeting
 point would be the same. 

Now we shall take a rigourous look at Lebesgue's idea. First we need a name for the functions that Lebesgue is using to approximate
 areas. We shall call them <TERM>simple</TERM>  functions.

<DEFN name="Simple function">
A function is called <TERM>simple</TERM>  if it takes only finitely many values.
</DEFN>

We can  express a simple function mathematically using indicator functions. Let a simple function take only the values <M>c_1,...,c_k</M> 
 (all distinct). Let <M>A_i = \{\omega\in\Omega~:~f(\omega) = c_i\}.</M>  An example is shown below.
<CIMG web="simpstep2.png">Clearly the <M>A_i</M>'s partition <M>\Omega</M>.</CIMG>
The <M>A_i</M>'s need not always be just finite union of intervals. For example, in case of the Dirichlet function, we have
 just two <M>A_i</M>'s, one is <M>\qq\cap [0,1]</M>  and the other <M>\qq^c\cap [0,1].</M>  However, we always have only
 finitely many <M>A_i</M>'s. We can now write the simple function as 
<D>f(\omega) = \sum_{i=1}^k c_i\ind_{A_i}(\omega).</D>
Lebesgue wanted to think that each <M>c_i</M>  constributes a "rectangle" with  height <M>c_i</M>  on
 the base <M>A_i.</M> Such a "rectangle" should have
 area <M>c_i\times</M> length
 of <M>A_i</M>. But how to measure length of <M>A_i</M>'s? 
It is this question that first
 led him to create measure
 theory. As we have already
 stated, not all subsets
 are measurable. So naturally he restrited his attention to only those simple functions <M>\sum_i
 c_i\ind_{A_i}</M>, where the <M>A_i\in\calB.</M>
 

The next step in Lebesgue's intuition is to approximate the given function using simple functions,
 from below and from above. This is where Lebesgue's approach beats Riemann's approach:

<THM><I>Every</I>  measurable
 non-negative function is Lebesgue integrable.</THM> 


 
<HEAD2>Problem set <PS/></HEAD2>
<EXR>Prove that  <M>\sum_i c_i\ind_{A_i}</M>    is measureable if and only if <M>\forall i~~A_i\in\calF.</M></EXR>

<HEAD1>Generalising the solution</HEAD1>
Lebesgue's approach can be generalised in different ways. We shall discuss these now.
<HEAD2>Allowing unbounded functions</HEAD2>
Since the Lebesgue integral exists for all bounded, non-negative measurable functions, hence it is enough to consider only
 the sup of the approximations from below. This immediately allows us to define Lebesgue integral
 for unbounded, measurable functions
 as well. We just allow the sup to be <M>\infty.</M>  Similarly we may now carry out the procedure
 over any measurable domain, not just intervals like <M>[a,b].</M>  Any "rectangle" with base
 measure <M>\infty</M>  and positive height has area <M>\infty</M>, and any "rectangle" with zero
 height has zero area (even if its base has measure <M>\infty</M>).  

<HEAD2>Allowing negative values</HEAD2>
Moving from non-negative functions to general functions is easy. For <M>f:\rr\to\rr</M>  we define <M>f_+ =\max\{f,0\}</M> 
 and <M>f_- =\max\{-f,0\}.</M>  Then <M>f = f_+-f_-.</M>  We define <M>\int f\, d \lambda = \int f_+\, d \lambda -\int f_-\, d \lambda,</M>  if
 both the integrals on the rhs are not <M>\infty.</M>
<HEAD2>Allowing other measures</HEAD2>
The construction so far has used "length" of measure the bases of the rectangles. We can use any
 other measure, <M>\mu</M>, as well, as
 long as the measure of the domain has finite measure. Then we shall write the Lebesgue integral as <M>\int f\, d\mu.</M>

When we take some probability measure in place of <M>\mu,</M>  we get the familiar definition of expectation. 
<HEAD2>Problem set <PS/></HEAD2>
<HEAD1>Two technical results</HEAD1>
<THM>
If <M>f:\Omega\to[0,\infty)</M>  is any measurable function, then there is a non-decreasing sequence <M>(s_n)</M>  of simple
measurable functions such that
<D>\forall \omega\in\Omega~~s_n(\omega) \uparrow f(\omega).</D> 
</THM>
<PF>
For <M>n\in\nn</M>  and <M>\omega\in\Omega</M>  we define <M>s_n</M>  as follows. First partition the codomain
 <M>[0,\infty)</M>  into <M>2</M>  intervals <M>[0,n)</M>  and <M>[n,\infty)</M>  and then
 subdivide the first into equal subintervals of length <M>2^{-n}.</M>  So you get <M>N=n2^n+1</M>  subintervals in all. Call
 these <M>[a_1,b_1),...,[a_N,b_N).</M>  These constitute a partition of the codomain.

Now  set <M>s_n(\omega) = a_k</M>  if <M>f(\omega) \in[ a_k,b_k).</M>  

The following picture shows this process for <M>n=1</M>  and <M>n=2.</M>
<CIMG web="subdiv.png">Notice how the subdivisions for <M>n=2</M>  fit into those for <M>n=1.</M></CIMG>     

For each <M>\omega\in\Omega</M>  and for each <M>n\in\nn</M>  we have <M>s_n(\omega)\leq s_{n+1}(\omega).</M>
<BECAUSE>
If <M>s_n(\omega) = a</M>   and <M>s_{n+1}(\omega) = b,</M>  then <M>f(\omega)\in[a+2^{-n})</M>  and also <M>f(\omega)\in[b+2^{-n-1}).</M> 
 
So, by the contruction of the partitions, <M>[b+2^{-n-1})\seq[a,2^{-n}).</M>

Thus, <M>a\leq b,</M>  as required.
</BECAUSE>

Again, for each <M>\omega\in\Omega</M>  we have <M>s_n(\omega)\to f(\omega).</M>  
<BECAUSE>
To show:

<TGT>\forall \omega\in\Omega~~\forall \epsilon>0~~\exists M\in\nn~~\forall n\geq M ~~|f(\omega)-s_n(\omega)| < \epsilon.</TGT>

<FLL>\omega</FLL> Take any <M>\omega\in\Omega.</M>

<FLL>\epsilon</FLL> Take any <M>\epsilon>0.</M>

<EXS>M</EXS> Choose <M>M\in\nn</M>  such that <M>M> f(\omega)</M>  and <M>2^{-M} < \epsilon.</M>  (Possible since <M>\nn</M> 
 is unbounded above and <M>2^{-n}\to 0</M>  as <M>n\to \infty.</M>

<FLL>n</FLL> Take any <M>n\geq M.</M>

<CHK/>Since <M>f(\omega) < M\leq n,</M>  hence <M>s_n(\omega) < n.</M>  

Thus, <M>f(\omega) \in [s_n(\omega),s_n(\omega)+2^{-n}).</M>
</BECAUSE>
This completes the proof.
</PF>

<EXR>
Show that the convergence is uniform if <M>f</M>  is bounded.
</EXR>

<EXR>
Show that if, in the theorem above,  <M>f</M>  is measurable (w.r.t. any given <M>\sigma</M>-field <M>\calF</M>
 over <M>\Omega</M>  and the Borel <M>\sigma</M>-field over <M>\rr</M>), then so must be each <M>s_n.</M> 
</EXR>

The next step is to show that the red areas indeed converge to the supremum. 
<THM>
If <M>f</M>  is a non-negative measurable function, and <M>s_n</M>'s are simple random variables with <M>s_n\uparrow f,</M> 
 then <M>\int s_n \uparrow \int f.</M>
</THM>
<PF>
Shall show

<TGT>\forall \epsilon>0~~\exists N\in\nn~~\forall n\geq N~~ \int s_n > \int f-\epsilon.</TGT>

This will complete the proof, since anyway <M>(*(\int s_n)*)</M>  is a non-decreasing sequence bounded
 from above by <M>\int f</M> 
  (The case <M>\int f=\infty</M>  is trivially
 included in it). 

Since <M>\int f = \sup\{\int z~:~  z\leq f,~~z \mbox{ simple}\},</M>

hence <M>\exists</M> simple <M> z\leq f</M>  with <M>\int z > \int f-\epsilon.</M> 


Fix some <M>\delta>0.</M>

Let <M>A_n =\{s_n > z-\delta\}.</M>

Then <M>A_n\uparrow\Omega.</M>
<BECAUSE>
Since <M>s_n</M>'s are non-decreasing, hence <M>A_1\seq A_2\seq A_3\seq\cdots.</M>  

Also since <M>\forall\omega\in\Omega~~s_n(w)\uparrow f(w),</M>  hence <M>\cup_n A_n=\Omega.</M>  
</BECAUSE>  

So <M>\int s_n\geq \int_{A_n} s_n \geq \int_{A_n}z  \geq \int z-M\mu(A_n^c)-\delta,</M>
where <M>M = \max Z.</M>  

Taking limit <M>\lim \int s_n \geq z- \delta.</M>  

Since <M>\delta>0</M>  is arbitrary, we have <M>\lim \int s_n \geq \int z.</M>  
</PF>
<HEAD2>Problem set <PS/></HEAD2>
<HEAD1>Additivity</HEAD1>
We had stated last semester that if <M>X,Y</M>  are two jointly distributed random variables with expectations, and <M>a,b\in\rr</M> 
 are any two numbers, then <M>aX+bY</M>  is also a random variable with expectation, and <M>E(aX+bY) = aE(X)+bE(Y).</M>

First we show that <M>E(X+Y) = E(X)+E(Y)</M>  in three steps.

<U>Step 1</U>: Show this when <M>X,Y</M>  are  simple random variables. We have already done this last semester.

<U>Step 2</U>: Show this for non-negative <M>X,Y.</M>  Let <M>(S_n)</M>  and <M>(T_n)</M>  be simplifications for <M>X</M> 
 and <M>Y,</M>  respectively. Then <M>(S_n+T_n)</M>  is a simplification for <M>X+Y.</M>  

Also <M>E(S_n+T_n) = E(S_n)+E(T_n).</M>  Te result now follows on taking limit of both sides.

<U>Step 3</U>: Show this for general <M>X,Y.</M>  Here we apply step 2 to <M>X_+, X_-, Y_+</M>  and <M>Y_-.</M>  

Then we show that for <M>a>0</M>  we have <M>E(aX) = E(X).</M>  This proof also proceeds in three steps (left as an exercise).

Finally, we show <M>E(-X)= -E(X).</M>  Let <M>Y = -X.</M>  Then <M>Y_+ = X_-</M>  and <M>Y_- = X_+.</M>  So <M>E(Y) = E(Y_+)-E(Y_-) = E(X_-)-E(X_+) = -E(X).</M>

<HEAD1>Monotone convergence theorem (MCT)</HEAD1>
<THM name="MCT (simple version)">
Let <M>X_n</M> 's be non-negative  random variables with <M>X_n\uparrow X</M>  for some random variable <M>X.</M>
 Then <M>E(X_n)\uparrow E(X).</M>
</THM>
<PF>
Enough to show simple random variables <M>Y_n</M>  such that <M>Y_n\uparrow X </M> and <M>Y_n\leq X_n.</M>
<BECAUSE>
We already know <M>E(Y_n)\uparrow E(X).</M>  But <M>E(X_n)</M>  is sandwiched between <M>E(Y_n)</M>  and <M>E(X).</M>
</BECAUSE>
Let <M>(Z_{n,k})_k</M>  be the simplification of <M>X_n.</M>  

Let <M>Y_n = \max\{Z_{1,n},...,Z_{n,n}\}.</M>

Then <M>Y_1\leq Y_2\leq\cdots</M>
<BECAUSE>
<MULTILINE>
Y_{n+1} & = & \max\{Z_{1,n+1},...,Z_{n+1,n+1}\}\\
& \geq & \max\{Z_{1,n+1},...,Z_{n,n+1}\}<SINCE><M>\because</M> superset cannot have smaller max</SINCE>\\
& \geq & \max\{Z_{1,n},...,Z_{n,n}\},
</MULTILINE>
by non-decreasing property of <M>Z_{n,k}</M>  w.r.t. <M>k.</M>
</BECAUSE>
Also <M>Y_n\leq X_n.</M>
<BECAUSE>
<M>Z_{k,n}\leq X_k\leq X_n.</M>
</BECAUSE>
Finally, <M>Y_n\uparrow X.</M>
<BECAUSE>
We have <M>Z_{n,k} \leq Y_k.</M>  

Taking limit as <M>k\to \infty,</M>  we have <M>X_n\leq \lim_k Y_k.</M>

Now taking limit as <M>n\to \infty,</M>  we have <M>X\leq \lim_k Y_k.</M>  

Also we have <M>Y_n\leq X_n\leq X.</M>  So <M>\lim_k Y_k\leq X.</M>  

Hence <M>\lim_k Y_k= X.</M>
</BECAUSE>
This completes the proof.
</PF>

<EXR>
If <M>(X_n)</M>  is a <I>nonincreasing</I> sequence of nonnegative random variables converging to some random variable <M>X,</M> 
 and <M>E(X_1)<\infty,</M>  then show that <M>E(X_n)\downarrow E(X).</M>  What if the assumption <M>E(X_1)<\infty</M>  is
 dropped?
</EXR>

<EXR>
Suppose that <M>X_n</M>'s are nonnegative random variables. Show that 
<D>E(\sum_1^\infty X_n) = \sum_1^\infty E(X_n).</D>
</EXR>
In the simple version we assumed that the limit of <M>(X_n)</M>  is a random variable. In particular, we assumed that for
 each <M>\omega\in\Omega</M>  the sequence <M>(X_n(\omega))</M>  converges to some real number. We may actually drop these
 assumptions. We may allow <M>(X_n(\omega))</M>  to diverge. Then the limit <M>X(\omega)</M>  is a function from <M>\Omega</M> 
 to <M>[0,\infty].</M>  One can then <I>show</I>  (not <I>assume</I>) that this <M>X</M>  is a random variable. We do this
 below.
<HEAD1>Fatou and DCT</HEAD1>
<THM name="Fatou's lemma">
Let <M>(X_n)</M>  be  a sequence of nonnegative random variables.  Then
<D>E(\liminf X_n) \leq \liminf E(X_n).</D>
</THM>
<PF>
Let <M>Y_n = \inf\{X_k~:~k\geq n\}.</M>

Then <M>Y_n\uparrow \liminf X_n.</M>

So, by MCT, <M>E(Y_n)\to E(\liminf X_n).</M>

Now <M>E(X_n) \geq E(Y_n).</M>

Hence 
<D>E(\liminf X_n) \leq \liminf E(X_n),</D>
as required.
</PF>

<THM name="Dominated Convergence Theorem (DCT)">
Let <M>(X_n)</M>  be a a sequence of random variables with <M>\forall n~~|X_n|\leq Y</M>  for some <M>Y</M>  
with <M>E(|Y|)< \infty.</M> Also let <M>X_n\to X.</M>  
 Then <M>E|X_n-X|\to 0</M>  and so, in particular, <M>E(X_n)\to E(X).</M>
</THM>
<PF>
Clearly, <M>|X|\leq Y.</M>

So, by triangle inequality, <M>|X_n-X|\leq |X_n|+|X|\leq 2Y.</M>

Let <M>Z_n = 2Y-|X_n-X|.</M>  Then <M>Z_n</M>'s are all nonnegative random variables. 

Applying Fatou's lemma to <M>(Z_n)</M>, we have 
<D>E(\liminf Z_n)\leq \liminf E(Z_n) = 2E(Y)-\limsup E|X_n-X| = 2E(Y).</D>
Now 
<D>\liminf Z_n = 2Y-\limsup|X_n-X| = 2Y,</D>
and 
<D>\liminf E(Z_n) = 2E(Y)-\limsup E|X_n-X| .</D>
So we have 
<D>2Y\leq 2Y-\limsup E|X_n-X|,</D>
or <M>\limsup E|X-n-X|\leq 0.</M>

Hence <M>E|X_n-X|\to 0,</M>  as required.
</PF>
<HEAD1>Radon-Nikodym theorem</HEAD1>
<THM name="Radon-Nikodym theorem">
Let <M>\mu</M>  be any <M>sigma</M>-finite measure on <M>(\Omega,\calF).</M>  Let <M>\nu</M>  be another meaure on <M>(\Omega,\calF)</M> 
 with the property that 
<D>\forall B\in\calF~~(\mu(B)=0\Rightarrow\nu(B)=0).</D>
Then there is a measurable f <M>f:\Omega\to\rr</M>  such that for any measurable function <M>h:\Omega\to\rr</M>  we have
 <D>\int h\, d\nu = \int hf\, d\mu.</D> 
This <M>f</M>  is called a <TERM>density</TERM>  of <M>\nu</M>  wrt <M>\mu.</M>
</THM>
<PF>Omitted.</PF>

We have used a special case of this theorem, where <M>\nu</M>  is a probability measure and <M>\mu</M>  is the Lebesgue measure.
 Such probability measures are called <TERM>absolutely continuous</TERM>. We have worked with the special case where we had
 a density that was Riemann integrable as well.
 
<HEAD2>Problem set <PS/></HEAD2>

</NOTE>@}
