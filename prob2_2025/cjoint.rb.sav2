 @{<NOTE>
<HEAD1>Joint distribution</HEAD1>
Just as we had encountered joint distribution while learning about discrete random variables, we have the
concept of joint distribution for absolutely continuous distributions as well. 

<DEFN>
Let <M>X,Y</M>  be jointly distributed random variables. We say that they are <TERM>jointly absolutely continuous</TERM> if 
 there is some <M>f:\rr^2\to[0,\infty)</M>  such that 
<D>\forall a \leq b, c \leq d~~P(#( (X,Y)\in[a,b]\times[c,d] )#) = \int_a^b \int_c^d f(x,y)\, dxdy.</D>
Any such <M>f</M>  is called a <TERM>joint PDF</TERM>  of <M>(X,Y).</M> 
</DEFN>
Note that if <M>X</M>  and <M>Y</M>  are jointly distributed discrete random variables, then immediatly we are assured of
having their joint PMF. But not so in case of absolutely continuous distributions. Even if
 <M>X</M>  and <M>Y</M>  are each absolutely continuous, still <M>(X,Y)</M>  may fail to be <I>jointly</I>  absolutely continuous.

<EXM>Suppose <M>X\sim Unif(0,1)</M>  and <M>Y = X.</M>  Then show that <M>(X,Y)</M>  is not
 jointly absolutely continuous. 
<SOLN/>
Here the CDF of <M>(X,Y)</M>  is 
<D>
F(x,y)=P(X\leq x,\, Y\leq y) = P(X\leq\min\{x,y\}) = <CASES>0<IF>\min\{x,y\}<0</IF> 1<IF>0\leq \min\{x,y\} < 1</IF> 1<IF>\min\{x,y\} \geq 1</IF></CASES>.
</D>
Hence, if <M>(X,Y)</M>  is jointly absolutely continuous, then a joint density would be given by <M>f(x,y)</M>, where 
<D>f(x,y) = [[\partial^2][\partial x\partial y]] F(x,y).</D>
This forces <M>f(x,y)\equiv 0,</M>  which is not a PDF.
  </EXM>
However, if <M>(X,Y)</M>  is jointly absolutely continuous, then both <M>X</M>  and <M>Y</M>  must also be absolutely continuous.

<HEAD2>Computing probabilities of non-rectangular regions</HEAD2>
<HEAD2>Marginals</HEAD2>

<HEAD2>Conditional distribution</HEAD2>
So far the absolutely continuous case closely mimics the discrete case, with integration replacing summation.  
But we cannot follow the same path for conditional distribution. If <M>(X,Y)</M>  are jointly discrete then we defined the
 conditional PMF of <M>X</M>  given <M>Y=y</M>  as <M>x\mapsto P(X=x|Y=y) = [[P(x=x,Y=y][P(Y=y)]].</M>  

So for the jointly absolutely continuous case, we define the <TERM>conditional PDF</TERM>  of <M>X</M>  given <M>Y=y</M> 
 as <M>f(x,y)/f_Y(y).</M>  There are, however, a couple of points here:
<OL>
<LI>It is possible to have <M>f_Y(y)=0.</M>  Then we define the conditional density as 0.</LI>
<LI>Unlike conditional PMF, the conditional PDF is not a conditional probability, since <M>P(Y=y)=0.</M>
</LI>
<LI><M>\int_a^bf_{X|Y}(x,y)\, dx</M>  does not give <M>P(X\in [a,b]|Y=y),</M>  as <M>P(Y=y)=0.</M></LI>
<LI>However, <M>\int_c^d \int_a^bf_{X|Y}(x,y)\, dxdy</M>  does indeed give <M>P(X\in [a,b], Y\in[c,d]).</M>
</LI>
</OL>
It is this last step (which is the analog of total probability) that justifies the definition of conditional PDF.

Other than this difference, the rest follows as in the discrete case. We have the concepts of
 conditional expectation, conditional variance etc as usual. 

<DEFN>
If <M>(X,Y)</M>  is jointly absolutely continuous, then <M>E(X|Y=y) = \int_{-\infty}^\infty f_{X|Y}(x,y)\, dx</M>  and 
<D>V(X|Y=y) = E((X-E(X|Y=y))^2|Y=y).</D>
</DEFN> 
The tower property also works as before. 
<HEAD1>Exchangeable distribution</HEAD1>
If <M>X_1, X_2, X_3</M>  are IID, then the joint distribution of <M>(X_1,X_2,X_3)</M>  is the same
 as that of <M>(X_2,X_3,X_1)</M>  or <M>(X_1,X_3,X_2)</M>   or any other permutation of the three random variables. 
This "invariance under permutation" property is called <TERM>exchangability</TERM>, and is found in many joint distributions
 other than the IID set up. 

<DEFN>
We say that the jointly distributed random variables <M>X_1,...,X_n</M>  are <TERM>exchangable</TERM>  if for any permutation
 <M>\pi</M>  of <M>(1,...,n)</M>  the joint distribution of <M>(X_1,...,X_n)</M>  is the same as that of <M>(X_{\pi(1)},...,X_{\pi(n)}).</M>
</DEFN> 
Here is a non-IID example.
<EXM>
In a box we have 10 balls 4 of which are black, the rest being light magenta (with a tinge of yellow on one side). 2 balls
 are drawn one by one using SRSWOR. Let <M>X_1=</M> the indicator of the <M>i</M>-th selected ball being black (<M>i=1,2</M>).
 Then show that <M>X_1,X_2</M>  are exchangeable. 
<SOLN/>
<TABLE>
<TR><TH/><TD><M>X_2=0</M></TD><TD><M>X_2=1</M></TD></TR>
<TR><TD><M>X_1=0</M></TD><TD><M>[[6\times5][10\times9]]</M></TD><TD><M>[[6\times4][10\times9]]</M></TD></TR>
<TR><TD><M>X_1=1</M></TD><TD><M>[[4\times6][10\times9]]</M></TD><TD><M>[[4\times3][10\times9]]</M></TD></TR>
</TABLE>
Since this matrix is symmetric, hence the result.
</EXM>
Obviously such brute force computation will be infeasible if the number of random variables increase. So you will need to
proceed more systematically to answer the next problem.
<EXR>
We have <M>n</M>  balls <M>m</M>  of which are dark purple (the rest being of a nondescript colour). We draw an SRSWOR of
 <M>k</M>  balls. Let <M>X_i=</M>  the indicator of the <M>i</M>-th selected ball being dark purple. Show that <M>X_1,...,X_k</M> 
 are exchangeable.
</EXR> 

<EXR>Consider Polya's urn scheme (5 black 5 white to start with, 1 ball drawn at each step,
 replaced and 1 more ball of the observed colour added). Let <M>X_i=</M> indicator of the
 <M>i</M>-th drawn ball being black. Show that <M>X_1,X_2,...,X_n</M>  are exchangeable for <M>n\in\nn.</M> </EXR>

<THM>
If <M>X_1,...,X_n</M>  are exchangeable, then for any <M>\{i_1,...,i_k\}\seq \{1,...,n\}</M>  the joint distribution of <M>(X_{i_1},...,X_{i_k})</M> 
depends only on <M>k,</M>  and not on  <M>i_1,...,i_k.</M>  
</THM>
<PF>
Let <M>F(x_1,...,x_n)</M>  be the joint CDF of <M>(X_1,...,X_n).</M>  

Let <M>\pi</M>  be any permutation <M>\{1,...,n\}</M>  with <M>\pi(1)=i_1, ..., \pi(k)=i_k.</M>  Then 
by exchangeability <M>F(x_1,...,x_n)</M>  is the joint CDF of <M>(X_{\pi(1)},...,X_{\pi(n)})</M>  as well. 

Then the joint CDF of <M>(X_{i_1},...,X_{i_k})</M>  is <M>F(x_1,...,x_k,\infty,...,\infty),</M>  which is free of <M>i_1,...,i_k,</M> 
 as required.
</PF>

Exchangeable random variables allow for symmetry arguments. The next problem is
<EXR>
If <M>X_1,...,X_n</M>  are exchangeable positive random variables with finite expectations,  then find <M>E((X_1+X_2)/(X_1+\cdots+X_n)).</M>
</EXR>

<HEAD1>Problems for practice</HEAD1>
::<EXR><EIMG web="hpsjoint1.png"/></EXR>
::<EXR><EIMG web="hpsjoint2.png"/></EXR>
::<EXR><EIMG web="hpsjoint3.png"/></EXR>
::<EXR><EIMG web="hpsjoint4.png"/></EXR>
::<EXR><EIMG web="hpsjoint5.png"/></EXR>
::<EXR><EIMG web="hpsjoint6.png"/></EXR>
::<EXR><EIMG web="hpsjoint7.png"/></EXR>
::<EXR><EIMG web="hpsjoint8.png"/></EXR>
::<EXR><EIMG web="hpsjoint9.png"/></EXR>
::<EXR><EIMG web="hpsjoint10.png"/></EXR>
::<EXR><EIMG web="hpsjoint11.png"/></EXR>
::<EXR><EIMG web="rossipmjoint1.png"/></EXR>
::<EXR><EIMG web="rossipmjoint2.png"/></EXR>
::<EXR><EIMG web="rossipmjoint3.png"/></EXR>
::<EXR><EIMG web="rossipmjoint4.png"/></EXR>
::<EXR><EIMG web="rossipmjoint5.png"/></EXR>
::<EXR><EIMG web="rossipmjoint6.png"/></EXR>
::<EXR><EIMG web="rossipmjoint7.png"/></EXR>
::<EXR><EIMG web="rossipmjoint8.png"/></EXR>
::<EXR><EIMG web="rossipmjoint9.png"/></EXR>
::<EXR><EIMG web="rossipmjoint10.png"/></EXR>
::<EXR><EIMG web="rossipmjoint11.png"/></EXR>
::<EXR><EIMG web="rossipmjoint12.png"/></EXR>
::<EXR><EIMG web="rossipmjoint13.png"/></EXR>
</NOTE>@}
