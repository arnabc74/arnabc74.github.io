<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE-BAN-OTF-new.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v19.0" nonce="Q7jTbrCq"></script>

<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else {
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script>
<style>
body,table {
  margin: 0;
  font-size: 40;
  //background: #000;
  //color: #fff;
}

.ans {
  display:none;
  background: #ccffcc;
}

.sticky {
  position: fixed;
  top: 0;
  width: 100%;
  background: #555;
  color: #f1f1f1;
}

.cu {
  background: #ffcccc;
}

.bu {
  background: #ccccff;
}

.scrpt {
  margin:10px;
  border-left: 5px solid black;
}

.box {
  background-color: yellow; 
  //border: 2px solid black;
  display: inline-block;
}

.hl {
  list-style-type: upper-alpha;
}
</style>
<script>
window.onscroll = function() {myFunction()};
window.onload = function() {myInit()};

var header, tphldr;
function myInit() {
  header = document.getElementsByClassName("header");
  tphldr = document.getElementById("topholder");
}

function myFunction() {
  var index = -1
  for(i=0;i<header.length;i++) {
    if (window.pageYOffset > header[i].offsetTop) {
       index = i
    }
    else {
       break
    }
  }

  if(index < 0) 
    tphldr.innerHTML = "";
  else
    tphldr.innerHTML = header[index].innerHTML
}
</script><script type="text/javascript" src="https://arnabc74.github.io/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="file:///home/asu/na/v/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head><body>
<div class="sticky" id="topholder"> </div>
<a href="http://www.isical.ac.in/~arnabc/">[Home]</a>
<h3>Joint distribution</h3>
<ul>
<li>
<a href="#Quick primer on multivariate calculus (part 1)">Quick primer on multivariate calculus (part 1)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Graph">Graph</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Continuity">Continuity</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Differentiability">Differentiability</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Mixed partials">Mixed partials</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 1">Problem set 1</a>
</li>
<li>
<a href="#Quick primer on multivariate calculus (part 2)">Quick primer on multivariate calculus (part 2)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Iterated integrals">Iterated integrals</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 2">Problem set 2</a>
</li>
<li>
<a href="#Joint density">Joint density</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 3">Problem set 3</a>
</li>
<li>
<a href="#Computing probability using iterated integrals">Computing probability using iterated integrals</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 4">Problem set 4</a>
</li>
<li>
<a href="#Joint CDF">Joint CDF</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 5">Problem set 5</a>
</li>
<li>
<a href="#Joint density from CDF">Joint density from CDF</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 6">Problem set 6</a>
</li>
<li>
<a href="#Properties of joint distribution: Non-decreasing">Properties of joint distribution: Non-decreasing</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 7">Problem set 7</a>
</li>
<li>
<a href="#Properties of joint distribution: Limits at $\pm\infty$, right continuity">Properties of joint distribution: Limits at $\pm\infty$, right continuity</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 8">Problem set 8</a>
</li>
<li>
<a href="#Univariate vs multivariate CDF">Univariate vs multivariate CDF</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 9">Problem set 9</a>
</li>
<li>
<a href="#Marginals">Marginals</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 10">Problem set 10</a>
</li>
<li>
<a href="#A subtle difference between joint PDF and PMF">A subtle difference between joint PDF and PMF</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 11">Problem set 11</a>
</li>
<li>
<a href="#Independence">Independence</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 12">Problem set 12</a>
</li>
<li>
<a href="#Expectation using joint density">Expectation using joint density</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 13">Problem set 13</a>
</li>
<li>
<a href="#Expectation of product of independent random variables">Expectation of product of independent random variables</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 14">Problem set 14</a>
</li>
<li>
<a href="#Expectation, variance of random vectors">Expectation, variance of random vectors</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 15">Problem set 15</a>
</li>
<li>
<a href="#Motivation behind the definition of dispersion matrix">Motivation behind the definition of dispersion matrix</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 16">Problem set 16</a>
</li>
<li>
<a href="#Properties of expectation, variance of random vectors">Properties of expectation, variance of random vectors</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 17">Problem set 17</a>
</li>
<li>
<a href="#Conditional density (intuition)">Conditional density (intuition)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 18">Problem set 18</a>
</li>
<li>
<a href="#Conditional density (rigour)">Conditional density (rigour)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 19">Problem set 19</a>
</li>
<li>
<a href="#Exchangeable distribution">Exchangeable distribution</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 20">Problem set 20</a>
</li>
<li>
<a href="#Miscellaneous problems">Miscellaneous problems</a>
</li>
</ul>
<hr/>
$\newcommand{\v}[1]{{\mathbf #1}}$
<title>Joint distribution</title>

<h1><a
name="Quick primer on multivariate calculus (part 1)">Quick primer on multivariate calculus (part 1)</a></h1>
<a href="https://youtu.be/wOwkJg5bjg0">Video for this section</a>
<p></p>
We are going to use certain results from multivariate calculus that you will learn rigourously in the Analysis 3 course.
 For now, we shall only learn some definitions and results from multivariate calculus. 
<h2><a
name="Graph">Graph</a></h2>
When we
 work with $f:{\mathbb R}\rightarrow{\mathbb R}$  we often think about its graph which  we visualise as a curve. When
 we deal with $f:{\mathbb R}^2\rightarrow{\mathbb R}$ 
 we visualise its graph as a surface. 
<p></p>

<h2><a
name="Continuity">Continuity</a></h2>
For  $f:{\mathbb R}\rightarrow{\mathbb R}$  continuity means its graph has no break. Similarly, $f:{\mathbb R}^2\rightarrow{\mathbb R}$  is called continuous,
 when its graph is an unbroken surface, no hole, cut or gap. More rigourously, you can think of 
 continuity in terms limits:
<p></p>
 $f:{\mathbb R}^2\rightarrow{\mathbb R}$  is continuous at $\v a$  means, whenever $\v x\rightarrow \v a$ 
 we have $f(\v x)\rightarrow f(\v a).$  
<p></p>

<h2><a
name="Differentiability">Differentiability</a></h2>
We say that $f:{\mathbb R}\rightarrow{\mathbb R}$  is differentiable at some point $a$  if the graph is smooth
 above $x=a$  (<i>i.e.</i>, may be well-approximated by a straight line passing through $(a,f(a))$ , and the line is not
 vertical. This line is called the tangent to the curve at that point. Any such line has equation of the form $y= f(a)+m\cdot(x-a).$ 
 This $m$   is called the
 derivative of the $f$ 
 at $a.$  
<p></p>
Similarly, $f:{\mathbb R}^2\rightarrow{\mathbb R}$  is called differentiable at some point $(a,b)$  if the surface is smooth over that
 point (<i>i.e.</i>, is well-approximated by a plane passing through $(a,b,f(a,b))$, which is not vertical. Any such 
plane has equation of the form $y= f(a,b)+m_1\cdot(x-a)+m_2\cdot (y-b).$  The pair $(m_1,m_2)$  (which is commonly
 considered as a $1\times 2$  matrix) is called the derivative of $f$  at $(a,b).$  
<p></p>
It turns out that if $f$  is differentiable at $(a,b),$  then $m_1 = \frac{\partial
 f}{\partial x}$  and $m_2 = \frac{\partial f}{\partial y}$  at $(a,b).$  
<p></p>
$\frac{\partial f}{\partial x}$  is obtained by differentiating $f(x,y)$  wrt $x$ 
 along <i>keeping $y$  fixed</i>. Similarly for $\frac{\partial f}{\partial y}.$
<p></p>

<p>
<b>EXAMPLE 1:</b>&nbsp;
If $f(x,y)  = xy^2+y + e^x,$  then $\frac{\partial f}{\partial x} = y^2+e^x.$
 ■
</p>

<p></p>
Just
 the existence of $\frac{\partial f}{\partial x}$  and $\frac{\partial f}{\partial y}$  is not enough to guarantee the
 differentiability of $f.$  However, if the partial derivatives are also continuous over a neighbourhood of $(a,b),$ 
 then $f$  must be differentiable at $(a,b).$
<h2><a
name="Mixed partials">Mixed partials</a></h2>
We can also talk about the mixed partial derivatives $\frac{\partial^2 f}{\partial y\partial x}$  and $\frac{\partial^2 f}{\partial x\partial y}.$ 
<p></p>
Here $\frac{\partial^2 f}{\partial y\partial x}$  means $\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x} \right),$ 
 and 
$\frac{\partial^2 f}{\partial x\partial y}$  means $\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y} \right).$ 
<p></p>

<p>
<b>EXAMPLE 2:</b>&nbsp;
If $f(x,y)  = xy^2+y + e^x,$  then 
$\frac{\partial^2 f}{\partial y\partial x} =
 \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x} \right) = \frac{\partial}{\partial y}(y^2+e^x) = 2y.$
<p></p>
Also 
$\frac{\partial^2 f}{\partial x\partial y} =
 \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y} \right) = \frac{\partial}{\partial x}(2xy+1) = 2y.$
 ■
</p>
Notice that they turn out to be equal in this example. This is mostly the case. 
There are pathological examples, where they are unequal. However, for all the cases we shall need they will be equal. 
<h2><a
name="Problem set 1">Problem set 1</a></h2>

<p>
<b>EXERCISE 1:</b>&nbsp;
For each of the following functions  find $\frac{\partial f}{\partial x}$, $\frac{\partial
 f}{\partial y}$  $\frac{\partial^2
 f}{\partial y\partial x}$  and $\frac{\partial^2 f}{\partial x\partial y}.$
<ol type="">

<li>$f(x,y)  = e^{-x^2-y^2+2x}.$</li>

<li>$f(x,y)  = \frac xy$</li>

<li>$f(x,y)  = \sin x+\cos y.$</li>

<li>$f(x,y)  = xy.$</li>

</ol>

</p>

<p></p>

<h1><a
name="Quick primer on multivariate calculus (part 2)">Quick primer on multivariate calculus (part 2)</a></h1>
<a href="https://youtu.be/ITbIeZdJTCc">Video for this section</a>
<p></p>

<h2><a
name="Iterated integrals">Iterated integrals</a></h2>
Just as we can differentiate $f(x,y)$  wrt a single variable at a time, we can
 integrate it wrt a single variable
 at a time, as well. This is called an <b><font color="red" size="40">iterated integral</font></b>.
The integrand
 is a function of two variables, $x,y.$  Each integral is done wrt one variable. When you do
 the inner integral, you treat the variable for the outer integration as a constant.
<p></p>

<p>
<b>EXAMPLE 3:</b>&nbsp;
$\int_0^1\int_0^{y^2} xy\,dxdy = \int_0^1\left. \frac{x^2y}{2} \right|_0^{y^2} dy = \frac 12\int_0^1
 y^5\,dy = \frac{1}{12}.$
 ■
</p> 
Just as a single variable integrals may be thought of as an area, an iterated integral in two
 variables may be considered as a volume. The iterated integral above gives the volume under the
 surface over the region shown below:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/floor.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center> 

<p></p>
Here we have integrated first wrt $x$  (the inner integral) and then wrt $y$  (the outer integral). We could have
 done it the otherway around: then the iterated integral would have been 
$$\int_0^1 \int_{\sqrt{x}}^1 xy\, dydx.$$  
Check that this also gives the same answer. 
<p></p>
In this example, both the iterated integrals give the same answer. This is the case for
a very general class of integrans (including all nonnegative integrands). However, there are pathological examples where
 they may not be equal. In our course we shall always assume them to be equal. 
<h2><a
name="Problem set 2">Problem set 2</a></h2>

<p></p>

<p>
<b>EXERCISE 2:</b>&nbsp;Find $\int_0^1 \int_{\sqrt{x}}^1 xy\, dydx.$</p>

<p></p>

<p>
<b>EXERCISE 3:</b>&nbsp;What is the volume under the graph of $f(x,y) = x^2+y$  over the region
 $[0,1]\times[1,3]?$ Try both orders of integration ($x$  followed by $y$, and also 
 $y $ followed by $x$).</p>

<h1><a
name="Joint density">Joint density</a></h1>
<a href="https://youtu.be/sKen_7PFUh4">Video for this section</a>
<p></p>
Just as we had encountered joint distribution while learning about discrete random variables, we have the
concept of joint probability density, as well. 
<p></p>

<fieldset>
<legend><b>Definition: </b></legend>
Let $X,Y$  be jointly distributed random variables. We say that they have <b><font color="red" size="40">joint probability density</font></b>  
$f:{\mathbb R}^2\rightarrow[0,\infty)$ if
$$\forall a \leq b, c \leq d~~P\big( (X,Y)\in[a,b]\times[c,d] \big) = \int_a^b \int_c^d f(x,y)\, dydx.$$
</fieldset>
If you are new to this "integral inside integral" notation, it is called an <b><font color="red" size="40">iterated integral</font></b>. 
<p></p>

<p>
<b>EXAMPLE 4:</b>&nbsp;
$\int_0^1\int_0^1 xy^2\,dxdy = \int_0^1\left[\int_0^1 xy^2\,dx\right]dy = \int_0^1\left[y^2\int_0^1 x\,dx\right]dy =\int_0^1\frac 12y^2\,dy =\frac 16.$
 ■
</p>  
To visualise a joint density function, think of its graph as a surface hanging like a roof over
 the $xy$-plane. Then, for any rectangle in the $xy$-plane, the probability of
 $(X,Y)$  being inside that rectangle is the volume of the "tent" with the rectangle as its
 floor, and the surface as its roof. Indeed, thanks to the probability axioms, we can use this "volume of
 tent" idea for floors of shapes other than rectangles as well (<i>e.g.</i>, countable
 unions/intersections of rectangles, and their complements).
<p></p>
The following theorem is not unexpected.
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $f:{\mathbb R}^2\rightarrow[0,\infty)$  is  joint density of some $(X,Y)$, then 
 $\int_{-\infty}^\infty\int_{-\infty}^\infty f(x,y)\, dxdy = 1.$
</fieldset>

<p>
<b><i>Proof:</i></b>This is because the double integral denotes $P(X\in{\mathbb R},\,Y\in{\mathbb R})=1.$.<b><i>[QED]</i></b>
</p>

<p></p>

<p>
<b>EXAMPLE 5:</b>&nbsp;If $f(x,y) = \left\{\begin{array}{ll}c&\text{if }x^2+y^2\leq 1\\ 0&\text{otherwise.}\end{array}\right.$  is a density, then find $c.$
<p></p>
<b>SOLUTION:</b> The total area under the density is the volume of the cylinder with unit radius and height $c.$  This volume
 is $\pi c.$  So we need $\pi c = 1,$  <i>i.e.</i>, $c = \frac 1\pi.$
 ■
</p>

<p>
<b>EXAMPLE 6:</b>&nbsp;Find $c\in{\mathbb R}$  such that $f(x,y) =\left\{\begin{array}{ll}c(x+y)&\text{if }0\leq x,y,\leq 2\\ 0&\text{otherwise.}\end{array}\right. $  is a density.
<p></p>
<b>SOLUTION:</b>
We need $\int_{-\infty}^\infty\int_{-\infty}^\infty f(x,y)\, dx dy = 1,$  <i>i.e.</i>, 
$$\int_0^2\int_0^2 c(x+y)\, dx dy = 1.$$
Now
$$\int_0^2\int_0^2 c(x+y)\, dx dy = c\int_0^2\left[\int_0^2 c(x+y)\, dx\right] dy = c\int_0^2\left[ \frac 12x^2+xy\right]_0^2 dy=c\int_0^2( 2+2y)\, dy=8c.$$
So we need $8c=1$  or $c = \frac 18.$
 ■
</p>

<p></p>

<h2><a
name="Problem set 3">Problem set 3</a></h2>

<p></p>
::<p>
<b>EXERCISE 4:</b>&nbsp;Find $c\in{\mathbb R}$  such that $f(x,y) =\left\{\begin{array}{ll}cxy&\text{if }0\leq x,y,\leq 2\\ 0&\text{otherwise.}\end{array}\right. $  is a density.</p>

<p></p>
::<p>
<b>EXERCISE 5:</b>&nbsp;If $ax+by$  is a density on the unit square, what are the possible values for $a,b?$</p>

<p></p>
::<p>
<b>EXERCISE 6:</b>&nbsp;Find $c\in{\mathbb R}$  such that $f(x,y) =\left\{\begin{array}{ll}ce^{-x-y}&\text{if }0\leq x,y,&lt; \infty\\ 0&\text{otherwise.}\end{array}\right.$
is a density.</p>

<p></p>
::<p>
<b>EXERCISE 7:</b>&nbsp;Find $c\in{\mathbb R}$  such that $f(x,y) =\left\{\begin{array}{ll}cye^{-x}&\text{if }0\leq x,y,&lt; \infty\\ 0&\text{otherwise.}\end{array}\right.
 $  is a density<p><a
href="javascript:hideShow('lab1')"><b>[Hint]</b></a><div
class="ans" id="lab1">Impossible since the integral is divergent.</div></p>
</p>

<p></p>
::<p>
<b>EXERCISE 8:</b>&nbsp;Find $c\in{\mathbb R}$  such that $f(x,y) =\left\{\begin{array}{ll}cxy&\text{if }(x,y)\in[-1,1]\times[0,1]\\ 0&\text{otherwise.}\end{array}\right. $  is a density.
<p><a
href="javascript:hideShow('lab2')"><b>[Hint]</b></a><div
class="ans" id="lab2">Impossible, $f(x,y)$  takes negative values.</div></p>
</p>

<p></p>
::<p>
<b>EXERCISE 9:</b>&nbsp;<font size="-2">[hpsjoint1.png]</font><img width="" src="image/hpsjoint1.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 10:</b>&nbsp;<font size="-2">[hpsjoint2.png]</font><img width="" src="image/hpsjoint2.png" style="vertical-align:text-top;"></p>

<p></p>

<h1><a
name="Computing probability using iterated integrals">Computing probability using iterated integrals</a></h1>
<a href="https://youtu.be/eAOWy_idRSE">Video for this section</a>
<p></p>
So far we have been finding volumnes under the joint density graph using geometry. This works only for very simple shapes.
 For more complicated cases we need to use iterated integrals. 
<p>
<b>EXAMPLE 7:</b>&nbsp;
Let $(X,Y)$  have density $f(x,y) = \left\{\begin{array}{ll}x+y&\text{if }0\leq x,y\leq 1\\ 0&\text{otherwise.}\end{array}\right..$  Find $P(Y\leq X^2).$
<p></p>
<b>SOLUTION:</b>
The random point $(X,Y)$  always lies in the unit square. Our set of interest is shown in
 red below.
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jsq.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
 We need to integrate
 the density over this set. In other words, we are trying to find the volume of the tent with the density as its roof and
 the red region as its floor. 

This may be computed as follows:
$$\int_0^1 \left[\int_0^{x^2} (x+y)\, dy\right] dx = \int_0^1 \left[ xy+\frac 12y^2 \right]_0^{x^2} \,
 dx=\int_0^1 x^3+\frac 12x^4\, dx = \frac 14 + \frac{1}{10} = \frac{7}{20}.$$
We could have done it the other way around, too:
$$\int_0^1 \left[\int_{\sqrt y}^1 (x+y)\, dx\right] dy = \cdots.$$
This should also lead to the same answer (check!). 
 ■
</p>

<h2><a
name="Problem set 4">Problem set 4</a></h2>

<p>
<b>EXERCISE 11:</b>&nbsp;Let $(X,Y)$  have joint density $f(x)=\left\{\begin{array}{ll}cxy&\text{if }x,y\in[0,1],\,x\leq y\\
 0&\text{otherwise.}\end{array}\right..$  Find $P(Y&lt; \sqrt{X}).$</p>

<p></p>

<p>
<b>EXERCISE 12:</b>&nbsp;Let $(X,Y)$  have joint density $f(x)=\left\{\begin{array}{ll}c(x+y)&\text{if }x,y\in[0,1]\\
 0&\text{otherwise.}\end{array}\right..$  Find $P\left(Y&lt; \frac 12\right).$</p>

<p></p>

<p>
<b>EXERCISE 13:</b>&nbsp;Let $X,Y$  be IID $Unif(0,1).$  Find $P(X^2\leq Y \leq X).$</p>

<p></p>

<p>
<b>EXERCISE 14:</b>&nbsp;If $(X,Y)$  has joint density $e^{-(x+y)}$  for $x,y&gt;0,$  (and 0 else), then find $P(X^2+Y^2&lt;1).$
 Leave the answer in terms of a single-variable integral.
</p>

<p></p>

<h1><a
name="Joint CDF">Joint CDF</a></h1>
<a href="https://youtu.be/0pRDbb0_fsk">Video for this section</a>
<p></p>
We have already learned the definition of joint CDF in the last semester:
<p></p>

<fieldset>
<legend><b>Definition: CDF</b></legend>
If $X,Y$  are jointly distributed random variables, then their <b><font color="red" size="40">joint cumulative distribution function</font></b> is defined as
 $F:{\mathbb R}^2\rightarrow[0,1]$, where
$$F(x,y) = P(X\leq x,\, Y\leq y).$$
 </fieldset>
This definition does not care if $X,Y$  are discrete, continuous or has density or not.
<p></p>
Note that for any given $(x,y),$  the value of the CDF, $F(x,y)$  is the probability that the random point $(X,Y)$ 
 lies in the infinite rectangle lying south-west of $(x,y):$
<center>
<table width="100%">
<tr>
<th><img width="" src="image/sw.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Since the CDF is defined in terms of probability, we can compute it by geometry in simple cases, and iterated
 integrals in more compicated cases.
<p>
<b>EXAMPLE 8:</b>&nbsp;Let $(X,Y)$  have uniform distribution over the unit square. Find its CDF, $F(x,y).$
<p></p>
<b>SOLUTION:</b>
The values of $F(x,y)$  over certain regions of ${\mathbb R}^2$  should be clear, as shown below.
<center>
<table width="100%">
<tr>
<th><img width="" src="image/clearcdf.png"></th>
</tr>
<tr>
<th>The unit square is shown in red</th>
</tr>
</table>
</center>
The red square is the floor of the tent. Since its area is 1, and the roof of the tent is flat, horizontal, the height must
 be $\frac 11=1$  to keep the total volume $1.$  
So to find $F(x,y)$  for $(x,y)$  in the red region we just divide the area of the shaded rectangle by the red
 square. 
<center>
<table width="100%">
<tr>
<th><img width="" src="image/redcdf.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
This gives $F(x,y) = xy.$
<p></p>
Similarly, if $(x,y)$  is in the blue region, we need to consider only the red part of the shaded rectangle. 
<center>
<table width="100%">
<tr>
<th><img width="" src="image/bluecdf.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
This gives $y.$
<p></p>
Similar consideration shows $F(x,y) = x$  over the green part. 
<p></p>
So the CDF is
$$F(x,y) = \left\{\begin{array}{ll}xy&\text{if }0&lt; x,y\leq 1\\ 
x&\text{if }0&lt;x\leq 1, y&gt;1\\
y&\text{if }0&lt;y\leq 1, x&gt;1\\
0&\text{if }x\leq 0\mbox{ or } y\leq 0\\
1&\text{if }x, y&gt;1\\
\end{array}\right.. 
$$
<center>
<table width="100%">
<tr>
<th><img width="" src="image/unit.png"></th>
</tr>
<tr>
<th>The graph of the CDF</th>
</tr>
</table>
</center>
 ■
</p>

In this example we could avoid integration because the distribution was uniform. The next example is more general. 
<p></p>

<p>
<b>EXAMPLE 9:</b>&nbsp;Let $(X,Y)$  have density $f(x,y)=x+y$ over the unit square. Find its CDF, $F(x,y).$
<p></p>
<b>SOLUTION:</b>
The red-blue-green break up remains the same here as in the last example, as the support of the distribution is the unit
 square. The values (0 and 1) of the CDF over the white regions are also as before. 
<p></p>
For $(x,y)$  in the red region,
$$F(x,y) = \int_0^x\int_0^y (u+v)\, dudv  = \int_0^x\frac 12y^2+yv\,dv = \frac 12xy^2+xy.$$
Similarly, work out the values for the blue and green regions. ■
</p>

<p></p>

<h2><a
name="Problem set 5">Problem set 5</a></h2>

<p>
<b>EXERCISE 15:</b>&nbsp;Compute the remaining parts of the CDF in the  example above.</p>

<p></p>

<p>
<b>EXERCISE 16:</b>&nbsp;Find the CDF of $(X,Y)$  is the joint density is $f(x,y) = \left\{\begin{array}{ll}e^{-x-y}&\text{if }x,y&gt;0\\ 0&\text{otherwise.}\end{array}\right.$</p>

<p></p>

<h1><a
name="Joint density from CDF">Joint density from CDF</a></h1>
<a href="https://youtu.be/yzjbQ_qotqQ">Video for this section</a>
<p></p>
Finding the CDF from the density requires quite a bit of effort. But going the other way around is a lot easier. 
<p></p>
Suppose that you are given a CDF, $F(x,y)$  for a distribution having a density.  
Then let
$$f(x,y) = \frac{\partial^2}{\partial x\partial y} F(x,y)=\frac{\partial^2}{\partial y\partial x} F(x,y).$$
For $(x,y)$  where the partial derivatives
 fail to exist, set $f(x,y) = 0$  (or any arbitrary non-negative value). This $f(x,y)$  will be a density for CDF
 $F(x,y).$
<p></p>

<p>
<b>EXAMPLE 10:</b>&nbsp;
Let our CDF be
$$F(x,y) = \left\{\begin{array}{ll}xy&\text{if }0&lt; x,y\leq 1\\ 
x&\text{if }0&lt;x\leq 1, y&gt;1\\
y&\text{if }0&lt;y\leq 1, x&gt;1\\
0&\text{if }x\leq 0\mbox{ or } y\leq 0\\
1&\text{if }x, y&gt;1\\
\end{array}\right.. 
$$
You are told that there is a density corresponding to it. Find one such density.
<p></p>
<b>SOLUTION:</b>
Since we are about to differentiate wrt both $x$  and $y,$  the parts of $F(x,y)$ 
 that do not involve both the
 variables must vanish. So we need to work with only the $xy$  part, which after the two differentiations would yield
 $1.$  So a density is 
$f(x,y) = \left\{\begin{array}{ll}1&\text{if }0&lt;x,y&lt;1\\ 0&\text{otherwise.}.\end{array}\right. $
 ■
</p>

<h2><a
name="Problem set 6">Problem set 6</a></h2>

<p>
<b>EXERCISE 17:</b>&nbsp;Find the joint CDF of $(X,Y)$  if $X\sim Bern(1/2)$  and $Y\sim Unif(0,1)$  and they are independent.</p>

<p></p>

<p>
<b>EXERCISE 18:</b>&nbsp;Let $F(x,y)=\min\{x,y\}$  for $0\leq x,y\leq 1$  be the joint CDF of $(X,Y).$ 
 Find $P\left(X\leq \frac 12, Y\leq \frac 12\right)).$</p>

<p></p>

<p>
<b>EXERCISE 19:</b>&nbsp;If $(X,Y)$  have joint density $c(x^2+y)$  over the unit square, then find the joint CDF.</p>

<p></p>

<h1><a
name="Properties of joint distribution: Non-decreasing">Properties of joint distribution: Non-decreasing</a></h1>
<a href="https://youtu.be/xPX68qZ12Q0">Video for this section</a>
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $F(x,y)$  be a bivariate CDF. Then 
<ol type="">

<li>for each fixed value of $y$, the function $x\mapsto F(x,y)$  is non-decreasing.</li>

<li>for each fixed value of $x$, the function $y\mapsto F(x,y)$  is non-decreasing.</li>

</ol> 

</fieldset>

<p>
<b><i>Proof:</i></b>
Fix any $y.$  Fix any $x_1 &lt; x_2.$  Then $F(x_2,y)-F(x_1,y) = P(X\leq x_2, Y\leq
 y)-P(X\leq x_1, Y\leq y)=P(x_1&lt; X\leq x_2, Y\leq y)\geq0.$
<p></p>
Hence the first result. Similarly for the other.
<b><i>[QED]</i></b>
</p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $F(x,y)$  be a bivariate CDF. Then 
$\forall x,y\in{\mathbb R}~~\forall a,b \geq 0~~ F(x,y)-F(x,y-b)-F(x-a,y)+F(x-a,y-b)\geq 0.$
</fieldset>

<p>
<b><i>Proof:</i></b>
Let $A = \{x-a &lt; X \leq x,\, y-b &lt; Y \leq y\}$, $B = \{x-a &lt; X \leq x,\, Y \leq y\}$, $C = \{ X \leq x,\, y-b &lt; Y \leq y\}$,
 and $C = \{ X \leq x-a,\, Y \leq y-b\}.$
<center>
<table width="100%">
<tr>
<th><img width="" src="image/parts.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Then
$$\begin{eqnarray*}
F(x,y) &amp; = &amp; P(A)+P(B)+P(C)+P(D),\\
F(x,y-b) &amp; = &amp; P(B)+P(D),\\
F(x-a,y) &amp; = &amp; P(C)+P(D),\\
F(x-a,y-b) &amp; = &amp; P(D).
\end{eqnarray*}$$
So $F(x,y)-F(x-a,y)-F(x,y-b)+F(x-a,y-b)=P(A)\geq 0.$
<b><i>[QED]</i></b>
</p>
This property is stronger than the non-decreasing properties mentioned earlier.
<p></p>

<h2><a
name="Problem set 7">Problem set 7</a></h2>

<p>
<b>EXERCISE 20:</b>&nbsp;Let $F(x,y)$  be CDF of $(X,Y).$  Then express
$$\lim_{a,b\rightarrow0+} (F(x,y)-F(x-a,y)-F(x,y-b)+F(x-a,y-b))$$
as the probability of some familiar event.
</p>

<p></p>

<p>
<b>EXERCISE 21:</b>&nbsp;For a univariate CDF $F(x)$,  the non-decreasing property was $\forall
 x\in{\mathbb R}~~\forall a&gt;0~~F(x)-F(x-a)\geq 0.$  The proof was to note that this is $P(X\in(x-a,x])).$  
<p></p>
For bivariate CDF $F(x,y)$  the non-decreasing property is 
$\forall x,y\in{\mathbb R}~~\forall a,b \geq 0~~ F(x,y)-F(x,y-b)-F(x-a,y)+F(x-a,y-b)\geq 0.$
<p></p>
The proof is to equate the lefd hand side to $P((X,Y)\in(x-a,x]\times(y-b,y]).$
<p></p>
Generalise this for trivariate CDFs. Drawing a picture would help. Remember the inclusion-exclusion principle.
</p>

<h1><a
name="Properties of joint distribution: Limits at $\pm\infty$, right continuity">Properties of joint distribution: Limits at $\pm\infty$, right continuity</a></h1>
<a href="https://youtu.be/3sl9R0h2gpM">Video for this section</a>
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $F(x,y)$  be a bivariate CDF. Then 
<ol type="">

<li>as $\min\{x,y\}\rightarrow \infty$, we have $F(x,y)\rightarrow 1$.</li>

<li>as $\min\{x,y\}\rightarrow -\infty$, we have $F(x,y)\rightarrow 0$.</li>

</ol> 

</fieldset>

<p>
<b><i>Proof:</i></b>
To show
$$\forall \epsilon&gt;0~~\exists M\in{\mathbb R}~~\forall x,y~~(\min\{x,y\}&gt;M\Rightarrow F(x,y)&gt;1-\epsilon).$$
<p></p>
Take any $\epsilon&gt;0.$
<p></p>
Let $A_n\subseteq\Omega$  be defined as $A_n=\{X\leq n,\, Y\leq n\}.$ 
<p></p>
Then $A_n$'s increase and $\cup_n A_n = \Omega.$
<p></p>
So $P(A_n)\rightarrow 1.$  <i>i.e.</i>, $F(n,n)\rightarrow 1$  as $n\rightarrow \infty.$
<p></p>
Hence $\exists M\in{\mathbb N}~~F(M,M)&gt; 1-\epsilon.$
<p></p>
Choose this $M.$
<p></p>
Take any $x,y$  with $\min\{x,y\} &gt; M.$
<p></p>
Then $F(x,y) \geq F(M,y) \geq F(M,M) &gt; 1-\epsilon,$  as required.
<p></p>
This completes the proof of the first result. 
<p></p>
The second result as a similar proof.
<b><i>[QED]</i></b>
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $F(x,y)$  is the CDF of some $(X,Y)$, then $F$  is "north-east continuous" <i>i.e.</i>, at each $(a,b)\in{\mathbb R}^2$ 
 if $x_n\downarrow a$  and $y_n\downarrow b$, then $F(x_n,y_n)\rightarrow F(a,b).$
</fieldset>

<p>
<b><i>Proof:</i></b>
Let $A_n=\{X\leq x_n,\, Y\leq y_n\}$   and $A=\{X\leq a,\, Y\leq b\}.$
<p></p>
Since $x_n\downarrow a$  and $y_n\downarrow b$, we have $A_n\downarrow A.$
<p></p>
Hence the theorem follows by continuity of probability.  
<b><i>[QED]</i></b>
</p>

<h2><a
name="Problem set 8">Problem set 8</a></h2>

<p>
<b>EXERCISE 22:</b>&nbsp;Let $(X,Y)$  have joint CDF $F(x,y).$  Let $x_n\uparrow a$  and 
$y_n\uparrow  b.$  Then is it true that $F(x_n,y_n)\uparrow F(a,b)$?
</p>

<p></p>

<p>
<b>EXERCISE 23:</b>&nbsp;Let $(X,Y)$  have joint CDF $F(x,y).$
Find 
$\lim_{n\rightarrow \infty} F(x_n,y_n)$  in each of the following cases. Express the limit as the probability of some event
 in terms of $X,Y$, whenever possible.
<ol type="">

<li>$x_n\rightarrow \infty, y_n\rightarrow \infty.$</li>

<li>$x_n\rightarrow \infty, y_n\rightarrow -\infty.$</li>

<li>$x_n\rightarrow -\infty, y_n\rightarrow -\infty.$</li>

<li>$x_n\rightarrow -\infty, y_n\rightarrow \infty.$</li>

<li>$x_n\equiv a, y_n\rightarrow \infty.$</li>

<li>$x_n\equiv a, y_n\rightarrow -\infty.$</li>

<li>$x_n\rightarrow \infty, y_n\equiv b.$</li>

<li>$x_n\rightarrow -\infty, y_n\equiv b.$</li>

<li>$x_n\uparrow a, y_n\uparrow b.$</li>

<li>$x_n\downarrow a, y_n\uparrow b.$</li>

<li>$x_n\uparrow a, y_n\downarrow b.$</li>

<li>$x_n\downarrow a, y_n\downarrow b.$</li>

</ol>

</p>

<h1><a
name="Univariate vs multivariate CDF">Univariate vs multivariate CDF</a></h1>
<a href="https://youtu.be/uk4u5koiPr0">Video for this section</a>
<p></p>
Let $X$  be a random variable with CDF $F.$  Then the following two statements are equivalent:
<ol type="">

<li>$F$  is continuous everywhere.</li>

<li>$\forall a\in{\mathbb R}~~P(X=a)=0.$</li>

</ol>
Consider the corresponding statements in the bivariate scenario. 
<fieldset>
<legend><b><i>Theorem</i></b></legend>Let $(X,Y)$  have joint CDF $F(x,y).$ Consider  the statements
<ol type="">

<li>$F$  is continuous everywhere.</li>

<li>$\forall (a,b)\in{\mathbb R}^2~~P(X=a,\,Y=b)=0.$</li>

</ol>
Here the first statement implies the second statement, but the converse is not true in general. 
</fieldset>

<p>
<b><i>Proof:</i></b>
Let $a,b\in{\mathbb R}^2$  and $a_n\uparrow a$  and $b_n\uparrow b.$  We have
$$F(a,b)-F(x_n,b)-F(a,y_n)+F(x_n,y_n)=P(X\in(a_n,a],\, Y\in(b_n,b]).$$
As $n\rightarrow \infty,$  the left hand side tends to $0,$  since $F(x,y)$  is
 continuous at $(a,b).$  Also the events $\{X\in(a_n,a],\,Y\in(b_n,b]\}\downarrow \{X=a,\,
 Y=b\}.$  
<p></p>
So we have $P(X=a,\, Y=b)=0,$  as required.
<p></p>
A counterexample for the converse is discussed in the exercise below.
<b><i>[QED]</i></b>
</p>

<h2><a
name="Problem set 9">Problem set 9</a></h2>

<p>
<b>EXERCISE 24:</b>&nbsp;Let $X\sim Bernoulli\left(\frac 12\right)$  and $Y$ and density 
$$f(x)=\left\{\begin{array}{ll}1&\text{if }x\in[0,1]\\ 0&\text{otherwise.}\end{array}\right.$$
Let $X$  and $Y$   be independent random
 variables. Write down and sketch the CDFs $F_X(x)$  and $F_Y(y)$  of $X$  and $Y.$
Their joint
 CDF  is $F(x,y)=P(X\leq x,\, Y\leq y) =P(X\leq x)P( Y\leq y) = F_X(x)F_Y(y).$
Find it and fill in the cells below with appropriate formulae for $F(x,y).$  One cell has already been filled in for
 you.
<center>
<table width="100%">
<tr>
<th><img width="" src="image/fexp.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
 Is it continuous everywhere? What is
 $P(X=a,\,Y=b)$  for any given $(a,b)?$</p>

<p></p>
Univariate CDFs are nondecreasing functions, and hence can have only countably many discontinuities.
<blockquote><a
href="javascript:hideShow('reason1')"><b>[Because...]</b></a><div
class="ans" id="reason1">You can put rationals
 in the gaps.</div></blockquote>
However, for bivariate or higher dimensional CDFs, the situation is drastically different. 
<p>
<b>EXERCISE 25:</b>&nbsp;
There are different ways to approach a point in ${\mathbb R}^2.$  The following diagram shows some of them. 
<center>
<table width="100%">
<tr>
<th><img width="" src="image/limdir.png"></th>
</tr>
<tr>
<th>$(a,b)$  is the point at the centre.</th>
</tr>
</table>
</center>
In each case find
 $\lim_{(x,y)\rightarrow(a,b)} F(x,y).$  In each case the limit will be one of  
<blockquote>
$P(X &lt; a,\, Y&lt; b)$, $P(X \leq a,\, Y&lt; b)$, $P(X &lt; a,\, Y\leq b)$  and $P(X \leq a,\, Y\leq b).$
</blockquote> 

</p>

<p></p>

<p>
<b>EXERCISE 26:</b>&nbsp;(Continuation of the last exercise) In exactly three of the cases above we must have
 $\lim_{(x,y)\rightarrow(a,b)} F(x,y) = F(a,b).$  Which three?</p>

<p></p>

<p>
<b>EXERCISE 27:</b>&nbsp;(Continuation of the last exercise) Argue that $F(x,y)$  is discontinuous at
 $(a,b)$  if and only if $P(X &lt; a,\, Y&lt; b) &lt; P(X \leq a,\, Y\leq b).$</p>

<p></p>

<p>
<b>EXERCISE 28:</b>&nbsp;(Continuation of the last exercise) Argue that $F(x,y)$  is discontinuous at
 $(a,b)$  if and only if $P(X \leq a,\, Y= b \mbox{ or }X = a,\, Y\leq b)&gt;0.$</p>

<p>
<b>EXERCISE 29:</b>&nbsp;(Continuation of the last exercise) Sketch the set $\{X \leq a,\, Y= b \mbox{ or }X = a,\, Y\leq b\}$  in the
 $XY$-plane for $(a,b) = (1,2)$  and also for $(a,b) = (1,3).$  Argue that either
 $F(x,y)$  has no discontinuity, or has
uncountably many discontinuities.
</p>

<p></p>

<h1><a
name="Marginals">Marginals</a></h1>
<a href="https://youtu.be/SABBZjtYCE8">Video for this section</a>
<p></p>
We can find the distribution of $X$  and $Y$  separately given the joint distribution of $(X,Y).$  
The distributions of $X$  and $Y$  separately are called their <b><font color="red" size="40">marginal</font></b> 
 distributions. The term "marginal" is actually redundant. "Distribution of $X$ " is the same
 as "marginal distribution of $X$ ". The term is used just to contrast with "joint".  
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let the joint CDF of $(X,Y)$  be $F(x,y).$  Let the marginal CDFs of $X$  and $Y$  be, respectively,
 $F_X(x)$  and $F_Y(y).$  Then
<ul>

<li>$\forall x\in{\mathbb R}~~\lim_{y\rightarrow \infty} F(x,y) =F_X(x),$</li>

<li>$\forall y\in{\mathbb R}~~\lim_{x\rightarrow \infty} F(x,y) =F_Y(y).$</li>

</ul>

</fieldset>

<p>
<b><i>Proof:</i></b>
The event $\{X\leq x,\,Y\leq y\}$  increases to $\{X\leq x\}$ and $y\rightarrow \infty$ 
 and to $\{Y\leq y\}$  as $x\rightarrow \infty.$
<p></p>
 Applying continuity of
 probability, we get the result.
<b><i>[QED]</i></b>
</p>
If $(X,Y)$  has a joint density, then we can obtain  (marginal) densities of $X$  and $Y$  as follows.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $(X,Y)$  has a joint density $f(x,y)$, then a marginal density of $X$  is given by 
$$f_X(x) = \int_{-\infty}^\infty f(x,y)\, dy$$
and a marginal density of $Y$  by 
$$f_Y(y) = \int_{-\infty}^\infty f(x,y)\, dx$$
provided these are continuous and $\forall x\in{\mathbb R}~~\int_{-\infty}^x f_X(t)\, dt = F_X(x)$  and $\forall
 y\in{\mathbb R}~~\int_{-\infty}^y f_Y(t)\, dt = F_Y(y).$
</fieldset>

<p>
<b><i>Proof:</i></b>
Enough to show that $\forall a\leq b\in{\mathbb R}~~P(a\leq X\leq b) = \int_a^b f_X(x)\, dx.$
<p></p>
Take any $a\leq b\in{\mathbb R}.$  
<p></p>
Then 
$$P(a\leq X\leq b) = P(a\leq X\leq b,\, -\infty &lt; Y &lt; \infty) = \int_a^b \int_{-\infty}^\infty f(x,y)\, dy\, dx = \int_a^b f_X(x)\, dx,$$
as required. 
<p></p>
Similarly for $f_Y(y).$
<b><i>[QED]</i></b>
</p>

<h2><a
name="Problem set 10">Problem set 10</a></h2>
::<p>
<b>EXERCISE 30:</b>&nbsp;<font size="-2">[hpsjoint4.png]</font><img width="" src="image/hpsjoint4.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 31:</b>&nbsp;<font size="-2">[hpsjoint5.png]</font><img width="" src="image/hpsjoint5.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 32:</b>&nbsp;<font size="-2">[hpsjoint6.png]</font><img width="" src="image/hpsjoint6.png" style="vertical-align:text-top;"></p>

<p></p>

<h1><a
name="A subtle difference between joint PDF and PMF">A subtle difference between joint PDF and PMF</a></h1>
<a href="https://youtu.be/srZbN5JKwDE">Video for this section</a>
<p></p>
Note that if $X$  and $Y$  are jointly distributed discrete random variables, then immediatly we are assured of
having their joint PMF. But not so in case of densities. Even if
 $X$  and $Y$  each has its own density, still $(X,Y)$  may fail to have a
 <i>joint</i>  density.
<p></p>

<p>
<b>EXAMPLE 11:</b>&nbsp;Suppose $X$ has density $f(x)=\left\{\begin{array}{ll}1&\text{if }x\in(0,1)\\ 0&\text{otherwise.}\end{array}\right.$ and $Y = X.$  
Then show that $(X,Y)$  does not have a joint density.
<p></p>
<b>SOLUTION:</b>
Here the CDF of $(X,Y)$  is 
$$
F(x,y)=P(X\leq x,\, Y\leq y) = P(X\leq\min\{x,y\}) = \left\{\begin{array}{ll}0&\text{if }\min\{x,y\}&lt;0\\ \min\{x,y\}&\text{if }0\leq \min\{x,y\} &lt; 1\\ 1&\text{if }\min\{x,y\} \geq 1\\\end{array}\right..
$$
Hence, if $(X,Y)$  indeed had a joint density, then a joint density would be given
 by $f(x,y)$, where 
$$f(x,y) = \frac{\partial^2}{\partial x\partial y} F(x,y).$$
This forces $f(x,y)\equiv 0,$  which is not a PDF.
   ■
</p>
However, if $(X,Y)$  has a joint density, then both $X$  and $Y$  must also have (marginal) densities.
<p></p>

<h2><a
name="Problem set 11">Problem set 11</a></h2>

<p>
<b>EXERCISE 33:</b>&nbsp;If $X$  has density as above, then does $(X,X^2)$  have a joint density?</p>

<p></p>

<p>
<b>EXERCISE 34:</b>&nbsp;Does there exist a CDF  such that if $X$  has that CDF, then $(X,X)$  has a joint density?</p>

<p></p>

<h1><a
name="Independence">Independence</a></h1>
<a href="https://youtu.be/MYxgNQkhVhY">Video for this section</a>
<p></p>
We already know the following general definition of  jointly distributed random variables being independent:
<fieldset>
<legend><b>Definition: Independence</b></legend>
Let $X_1,...,X_n$  be jointly distributed random variables. We say they are <b><font color="red" size="40">(mutually) independent</font></b>  if
 for all $\{i_1,...,i_k\}\subseteq \{1,...,n\}$  and any $B_1,...,B_k\subseteq{\mathbb R}$  we have
$$P(X_{i_1}\in B_1, ..., X_{i_k}\in B_k) = P(X_{i_1}\in B_1)\times\cdotsP( X_{i_k}\in B_k).$$
</fieldset> 
Incidentally, it is not enough to have $P(X_i\in B_i, X_j\in B_j) = P(X_i\in B_i)P( X_j\in B_j)$  for all $i\neq j.$ 
 If only this holds, then we call $X_1,...,X_n$  only <b><font color="red" size="40">pairwise independent</font></b>, which is weaker than mutual
 independent.
<p></p>
So in particular if $X,Y$  are independent, then 
$$\forall x,y\in{\mathbb R}~~P(X\leq x,\, Y\leq y) = P(X\leq x)\times P(Y\leq y).$$
In other words, the joint CDF factors into the marginal CDFs:
$$\forall x,y\in{\mathbb R}~~F(x,y) = F_X(x)F_Y(y).$$
We had mentioned last semester that CDF characterises the entire distribution (<i>i.e.</i>, if we know the probabilities of all
 events of the form $\{X\leq x\},$  then we can work out $P(X\in B)$  for every event $B$). 
So the next theorem is anticipated.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Two jointly distributed random variables $X,Y$  are independent if and only if
$$\forall x,y\in{\mathbb R}~~F(x,y) = F_X(x)F_Y(y).$$
</fieldset>
This is the general case. Now, if there is a joint density, then that can be factored
 into marginal densities, as well:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Two jointly distributed random variables $X,Y$ having joint density $f(x,y)$ are independent if and only if
$$\forall x,y\in{\mathbb R}~~f(x,y) = f_X(x)f_Y(y),$$
for some marginal densities $f_X$  and $f_Y.$
</fieldset>
 
<p>
<b><i>Proof:</i></b>
<u>If part</u>: For any $x,y\in{\mathbb R}$  we have
$$F(x,y) = P(X\leq x,\,Y\leq y) = \int_{-\infty}^y\int_{-\infty}^x f(x,y)\, dx\,dy =\int_{-\infty}^y\int_{-\infty}^x f_X(x)f_Y(y)\, dx\,dy = \left[\int_{-\infty}^xf_X(x)\,dx\right]\times\left[\int_{-\infty}^y f_Y(y)\,dy\right] = F_X(x)F_Y(y).$$
<p></p>

<u>Only if part</u>: Let $X,Y$  be independent. Let $f_X$  and $f_Y$  be densities for $X$ and $Y.$
Then for any $[a,b]$ and $[c,d]$ we have 
$$\int_a^b\int_c^d f_X(x)f_Y(y)\,dy\,dx =\int_a^b f_X(x) \, dx \int_c^d f_Y(y)\,dy =
 P(X\in[a,b])P(Y\in[c,d]) = P(X\in[a,b],\,Y\in[c,d]).$$
Hence $f_X(x)f_Y(y)$  is a joint density for $(X,Y).$  
<b><i>[QED]</i></b>
</p>

<p></p>
As in the discrete case, here also we have the result that if $X,Y$  are independent, and $E(X), E(Y)$  exist,
 then $E(XY)$  exists and equals $E(X)E(Y).$  The proof is straightforward using factorisation of joint density. 
<h2><a
name="Problem set 12">Problem set 12</a></h2>

<p>
<b>EXERCISE 35:</b>&nbsp;We toss two fair coins independently, and define 3 random variables $X,Y,Z$  based on
 the outcomes as follows. $X=1$  or $0$  according as the first toss shows head or not.
Similarly, $Y=1$  or $0$  according as the second toss shows head or not. $Z=X$ if
 $Y=1$, else $Z=1-X.$  Show that $X,Y,Z$  are pairwise independent, but not mutually independent. 
</p>

<p>
<b>EXERCISE 36:</b>&nbsp;If two independent random variables $X,Y$  have marginal densities $f(t) = e^{-\lambda t}$  for
 $t&gt;0$  (and 0 else), then find the joint density of $(X,Y).$</p>

<p></p>

<p>
<b>EXERCISE 37:</b>&nbsp;$(X,Y)$  is distributed uniformly over the unit disc in ${\mathbb R}^2.$  Are $X,Y$  independent?</p>

<p></p>

<p>
<b>EXERCISE 38:</b>&nbsp;If the joint density of $(X,Y)$  is of the form $f(x)g(y),$  then show that
 $X$  and $Y$  must be independent. Also show that $f_X\propto f$  and $f_Y\propto g.$</p>

<p></p>

<p>
<b>EXERCISE 39:</b>&nbsp;If $(X,Y)$  are independent, then is it true that the joint CDF
is the product of the marginal CDFs?
</p>

<p></p>

<p>
<b>EXERCISE 40:</b>&nbsp;Let $(X,Y)$  have joint density $f(x,y) = g(x) h(y),$  where $g(\cdot)$  and
 $h(\cdot)$  are not necessarily density functions. Find margial densities of $X$  and
 $Y$, and show that $X$  and $Y$  must be independent.</p>

<p></p>

<h1><a
name="Expectation using joint density">Expectation using joint density</a></h1>
<a href="https://youtu.be/udq0I74hJMs">Video for this section</a>
<p></p>
Suppose $X,Y$  are jointly  distributed. We often need to find the expectation of $h(X,Y)$  for some given function
 $h(x,y).$  For this we can of course employ the definition by first defining a new random
 variable,  $Z=h(X,Y)$. This will take us back to the univariate set up, that we already know to handle. 
But a simpler alternative exists. 
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $(X,Y)$  have joint density $f(x,y).$  If $h(X,Y)$  is a non-negative random variable, then 
$$E(h(X,Y)) = \int_{-\infty}^\infty\int_{-\infty}^\infty h(x,y)f(x,y)\, dx\,dy.$$
This always exists (though may be $\infty$). 
</fieldset>
This theorem is the obvious generalisation of the univatiate density case, and like that will be proved once we learn about
 Lebesgue integral later in this course. If $h(X,Y)$  can take both positive and negative values, then we proceed in
 the usual way.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $(X,Y)$  have joint density $f(x,y).$  Let $h(X,Y)$  be a random variable. Let 
$$\begin{eqnarray*}
h(x,y)_+ &amp; = &amp; \max\{h(x,y),0\},\\
h(x,y)_- &amp; = &amp; \max\{-h(x,y),0\}.
\end{eqnarray*}$$ 
Then
$$E(h(X,Y)) = E(h(X,Y)_+)-E(h(X,Y)_-),$$
unless both the expectation on the rhs are $\infty$, in which case $E(h(X,Y))$  is undefined.
</fieldset>

<p>
<b><i>Proof:</i></b>
This follows immediately from the general definition of expectation.
<b><i>[QED]</i></b>
</p>

<p></p>
Again, as in univriate density case, we have a simpler formula for the special case where $E(|h(X,Y)|) &lt; \infty.$
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $(X,Y)$  have joint density $f(x,y).$  If $h(X,Y)$  is a  random variable with $E(|h(X,Y)|) &lt; \infty$,, then 
$$E(h(X,Y)) = \int_{-\infty}^\infty\int_{-\infty}^\infty h(x,y)f(x,y)\, dx\,dy.$$
This must be finite.
</fieldset>

<p></p>
We have the definition of covariance and correlation as before:
$$cov(X,Y) = E[(X-E(X))(Y-E(Y))] = E(XY)-E(X)E(Y),$$
and
$$cor(X,Y) = \frac{cov(X,Y)}{\sqrt{V(X)V(Y)}}.$$
 Cauchy-Schwartz inequality is also the same:
$$cov(X,Y)^2\leq V(X) V(Y),$$
where iequality hold if and only $V(aX+bY) = 0$  for some $a,b\in{\mathbb R}.$
The
 proof that we showed in Probability I was general. An immediate coensequence is that $-1\leq cor(X,Y) \leq 1.$  Also
 $cor(X,Y)=1$  if and only if $V(Y-aX) = 0$  for some $a&gt;0.$  SImilarly, $cor(X,Y)=-1$  holds if and
 only if $V(Y-aX) = 0$  for some $a &lt; 0.$  Check if you remember the proofs. 
<h2><a
name="Problem set 13">Problem set 13</a></h2>

<p>
<b>EXERCISE 41:</b>&nbsp;
Let $(X,Y)$  be uniformly distributed over $S=\{(x,y)~:~0\leq x\leq 1,~x\geq y\geq0\}.$
<ol type="">
<li>Sketch the set $S$  as a shaded subset of ${\mathbb R}^2.$</li>

<li>Write down a joint density for $(X,Y).$</li>

<li>Evaluate $cov(X,Y).$</li>

</ol>

</p>

<p></p>

<p>
<b>EXERCISE 42:</b>&nbsp;
Find $E(X^2Y)$  when $(X,Y)$  has joint density 
$$f(x,y) = \left\{\begin{array}{ll}x+y&\text{if }0&lt; x,y &lt; 1\\ 0&\text{otherwise.}\end{array}\right.$$
</p>

<p></p>

<h1><a
name="Expectation of product of independent random variables">Expectation of product of independent random variables</a></h1>
<a href="https://youtu.be/BYH9b8FLE3I">Video for this section</a>
<p></p>
Last semester we had worked with simple random variables, and had seen the result that if
 $X,Y$  are independent simple random variables, then $E(XY)=E(X)E(Y).$  This result is useful and can be generalised
 to other random variables, as well. However, for a general random variable  expectation may be infinite or undefined. In
 order to avoid $\infty\times 0$  situations, we restrict ourselves to only the random variables with finite
 expectations. Then we have the
 following theorem. 
<fieldset>
<legend><b><i>Theorem</i></b></legend>If $X,Y$  are independent, and $E(X)$, $E(Y)$  both are finite, then
 $E(XY)$  must also be finite, and $E(XY)=E(X)E(Y).$</fieldset>

<p>
<b><i>Proof:</i></b>
While the theorem holds for general random variables, we shall prove it here only for the case when $X,Y$  both have
densitities, $f_X(x)$  and $f_Y(y)$, say.
<p></p>
Since $X,Y$  are independent, hence $(X,Y)$  must have joint density $f(x,y) = f_X(x)f_Y(y).$
<p></p>
We have to work with expectation of $XY$, which may take both positive and negative values.
 So we cannot immediately apply the integration formula for expectation. 
<p></p>
Here this result will come to our help: for a random variable $Z$  we have $E(Z)$ 
 finite if and only $E(|Z|)&lt; \infty.$  We had seen its proof in Probability I (easy proof: $Z=Z_+-Z_-$  and $|Z|=Z_++Z_-$).
The advantage of working with $E(|Z|)$  instead of $E(Z)$  is that $|Z|$  is
 non-negative, and hence always has well-defined expectation (though possibly $\infty$). 
<p></p>
We shall first show that
 $E(|XY|)&lt; \infty,$  which will show the $E(XY)$  is finite, and will let us apply the integration formula. 
<p></p>
Since $|XY|$  is non-negative, hence we can use the integration formula for it:
$$\begin{eqnarray*}
E(|XY|) &amp; = &amp; \int_{-\infty}^\infty\int_{-\infty}^\infty |xy| f(x,y)\, dx\, dy\\
&amp; = &amp;  \int_{-\infty}^\infty\int_{-\infty}^\infty |xy| f_X(x)f_Y(y)\, dx\, dy\\
&amp; = &amp; \int_{-\infty}^\infty |x|f_X(x)\,dx\int_{-\infty}^\infty |y| f_Y(y)\, dy\\
&amp; = &amp; E(|X|)E(|Y|)&lt;\infty.
\end{eqnarray*}$$
<p></p>
Now we are entitled to use the integration formula for $E(XY),$  and so the same logic as above gives
$$\begin{eqnarray*}
E(XY) &amp; = &amp; \int_{-\infty}^\infty\int_{-\infty}^\infty xy f(x,y)\, dx\, dy\\
&amp; = &amp;  \int_{-\infty}^\infty\int_{-\infty}^\infty xyf_X(x)f_Y(y)\, dx\, dy\\
&amp; = &amp; \int_{-\infty}^\infty xf_X(x)\,dx\int_{-\infty}^\infty y f_Y(y)\, dy\\
&amp; = &amp; E(X)E(Y).
\end{eqnarray*}$$
<b><i>[QED]</i></b>
</p>
Two points to be noted about this theorem:
<ol type="">
<li>The converse is not true. We have seen counterexamples even in the simple case.</li>

<li>If $X,Y$  are jointly distributed, and both $E(X)$  and $E(Y)$  are finite,
 even then $E(XY)$  may fail to be finite, or even exist. The exercises below will give some counterexamples.</li>

</ol>

<p></p>

<h2><a
name="Problem set 14">Problem set 14</a></h2>

<p>
<b>EXERCISE 43:</b>&nbsp;Real analysis tells us that $\sum \frac{1}{n^3} &lt; \infty.$  Call this $c.$
 We can manufacture the following PMF from this:
$$p(x) = \left\{\begin{array}{ll}\frac{1}{c x^3}&\text{if }x\in{\mathbb N}\\ 0&\text{otherwise.}\end{array}\right.$$
Let $X$  be a random variable with this PMF. Let $Y = X.$  Show that $E(X), E(Y)$  are both finite, though
 $E(XY)$  is not finite. 
</p>

<p></p>

<p>
<b>EXERCISE 44:</b>&nbsp;Play with the last exercise to come up with a counterexample where $E(X), E(Y)$  are
 both fnite, but $E(XY)$  does not exist.<p><a
href="javascript:hideShow('lab3')"><b>[Hint]</b></a><div
class="ans" id="lab3">
Let $X$  take value $(-1)^n n$  with probability $\frac{1}{cn^3}$  where $c = \sum \frac{1}{n^3} &lt; \infty.$ 
 Take $Y = |X|.$
</div></p>
</p>

<p></p>

<p>
<b>EXERCISE 45:</b>&nbsp;
Let $X$  have density $f(x) =\left\{\begin{array}{ll}\frac 12&\text{if }|x| &lt; 1\\ 0&\text{otherwise.}\end{array}\right. $
Let $Y = X^2.$  Are $X,Y$  independent? Show that $E(XY) = E(X)E(Y).$
</p>

<h1><a
name="Expectation, variance of random vectors">Expectation, variance of random vectors</a></h1>
<a href="https://youtu.be/v25yBxM0laU">Video for this section</a>
<p></p>
If $X_1,...,X_n$  are jointly distributed random variables, then $\v X=(X_1,...,X_n)'$ 
 is called a <b><font color="red" size="40">random vector</font></b>, and is usually considered as an $n\times 1$  column vector. We define 
$E(\v X)$  as
$$E(\v X) = \left[\begin{array}{ccccccccccc}E(X_1)\\\vdots\\E(X_n)
\end{array}\right].$$
The motivation is again from statistical regularity. If you take many IID replications of 
$\v X$  and average them, then (under very general conditions) the average will converge to $E(\v X).$ 
<p></p>
The case of dispersion is slightly trickier. We shall start with the main definition and provide the motivation later. 
<fieldset>
<legend><b>Definition: Dispersion matrix</b></legend>
Let $\v X$  be a random vector with components $X_1,...,X_n.$  Then its <b><font color="red" size="40">dispersion matrix</font></b>  or <b><font color="red" size="40">variance matrix</font></b> 
 or <b><font color="red" size="40">variance-covariance matrix</font></b>  is defined as the $n\times n$  symmetric matrix with $(i,j)$-th entry
 $cov(X_i,X_j).$  In particular, its $i$-th diagonal entry is $V(X_i).$
</fieldset> 

<p></p>

<h2><a
name="Problem set 15">Problem set 15</a></h2>

<p>
<b>EXERCISE 46:</b>&nbsp;If $X_1,...,X_n$  are IID with mean 2 and variance 5, and $\v X = (X_1,...,X_n)'$,
 then find $E(\v X)$  and $V(\v X).$</p>

<p></p>

<p>
<b>EXERCISE 47:</b>&nbsp;If $X_1\sim Binom\left(10,\frac 13\right)$  and $X_2=10-X_1$  and $\v X = (X_1,X_2)'$ 
 then find $E(\v X)$  and $V(\v X).$</p>

<p></p>

<p>
<b>EXERCISE 48:</b>&nbsp;Let $\v X = (X_1,X_2,X_3)'$  have 
$$V(\v X) = \left[\begin{array}{ccccccccccc}
3 &amp; 2 &amp; 1\\
2 &amp; 4 &amp; 2\\
1 &amp; 2 &amp; 5

\end{array}\right].$$
Find $cor(X_1,X_3).$
</p>

<p></p>

<p>
<b>EXERCISE 49:</b>&nbsp;In the last problem, also find $V(X_1-3X_2)$  and $cov(X_1+X_2,X_3).$</p>

<h1><a
name="Motivation behind the definition of dispersion matrix">Motivation behind the definition of dispersion matrix</a></h1>
<a href="https://youtu.be/G5A8ZtFMy84">Video for this section</a>
<p></p>
To motivate the definition of dispersion matrixconsider the bivariate case. Let the two components of our random
 vector be $X$  and $Y.$  If we take
 many IID replications, we get points like $(X_1,Y_1),...,(X_n,Y_n).$  Think of these like a scatterplot. 
<center>
<table width="100%">
<tr>
<th><img width="" src="image/scat1.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
If you look at this cloud of points from position A, then the points appear more scatterred than when we look from B. This
 is an interesting feature of multivariate dispersion, it depends on how you look at it. A good  measure of dispersion
 should not depend on the direction you are looking from, rather it should capture the comprehensive picture,
from which we should be able to work out the dispersion from any desired direction. To achieve 
 this, imagine a ruler placed on the scatterplot with its 0 mark at the origin. Parallel rays of
 light are shining perpendicularly down on the ruler from both
 sides, casting shadows of the points on the ruler:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/scat2.png"></th>
</tr>
<tr>
<th>Light rays (shown in red) are dropping perpendicularly on the ruler</th>
</tr>
</table>
</center>
Then each bivariate point reduces to a single number along the scale, and we may compute variance of the numbers to measure
 the dispersion when looking from that particular direction. To quantify the placement of the ruler, imagine a unit
 vector $\v u$  along the
 ruler from its 0 mark (at the origin) reaching up to its 1 mark.
<center>
<table width="100%">
<tr>
<th><img width="" src="image/scat3.png"></th>
</tr>
<tr>
<th>Projecting a typical point perpendicularly on the ruler</th>
</tr>
</table>
</center>
Then
 a point $\v v \equiv (X,Y)$ 
 will project to the vector  
$$\frac{\v v'\v u}{\v u'\v u} \v u,$$
shown in blue. Measured in units of $\v u$,  this will show up at the mark $\frac{\v v'\v u}{\v u'\v u}$  of the ruler.
<p></p>
Now, $\frac{\v v'\v u}{\v u'\v u} = pX+qY$ for some $p,q\in{\mathbb R}.$  
Then the
 variance of $pX+qY$ 
 will be $p^2 V(X)+q^2 V(Y) + 2pq\, cov(X,Y),$  which may be written as  (check!)
$$\left[\begin{array}{ccccccccccc}p &amp; q
\end{array}\right]\left[\begin{array}{ccccccccccc}V(X) &amp; cov(X,Y)\\cov(X,Y) &amp; V(Y)
\end{array}\right]\left[\begin{array}{ccccccccccc}p\\q
\end{array}\right].$$
Here $p,q$  are controlled by the ruler. Notice that the matrix in the middle does not
 involve $p,q.$  Thus, it contains
 information about dispersion for every
 possible way of placing the ruler.
 This matrix is indeed the dispersion matrix we  defined above.
<p></p>
Here we talked about 1-dim projection of 2-dim. In general, we do 1-dim projection of $n$-dim
 data. To see this visually on a computer screen you may like to consider 2-dim projection of
 3-dim data. Run <a href="3dcloud.r">this R code</a>  on <a href="3ddata.txt">this data
 set</a>  to get a 3D scatterplot that you can turn with your mouse. Depending on how you turn
 it, the points will appear to be closely/loosely clustered.  
<h2><a
name="Problem set 16">Problem set 16</a></h2>

<p>
<b>EXERCISE 50:</b>&nbsp;Consider the toy bivariate data set $(1, 2), (3, 4), (2.1, 3.1), (4, 5).$  Draw the
 scatterplot. Imagine that we are looking down as shown. Guess the variance as seen from that
 direction. Check your guess by actual computation.
<center>
<table width="100%">
<tr>
<th><img width="" src="image/lookex.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

</p>

<p></p>

<p>
<b>EXERCISE 51:</b>&nbsp;Let the dispersion matrix of $(X,Y)$  be $\left[\begin{array}{ccccccccccc}1 &amp; 0\\0 &amp; 2
\end{array}\right]$. Find
 $\theta\in [0,\pi)$  such that $V(\cos (\theta) X + \sin(\theta) Y)$  is maximum. When is the variance minimum.</p>

<p></p>
 
<h1><a
name="Properties of expectation, variance of random vectors">Properties of expectation, variance of random vectors</a></h1>
<a href="https://youtu.be/7hEYnZoKuJg">Video for this section</a>
<p></p>
The following facts are immediate from the definition.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $E(\v X)=\v\mu$  and $V(\v X) = \Sigma,$  then for any matrix $A_{m\times n}$  and any vector $\v b_{m\times 1}$ 
 we have
<ol type="">

<li>$E(A\v X+\v b) = A\v\mu+\v b$,</li>

<li>$V(A\v X+\v b) = A\Sigma A'$</li>

</ol>

</fieldset> 

<p>
<b><i>Proof:</i></b>
Let $\v Y = A\v X+\v b.$  Then $\v Y = (Y_1,...,Y_m)'$, where $Y_i = \sum_j a_{ij}X_j + b_i.$  Here I have
 denoted the $(i,j)$-th entry of $A$  by $a_{ij}.$  
<p></p>
Now compute $E(Y_i)$  and $cov(Y_i,Y_j)$  directly to get the result. 
<p></p>
By the way, the $(i,j)$-th entry of $A\Sigma A'$  is $\sum_r\sum_s a_{ir} \sigma_{rs} a_{js}.$  
<b><i>[QED]</i></b>
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Any dispersion matrix is NND. In other words, if $\Sigma_{n\times n}=V(\v X)$, then $\forall \v\ell\in{\mathbb R}^n~~\v\ell'\Sigma\v\ell\geq 0.$
</fieldset>

<p>
<b><i>Proof:</i></b>
By the last theorem, $\v\ell'\Sigma\v\ell=V(\v\ell'\v X)\geq 0.$
<b><i>[QED]</i></b>
</p>

<p></p>
The converse is also true:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $\Sigma_{n\times n}$  is any NND matrix, then it $V(\v X)$  for some random vector $\v X_{n\times 1}.$ 
</fieldset>

<p>
<b><i>Proof:</i></b>
Let $U_1,...,U_n$  be independent with $\forall i~~V(U_i)=1.$    Then $\v U=
 (U_1,...,U_n)'$  has $V(\v U) = I_n.$  
<p></p>
Since $\Sigma$  is NND, hence $\Sigma = AA'$  for some $A_{n\times n}.$  
<p></p>
Let $\v X = A\v U.$  Then $V(\v X) = A I_n A' = AA' = \Sigma.$ 
<b><i>[QED]</i></b>
</p>

<h2><a
name="Problem set 17">Problem set 17</a></h2>

<p>
<b>EXERCISE 52:</b>&nbsp;Show that for $\left[\begin{array}{ccccccccccc}a &amp; b\\b &amp; c
\end{array}\right]$  to be a dispersion matrix, a necessary
 condition is that $b^2\leq ac.$  Is it a sufficient condition?</p>

<p></p>

<p>
<b>EXERCISE 53:</b>&nbsp;Show that $V(\v X)$  is singular if and only if $P(a_1X_1+\cdots+a_n X_n=c)=1$  for
 some constants $a_i$'s and $c$  such that not all $a_i$'s are zero.</p>

<p></p>

<p>
<b>EXERCISE 54:</b>&nbsp;If $\v X=(X,Y)'$  and $V(\v X)$  is a singular, then how will a scatterplot of
 replications from $\v X$  look like? Here we are running the random experiment underlying $\v
 X$  repeatedly, and getting $(X_1,Y_1), (X_2,Y_2),...,(X_n,Y_n),$  and plotting these
 $n$  points as a scatterplot. Your job is a to identify some geometric pattern in the plot. </p>

<p></p>

<h1><a
name="Conditional density (intuition)">Conditional density (intuition)</a></h1>
<a href="https://youtu.be/ThHaLNSDZKI">Video for this section</a>
<p></p>
So far distributions with densities  behave very similarly to the discrete distributions, with integration replacing summation.  
But we cannot follow the same path for conditional distribution. If $(X,Y)$  are jointly discrete then we defined the
 conditional PMF of $X$  given $Y=y$  as $x\mapsto P(X=x|Y=y) = \frac{P(X=x,Y=y)}{P(Y=y)},$  
and we did this only  for those $y$ for which $P(Y=y)&gt;0.$
<p></p>
But if $(X,Y)$  has a joint density, then $P(Y=y)$  is always 0. So we employ a little trick. For a discrete random variable
 $X$  with PMF $f(x)$  we have $\forall a\in{\mathbb R}~~f(a) = P(X=a).$  But had $f(x)$  been a
 density, then we could not write this anymore,
since $\forall a\in{\mathbb R}~~P(X=a)=0.$  However, if $f$  is
 continuous at $a$, then  we could think 
$$P(X\approx a) = P\left(X\in\left(a-\frac \epsilon2,a+\frac \epsilon2\right) \right) \approx f(a) \epsilon.\hspace{1in} \mbox{(*)}$$
Similarly, if $(X,Y)$  has joint density $f(x,y)$, which is continuous at $(a,b)$  we can say 
$$\begin{eqnarray*}
P(X\approx a,\, Y\approx b) 
&amp; = &amp; P\left(X\in \left(a-\frac \epsilon2, a+\frac \epsilon2\right) ,\,  Y\in \left(b-\frac \epsilon2,b+\frac \epsilon2\right) \right)\\
&amp; \approx &amp; f(a,b)\epsilon^2.
\end{eqnarray*}$$
So instead of working with $P(X=a | Y=b)$  we shall instead work with
$$P(X\approx a | Y\approx b) = \frac{P(X\approx a,\, Y\approx b)}{P(Y \approx b)} \approx \frac{f(a,b) \epsilon^2}{f_Y(b) \epsilon} = \frac{f(a,b)}{f_Y(b)} \epsilon.$$
The similarity between this and (*) immediately leads us to define <b><font color="red" size="40">conditional density</font></b>  of $X$  given $Y=y$ 
 as 
$$f_{X|Y}(x,y) =\frac{f(x,y)}{f_Y(y)}.$$ 
Since we are dividing by $f_Y(y)$, we naturally want $f_Y(y)&gt;0.$  But $f_Y$  being a density can be given
 arbitrary non-negative
 value at any fixed $y.$  To uniquely specify $f_Y(y)$  we naturally assume continuity of $f_Y$  at that point.
 So we arrive at thefollowing definition.
<fieldset>
<legend><b>Definition: Conditional density</b></legend>
Let $(X,Y)$  have joint density, $f(x,y).$  
Then we define <b><font color="red" size="40">conditional density</font></b>  of
 $X$  given $Y=y$ 
 as 
$$f_{X|Y}(x,y) =\frac{f(x,y)}{f_Y(y)}.$$ 
Here we are assuming that  $y$  is such that  $f_Y$
is continuous and positive at  $y.$
</fieldset>
It is obvious that this is a density, since it is non-negative, and 
$\int_{-\infty}^\infty f_{X|Y}(x,y)\, dx = \frac{\int_{-\infty}^\infty f(x,y)\, dx}{f_Y(y)}=1.$
The most glaring difference between conditional PDF and conditional PMF is that the conditional PDF is not a
 conditional probability,
 since $P(Y=y)=0.$  Due to the same reason,
$\int_a^bf_{X|Y}(x,y)\, dx$  does not give $P(X\in [a,b]|Y=y),$  as $P(Y=y)=0.$
<p></p>

<h2><a
name="Problem set 18">Problem set 18</a></h2>

<p>
<b>EXERCISE 55:</b>&nbsp;Find conditional density of $X$  given $Y=\frac 12$  if the joint density of $(X,Y)$  is </p>
$$f(x,y) = \left\{\begin{array}{ll}x+y&\text{if }0&lt; x,y &lt; 1\\ 0&\text{otherwise.}\end{array}\right.$$
<h1><a
name="Conditional density (rigour)">Conditional density (rigour)</a></h1>
<a href="https://youtu.be/-7x_sra_fyg">Video for this section</a>
<p></p>
We defined conditional density in a heuristic way. However, the theorem of total probability is still valid perfectly rigourously:
<p></p>

<fieldset>
<legend><b><i>Total probability</i></b></legend>$\int_c^d \int_a^bf_{X|Y}(x,y)f_Y(y)\, dxdy=P(X\in [a,b], Y\in[c,d]).$</fieldset>

<p>
<b><i>Proof:</i></b>  This is obvious
 from the definition of $f_{Y|X}(x,y):$
$$\begin{eqnarray*}
\int_c^d \int_a^bf_{X|Y}(x,y)f_Y(y)\, dx\,dy &amp; = &amp; \int_c^d \int_a^b\frac{f(x,y)}{f_Y(y)}f_Y(y)\, dx\,dy\\
&amp; = &amp; \int_c^d \int_a^b f(x,y)\,dx\,dy\\
&amp; = &amp; P(X\in [a,b], Y\in[c,d]).\end{eqnarray*}$$
<b><i>[QED]</i></b>
</p>
It is this theorem that justifies the definition of conditional PDF.
<p></p>
As in the discrete case, here also  we have  concepts of
 conditional expectation, conditional variance etc.
<p></p>

<fieldset>
<legend><b>Definition: </b></legend>
If $(X,Y)$  has a joint density $f(x,y),$ then $E(X|Y=y) = \int_{-\infty}^\infty x\,f_{X|Y}(x,y)\, dx$  and 
$$V(X|Y=y) = E((X-E(X|Y=y))^2|Y=y).$$
</fieldset> 
The tower property also works as before, as do the relation between conditional and unconditional variances:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $(X,Y)$  has a joint density, then
<ol type="">

<li>$E(X) = E(E(X|Y)).$</li>

<li>$V(X) = E(V(X|Y)) + V(E(X|Y))$.</li>

</ol>

</fieldset> 

<p>
<b><i>Proof:</i></b>
Enough to show the first, since the other two follow from it (as we have already seen last semester). 
<p></p>
Let $f(x,y)$  be a joint density of $(X,Y).$  Then 
$$E(X|Y=y) = \int_{-\infty}^\infty xf_{X|Y}(x,y)\, dx = \frac{\int_{-\infty}^\infty xf(x,y)\, dx}{f_Y(y)}.$$
<p></p>
So 
$$E(E(X|Y)) = \int_{-\infty}^\infty\frac{\int_{-\infty}^\infty xf(x,y)\, dx}{f_Y(y)}f_Y(y)\, dy = \int_{-\infty}^\infty\int_{-\infty}^\infty xf(x,y)\, dx\, dy =E(X),$$
as required. 
<b><i>[QED]</i></b>
</p>

<h2><a
name="Problem set 19">Problem set 19</a></h2>

<p></p>

<p>
<b>EXERCISE 56:</b>&nbsp;If $(X,Y)$  is uniformly distributed over the triangle $\{(x,y)~:~0\leq x \leq y,\,
 0\leq y\leq 1\}.$  Guess a conditional density of $X$  given $Y=y?$  First try
 to guess, and then check it from the definition.</p>

<p></p>

<p>
<b>EXERCISE 57:</b>&nbsp;Let $X|Y=y$  have density $f_{X|Y}(x,y) = \left\{\begin{array}{ll}c_y x^2&\text{if }x\in[0,y]\\
 0&\text{otherwise.}\end{array}\right.$, where $c_y$  is free of $x.$ 
Let $Y$  be uniformly distributed over $[0,1]$.  Find $f_{Y|X=x}(y,x).$</p>

<p></p>

<p>
<b>EXERCISE 58:</b>&nbsp;If $(X,Y)$  has joint density $f(x,y)=\left\{\begin{array}{ll}x+y&\text{if }0\leq x,y\leq 1\\ 0&\text{otherwise.}\end{array}\right.,$
then find $E(X|Y=y)$  and $V(Y|X=x).$</p>

<p></p>

<h1><a
name="Exchangeable distribution">Exchangeable distribution</a></h1>
If $X_1, X_2, X_3$  are IID, then the joint distribution of $(X_1,X_2,X_3)$  is the same
 as that of $(X_2,X_3,X_1)$  or $(X_1,X_3,X_2)$   or any other permutation of the three random variables. 
This "invariance under permutation" property is called <b><font color="red" size="40">exchangability</font></b>, and is found in many joint distributions
 other than the IID set up. 
<p></p>

<fieldset>
<legend><b>Definition: Exchangeable</b></legend>
We say that the jointly distributed random variables $X_1,...,X_n$  are <b><font color="red" size="40">exchangable</font></b>  if for any permutation
 $\pi$  of $(1,...,n)$  the joint distribution of $(X_1,...,X_n)$  is the same as that of $(X_{\pi(1)},...,X_{\pi(n)}).$
</fieldset> 
Here is a non-IID example.
<p>
<b>EXAMPLE 12:</b>&nbsp;
In a box we have 10 balls 4 of which are black, the rest being light magenta (with a tinge of yellow on one side). 2 balls
 are drawn one by one using SRSWOR. Let $X_1=$ the indicator of the $i$-th selected ball being black ($i=1,2$).
 Then show that $X_1,X_2$  are exchangeable. 
<p></p>
<b>SOLUTION:</b>

<center>
<table style="" border="1">

<tr>
<th colspan="" rowspan=""></th><td colspan="" rowspan="">$X_2=0$</td><td colspan="" rowspan="">$X_2=1$</td>
</tr>

<tr>
<td colspan="" rowspan="">$X_1=0$</td><td colspan="" rowspan="">$\frac{6\times5}{10\times9}$</td><td colspan="" rowspan="">$\frac{6\times4}{10\times9}$</td>
</tr>

<tr>
<td colspan="" rowspan="">$X_1=1$</td><td colspan="" rowspan="">$\frac{4\times6}{10\times9}$</td><td colspan="" rowspan="">$\frac{4\times3}{10\times9}$</td>
</tr>

</table>
</center>
Since this matrix is symmetric, hence the result.
 ■
</p>
Obviously such brute force computation will be infeasible if the number of random variables increase. So you will need to
proceed more systematically to answer the next problem.
<p>
<b>EXERCISE 59:</b>&nbsp;
We have $n$  balls $m$  of which are dark purple (the rest being of a nondescript colour). We draw an SRSWOR of
 $k$  balls. Let $X_i=$  the indicator of the $i$-th selected ball being dark purple. Show that $X_1,...,X_k$ 
 are exchangeable.
</p> 

<p></p>

<p>
<b>EXERCISE 60:</b>&nbsp;Consider Polya's urn scheme (5 black 5 white to start with, 1 ball drawn at each step,
 replaced and 1 more ball of the observed colour added). Let $X_i=$ indicator of the
 $i$-th drawn ball being black. Show that $X_1,X_2,...,X_n$  are exchangeable for $n\in{\mathbb N}.$ </p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X_1,...,X_n$  are exchangeable, then for any $\{i_1,...,i_k\}\subseteq \{1,...,n\}$  the joint distribution of $(X_{i_1},...,X_{i_k})$ 
depends only on $k,$  and not on  $i_1,...,i_k.$  
</fieldset>

<p>
<b><i>Proof:</i></b>
Let $F(x_1,...,x_n)$  be the joint CDF of $(X_1,...,X_n).$  
<p></p>
Let $\pi$  be any permutation $\{1,...,n\}$  with $\pi(1)=i_1, ..., \pi(k)=i_k.$  Then 
by exchangeability $F(x_1,...,x_n)$  is the joint CDF of $(X_{\pi(1)},...,X_{\pi(n)})$  as well. 
<p></p>
Then the joint CDF of $(X_{i_1},...,X_{i_k})$  is $F(x_1,...,x_k,\infty,...,\infty),$  which is free of $i_1,...,i_k,$ 
 as required.
<b><i>[QED]</i></b>
</p>

<p></p>
Exchangeable random variables allow for symmetry arguments. The next problem is one example. 
<h2><a
name="Problem set 20">Problem set 20</a></h2>

<p>
<b>EXERCISE 61:</b>&nbsp;
If $X_1,...,X_n$  are exchangeable positive random variables with finite expectations,  then find $E((X_1+X_2)/(X_1+\cdots+X_n)).$
</p>

<p></p>

<p>
<b>EXERCISE 62:</b>&nbsp;Three dice are rolled and their outcomes are called $X_1,X_2$   and $X_3.$  Let
 $Y_1 = X_1+X_2,$  $Y_2 = X_2+X_3,$  and $Y_3 = X_3+X_1.$  Is $(Y_1,Y_2,Y_3)$ 
 exchangeable? Justify your answer.</p>

<p></p>

<p>
<b>EXERCISE 63:</b>&nbsp;A box contains 10 balls numbered 1 to 10. A ball is drawn at random, and its number noted.
 Without replacing the ball, another ball is drawn at random from the rest, and its number is also
 noted. If the two numbers are $X$  and $Y$, respectively, then is $(X,Y)$  exchangeable?</p>

<p></p>

<p>
<b>EXERCISE 64:</b>&nbsp;(Continuation of the last exercise) Solve the last problem if at each step the ball with
 number $i$  on it  is selected with probability proportional to $i.$
</p>

<p></p>

<h1><a
name="Miscellaneous problems">Miscellaneous problems</a></h1>
::<p>
<b>EXERCISE 65:</b>&nbsp;<font size="-2">[hpsjoint3.png]</font><img width="" src="image/hpsjoint3.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 66:</b>&nbsp;<font size="-2">[hpsjoint7.png]</font><img width="" src="image/hpsjoint7.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 67:</b>&nbsp;<font size="-2">[hpsjoint8.png]</font><img width="" src="image/hpsjoint8.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 68:</b>&nbsp;<font size="-2">[hpsjoint9.png]</font><img width="" src="image/hpsjoint9.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 69:</b>&nbsp;<font size="-2">[hpsjoint10.png]</font><img width="" src="image/hpsjoint10.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 70:</b>&nbsp;<font size="-2">[hpsjoint11.png]</font><img width="" src="image/hpsjoint11.png" style="vertical-align:text-top;">
<p></p>
Here $X_i = U_{(i)}$  in our notation, and  $R=U_{(n)}-U_{(1)}$  is the range of the $U_i$'s.
<p></p>
I think this problem should better be attacked after learning about order statistics <a href="trans.html">in the next chapter</a>.
</p>
::<p>
<b>EXERCISE 71:</b>&nbsp;<font size="-2">[rossipmjoint1.png]</font><img width="" src="image/rossipmjoint1.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 72:</b>&nbsp;<font size="-2">[rossipmjoint2.png]</font><img width="" src="image/rossipmjoint2.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 73:</b>&nbsp;<font size="-2">[rossipmjoint3.png]</font><img width="" src="image/rossipmjoint3.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 74:</b>&nbsp;<font size="-2">[rossipmjoint4.png]</font><img width="" src="image/rossipmjoint4.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 75:</b>&nbsp;<font size="-2">[rossipmjoint5.png]</font><img width="" src="image/rossipmjoint5.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 76:</b>&nbsp;<font size="-2">[rossipmjoint6.png]</font><img width="" src="image/rossipmjoint6.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 77:</b>&nbsp;<font size="-2">[rossipmjoint8.png]</font><img width="" src="image/rossipmjoint8.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 78:</b>&nbsp;<font size="-2">[rossipmjoint9.png]</font><img width="" src="image/rossipmjoint9.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 79:</b>&nbsp;<font size="-2">[rossipmjoint10.png]</font><img width="" src="image/rossipmjoint10.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 80:</b>&nbsp;<font size="-2">[rossipmjoint11.png]</font><img width="" src="image/rossipmjoint11.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 81:</b>&nbsp;<font size="-2">[rossipmjoint12.png]</font><img width="" src="image/rossipmjoint12.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 82:</b>&nbsp;<font size="-2">[rossipmjoint13.png]</font><img width="" src="image/rossipmjoint13.png" style="vertical-align:text-top;"></p>
<hr/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/></body></html>
