<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE-BAN-OTF-new.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v19.0" nonce="Q7jTbrCq"></script>

<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else {
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script>
<style>
body,table {
  margin: 0;
  font-size: 40;
  //background: #000;
  //color: #fff;
}

.ans {
  display:none;
  background: #ccffcc;
}

.sticky {
  position: fixed;
  top: 0;
  width: 100%;
  background: #555;
  color: #f1f1f1;
}

.cu {
  background: #ffcccc;
}

.bu {
  background: #ccccff;
}

.scrpt {
  margin:10px;
  border-left: 5px solid black;
}

.box {
  background-color: yellow; 
  //border: 2px solid black;
  display: inline-block;
}

.hl {
  list-style-type: upper-alpha;
}
</style>
<script>
window.onscroll = function() {myFunction()};
window.onload = function() {myInit()};

var header, tphldr;
function myInit() {
  header = document.getElementsByClassName("header");
  tphldr = document.getElementById("topholder");
}

function myFunction() {
  var index = -1
  for(i=0;i<header.length;i++) {
    if (window.pageYOffset > header[i].offsetTop) {
       index = i
    }
    else {
       break
    }
  }

  if(index < 0) 
    tphldr.innerHTML = "";
  else
    tphldr.innerHTML = header[index].innerHTML
}
</script><script type="text/javascript" src="https://arnabc74.github.io/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="file:///home/asu/na/v/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head><body>
<div class="sticky" id="topholder"> </div>
<a href="http://www.isical.ac.in/~arnabc/">[Home]</a>
<h3>Multivariate normal and related distributions</h3>
<ul>
<li>
<a href="#Multivariate normal distribution (part 1)">Multivariate normal distribution (part 1)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Mean and dispersion">Mean and dispersion</a>
</li>
<li>
<a href="#Multivariate normal distribution (part 2)">Multivariate normal distribution (part 2)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Nonsingular and singular">Nonsingular and singular</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 1">Problem set 1</a>
</li>
<li>
<a href="#Multivariate normal distribution (part 3)">Multivariate normal distribution (part 3)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 2">Problem set 2</a>
</li>
<li>
<a href="#$\chi^2$  distribution">$\chi^2$  distribution</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Geometry of $\chi^2_{(k)}$">Geometry of $\chi^2_{(k)}$</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 3">Problem set 3</a>
</li>
<li>
<a href="#$t$  and $F$  distributions">$t$  and $F$  distributions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#$t$  distributions">$t$  distributions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#$F$  distributions">$F$  distributions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 4">Problem set 4</a>
</li>
<li>
<a href="#Sampling distributions for normal sample">Sampling distributions for normal sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 5">Problem set 5</a>
</li>
<li>
<a href="#Miscellaneous problems">Miscellaneous problems</a>
</li>
</ul>
<hr/>
$\newcommand{\v}[1]{{\mathbf #1}}$
$\newcommand{\k}[1]{\chi^2_{(#1)}}$
<title>Multivariate normal and related distributions</title>

<h1><a
name="Multivariate normal distribution (part 1)">Multivariate normal distribution (part 1)</a></h1>
Here we shall discuss the most commonly used multivariate distribution, the multivariate normal distribution. 
<fieldset>
<legend><b>Definition:  Multivariate normal</b></legend>
Let $\v X = (X_1,...,X_n)'$  be a random vector, where  $X_1,...,X_n$  are IID
 $N(0,1).$ Let $A_{m\times n}$  be any fixed matrix, and $\v b_{m\times1}$  be any
 fixed vector. Then the joint distribution of $\v Y=A\v X + \v b$  is called $m$-dimensional
<b><font color="red" size="40">multivariate normal</font></b>. 
</fieldset>
It turns out that $\v Y$  may not have joint density for certain choices of $A.$  Also, the joint CDF of $\v Y$ 
 not easy to write down. So, in order to deal with the multivariate normal family of distributions in general, we use characteristic function.
First, let us define characteristic function for a multivariate distribution.
<p></p>

<fieldset>
<legend><b>Definition: multivariate characteristic function</b></legend>
Let $\v U = (U_1,...,U_m)'$  be a random vector. Then its <b><font color="red" size="40">characteristic function</font></b>  is defined as $\xi:{\mathbb R}^m\rightarrow{\mathbb C}$ 
 where
$$\xi(t_1,...,t_m)\equiv \xi(\v t) = E(e^{i\v t'\v U}).$$
</fieldset>
The followin theorem, which is an obvious analogue for the corresponding univariate theorem, is what makes characteristic function
 useful.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
For any ${\mathbb R}^m$-valued random vector, its characteristic function exists. Also, the characteristic function uniquely
 determines the distribution.
</fieldset>

<p>
<b><i>Proof:</i></b>Not in this course.<b><i>[QED]</i></b>
</p>
Now let us return to multivariate normal, and find its characteristic function.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X_1,...,X_n$  be IID $N(0,1).$  Let $A_{m\times n}$  be any fixed matrix and let $\v b_{m\times 1}$ 
 be any fixed vector. Then $\v Y_{m\times 1} = A\v X+\v b$  has characteristic function
$\exp\left(-\frac 12\v t'AA'\v t+i\v t'\v b\right).$
</fieldset>

<p>
<b><i>Proof:</i></b>
Here $\v X$ has characteristic function 
$$E(\exp(i\v t'\v X) = E(e^{it_1X_1+\cdots it_nX_n}) =E(e^{it_1X_1}\cdots e^{it_nX_n})=
 E(e^{it_1X_1})\cdots E(e^{it_nX_n})) = e^{-t_1^2/2}\cdots e^{-t_n^2/2} = \exp\left(-\frac 12\v t'\v t\right).$$ 
So the characteristic function of $A\v X + \v \mu$  is 
$$E(\exp(i\v t'(A\v X+\v b)) =E(\exp(i((A'\v t)'\v X+\v t'\v b)) = \exp\left(-\frac 12\v t'AA'\v t+i\v t'\v b\right),$$
as required.
<b><i>[QED]</i></b>
</p>
Thus, we see that
 different values of $A$  and $\v b$  may
 lead to the same multivariate normal. The characteristic function shows that $A_1,\v b_1$  and $A_2,\v
 b_2$  lead to the same distribution if and only if $A_1A_1' = A_2A_2'$  and $\v b_1=\v
 b_2.$  
In other words, the distribution involves $\v b$  directly, but $A$  only through $AA'.$ 
So we parametrise
 multivariate normal by $\v b$  and $AA'.$ 
 These are more commonly denoted by $\v \mu$  and $\Sigma,$  and $m$-dimensional multivariate normal with
 these parameters is denoted by $N_m(\v \mu, \Sigma).$  The abbreviation MVN is also used for multivariate normal.
<p></p>
What are the possible values for $\v\mu$  and $\Sigma$? Well, since $\v\mu$  is just $\v b,$  which was
 any arbitrary vector in ${\mathbb R}^m,$  so $\v \mu$  can be any vector in ${\mathbb R}^m.$  The situation is slightly more
 tricky for $\Sigma.$   , because it is any matrix of the form $AA'$  where $A_{m\times n}$  is any arbitrary
 real matrix (for arbitrary $n\in{\mathbb N}$). Thanks to the following theorem from linear algebra, we see that
$\Sigma$  can be any $m\times m$  NND matrix.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
A matrix $\Sigma_{m\times m}$  is NND if and only if $\Sigma = AA'$  for some $A_{m\times n}$  for some $n\in{\mathbb N}.$
</fieldset>

<p>
<b><i>Proof:</i></b>Should be covered in your Vectors and Matrices course.<b><i>[QED]</i></b>
</p>

<p></p>

<h2><a
name="Mean and dispersion">Mean and dispersion</a></h2>
It is easy to find the mean vector and variance matrix of a multivariate normal distribution:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $\v Y\sim N_m(\v\mu,\Sigma)$, then  $E(\v Y)=\v b$   and $V(\v Y) = \Sigma.$ 
</fieldset>

<p>
<b><i>Proof:</i></b>
Let $\Sigma = AA'.$  Then $\v Y$  has the same distribution as $A\v X+\v \mu$  where the $\v X$  has
 components IID $N(0,1).$
<p></p>
So $E(\v X) = \v 0$  and $V(\v X) = I.$  
<p></p>
Hence $E(\v Y) = E(A\v X+\v\mu) = A E(\v X)+\v\mu = \v\mu,$
and $V(\v Y) = V(A\v X+\v\mu) = A V(\v X)A' = AA'=\Sigma.$ 
<b><i>[QED]</i></b>
</p>

<h1><a
name="Multivariate normal distribution (part 2)">Multivariate normal distribution (part 2)</a></h1>
As we have already mentioned, a multivariate normal distribution need not always have a density. However, for an important
 special case, it does. This case is called the nonsingular
 case, while the other case is called singular. We discuss these next.
<p></p>

<h2><a
name="Nonsingular and singular">Nonsingular and singular</a></h2>

<fieldset>
<legend><b><i>Theorem</i></b></legend>$N_m(\v\mu,\Sigma)$  admits a density if and only if $\Sigma$  is a nonsingular  matrix. 
In this case, the density is 
$$\frac{1}{\sqrt{(2\pi)^n det(\Sigma)}}\exp\left(-\frac 12(\v y-\v\mu)'\Sigma ^{-1}(\v y-\v\mu)\right)\mbox{ for }\v y\in{\mathbb R}^n,$$ 
<p></p>

</fieldset>

<p>
<b><i>Proof:</i></b>
 Since $\Sigma$ 
 is anyway NND, this extra assumption is equivalent to $\Sigma$  being PD.
<p></p>

<u>If part</u>: Since $\Sigma$  is PD, we can write $\Sigma = AA'$  for some nonsingular $A.$  So $N_m(\v\mu,\Sigma)$ 
 is the distribution of $A\v X+\v\mu,$  where $\v X$  has IID $N(0,1)$  components.
<p></p>
Clearly, the density of $\v X$  is 
$$\frac{1}{\sqrt{(2\pi)^n}}\exp\left(-\frac 12\v x'\v x\right)\mbox{ for }\v x\in{\mathbb R}^n.$$
Since $A$  is  nonsingular,  the transform $\v X\mapsto A\v X+\v \mu$  is a bijection. So the Jacobian formula
 gives (check!) the following density for $\v Y = A\v X+\v \mu$
$$\frac{1}{\sqrt{(2\pi)^n det(AA')}}\exp\left(-\frac 12(\v y-\v\mu)'(AA') ^{-1}(\v y-\v\mu)\right)\mbox{ for }\v y\in{\mathbb R}^n.$$ 
Here we have used the fact that $\sqrt{det(AA')} = \sqrt{det(A)det(A')} = |det(A)|.$
The density may be written as
$$\frac{1}{\sqrt{(2\pi)^n det(\Sigma)}}\exp\left(-\frac 12(\v y-\v\mu)'\Sigma ^{-1}(\v y-\v\mu)\right)\mbox{ for }\v y\in{\mathbb R}^n,$$ 
as required.
<p></p>

<u>Only if part</u>: Actually, this part does not even require normality. Let $\v Y$  have dispersion matrix $\Sigma$ 
 which is singular. 
<p></p>
Let, if possible, $\v Y$  have density.
<p></p>
Then $\exists \v a\neq\v0~~\Sigma \v a = \v 0.$  So $\v a'\Sigma \v a = 0.$   
<p></p>
But $\v a'\Sigma \v a = V(\v a' \v Y),$  hence we see that $\v a'\v Y$  must be a constant with probability 1.
<p></p>
We can extend $\{\v a\}$  to a basis $\{\v a,...\}$  of ${\mathbb R}^m.$ 
<p></p>
Let $P = \left[\begin{array}{ccccccccccc}\v a &amp; \cdots 
\end{array}\right]$  be the matrix with these as columns.
<p></p>
Then $P$  is nonsingular, and so $\v Z = P'\v Y$  is a bijective transform of $\v Y.$
<p></p>
So, by the Jacobian formula, $\v Z$  must also have joint density. Then its first component $\v a'\v Y$  must also
 have a (marginal) density. But that is impossible, since it is a degenrate random variable.
<p></p>
Hence the result. 
<p></p>

<b><i>[QED]</i></b>
</p>

<p></p>

<h2><a
name="Problem set 1">Problem set 1</a></h2>

<p>
<b>EXERCISE 1:</b>&nbsp;Describe $N_2(\v0,I)$  distribution.</p>

<p></p>

<p>
<b>EXERCISE 2:</b>&nbsp;Let $J_2$  be the $2\times2$  matrix with all entries equal to 1. Let 
$$\left[\begin{array}{ccccccccccc}X\\Y
\end{array}\right] \sim N_2(\v 0, J_2).$$
If we take data $(x_1,y_1),...,(x_n,y_n)$  from $(X,Y)$ , what will the scatterplot look like?
</p>

<p></p>

<h1><a
name="Multivariate normal distribution (part 3)">Multivariate normal distribution (part 3)</a></h1>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X_1,...,X_n$  are IID $N(\mu, \sigma^2),$  then  $\v X = (X_1,...,X_n)'\sim N_n((\mu,...,\mu)',\sigma^2 I_n).$
</fieldset>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $\v X\sim N_n(\v \mu, \Sigma)$  and $B_{m\times n}$  and $\v c_{m\times 1}$  are fixed, then $B\v X+\v c\sim N_m(B\v \mu + \v c, B\Sigma B').$
</fieldset>

<p>
<b><i>Proof:</i></b>
Directly from definition. Just be careful that the variance matrix is $B\Sigma B'$  and not $B'\Sigma B.$
<b><i>[QED]</i></b>
</p>

<p></p>
In particular, we have the following corrolary.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $\v X = (X_1,...,X_n)'\sim N_n(\v \mu,\Sigma),$  then any subvector of  $\v X$  has multivariate normal distribution
 with the corresponding subsector of $\v \mu$  and principal submatrix of $\Sigma.$
</fieldset>

<p></p>
If two random variables are
independent, they must also be uncorrelated. However, the converse is not true. For multivariate normal the converse is also
 true. 
<fieldset>
<legend><b><i>Theorem</i></b></legend>If 
$$\v X = \left[\begin{array}{ccccccccccc}\v X_1\\\v X_2
\end{array}\right]'\sim N_n\left(\underbrace{\left[\begin{array}{ccccccccccc}\v \mu_1\\\v \mu_2
\end{array}\right] }_{\v\mu},
\underbrace{\left[\begin{array}{ccccccccccc}\Sigma_{11} &amp;  \Sigma_{12}\\\Sigma_{12}' &amp; \Sigma_{22}
\end{array}\right] }_\Sigma\right),$$  then
 $\v X_1$  and $\v X_2$  are
 independent if and only if $\Sigma_{12} = O.$</fieldset>

<p>
<b><i>Proof:</i></b>
The characteristic function of $\v X$  is 
$\xi_{\v X}(\v t)=\exp\left(-\frac 12\v t'\Sigma\v t+i\v t'\v \mu\right).$
<p></p>
Writing $\v t =\left[\begin{array}{ccccccccccc}\v t_1\\\v t_2
\end{array}\right], $  we have
$$\v t'\Sigma\v t =\left[\begin{array}{ccccccccccc}\v t_1' &amp; \v t_2'
\end{array}\right]\left[\begin{array}{ccccccccccc}\Sigma_{11} &amp;  \Sigma_{12}\\\Sigma_{12}' &amp; \Sigma_{22}
\end{array}\right]\left[\begin{array}{ccccccccccc}\v t_1\\\v t_2
\end{array}\right]
= \v t_1'\Sigma_{11}\v t_1+ \v t_1'\Sigma_{22}\v t_2,$$
since $\Sigma_{12}=O.$
<p></p>
Again
$$\v t'\v \mu = \left[\begin{array}{ccccccccccc}\v t_1' &amp; \v t_2'
\end{array}\right]\left[\begin{array}{ccccccccccc}\v \mu_1 &amp; \v \mu_2
\end{array}\right]  = \v t_1'\v \mu+\v t_2'\v \mu_2$$
So the characteristic function factorises as
$$\xi_{\v X}(\v t)\equiv \xi_{\v X_1}(\v t_1)\xi_{\v X_1}(\v t_2),$$
and hence $\v X_1$  and $\v X_2$  are independent, as required.
<b><i>[QED]</i></b>
</p>

<p></p>

<h2><a
name="Problem set 2">Problem set 2</a></h2>

<p></p>
Next we shall discuss some distributions related to the normal distribution.
<h1><a
name="$\chi^2$  distribution">$\chi^2$  distribution</a></h1>
If $X_1,...,X_n$  are IID $N(0,1)$  then the distribution of $\sum_1^n X_i^2$  is called <b><font color="red" size="40">chi-square distribution wit degrees of freedom</font></b> 
 $n.$  We write 
$$\sum_1^n X_i^2\sim \chi_{(n)}^2.$$
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
$\k n$  is the same as $Gamma\left(\frac n2,\frac 12\right).$
</fieldset>

<p>
<b><i>Proof:</i></b>
We shall proceed by induction.
<p></p>

<u>Basis</u>: $n=1$: Let $X\sim N(0,1).$
<p></p>
Then $X^2$  has CDF $F(\cdot),$  where $F(a)=0$  for $a&lt;0$  and for $a\geq 0$  we have
$$F(a) = P(X^2\leq a) =\frac{1}{\sqrt{2\pi}} \int_{-\sqrt a}^{\sqrt a} e^{-x^2/2}\, dx=\frac{2}{\sqrt{2\pi}} \int_0^{\sqrt a} e^{-x^2/2}\, dx.$$
Differentiating wrt $a$  we get the density
$$f(a) = F'(a) = \frac{2}{\sqrt{2\pi}} e^{-a/2}\mbox{ for }a&gt;0.$$
We immediately recognise it as the $\k 1$  density, completing the basis.
<p></p>

<u>Hypothesis</u>: Assume the result for $n=1,...,m$  fir some $m\geq 1.$
<p></p>

<u>Step</u>: Shall show for $n=m+1.$  
<p></p>
Let $X_1,....,X_m,X_{m+1}$  be IID $N(0,1).$  
<p></p>
Then $\sum_1^{m+1} X_i^2 = \underbrace{\sum_1^m X_i^2}_Y + \underbrace{X_{m+1}^2}_Z.$
<p></p>
By the inducion hypothesis both $Y\sim Gamma\left(\frac m2,\frac 12\right)$  and $Z\sim Gamma\left(\frac 12,\frac 12\right).$ 
<p></p>
Also they are independent. 
<p></p>
So $Y+Z\sim Gamma\left(\frac{m+1}{2},\frac 12\right),$  as required. 
<b><i>[QED]</i></b>
</p>

<h2><a
name="Geometry of $\chi^2_{(k)}$">Geometry of $\chi^2_{(k)}$</a></h2>
If we consider a random vetor $\v X$  in ${\mathbb R}^k$  with IID $N(0,1)$  components, $\|\v X\|^2\sim \k k.$ 
 This is little more than the definition. 
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>Now let be $\v X\sim N_n(\v 0, I).$  Let $S$  be any $k$-dimensional subspace of
 ${\mathbb R}^n. $  Consider the orthogonal projection $\v Y$  of $\v X$  onto $S.$ Then $$\|\v Y\|^2\sim \k k.$$ 
 </fieldset>

<p>
<b><i>Proof:</i></b>
We take any ONB of $S$  and extend it to an ONB of ${\mathbb R}^n. $  Pack the ONB as rows to get an orthogonal matrix
 $Q.$  
<p></p>
Then $\v Z=Q\v X\sim N_n(\v 0, I).$
<p></p>
Also $\|\v Y\|^2 = \sum_1^k Z_i^2\sim\k k,$  as required.
<b><i>[QED]</i></b>
</p>

<p></p>

<p>
<b>EXERCISE 3:</b>&nbsp;
Let $A$  be a symmetric, idempotent matrix. Let $\v X'A\v X\sim\k{r(A)}.$  Show tjis. 
</p>
The following result is used in linear models. 
<p>
<b>EXERCISE 4:</b>&nbsp;Let $\v X\sim N_n(\v \mu, I).$  Let $S$  be any $k$-dimensional subspace
 containing $\v \mu.$  Then show that the orthogonal projection of $\v X$  onto
 $S^\perp$  must have $\k {n-k}$  distribution.</p>

<p></p>

<fieldset>
<legend><b>Definition: Non-central $\k$</b></legend>
If $X_1,...,X_n$  are independent $N(\mu_i,1),$  then the distribution $\sum X_i^2$  is
 called <b><font color="red" size="40">noncentral $\k n$  with noncentralty parameter $\sum_i\mu_i^2.$</font></b> 

</fieldset>

<h2><a
name="Problem set 3">Problem set 3</a></h2>

<h1><a
name="$t$  and $F$  distributions">$t$  and $F$  distributions</a></h1>

<h2><a
name="$t$  distributions">$t$  distributions</a></h2>

<fieldset>
<legend><b>Definition: $t$-distribution</b></legend>
If $X\sim N(0,1)$  and $Y\sim\k n$  and they are independent, then the distribution of $X/(\sqrt{Y/n})$  is
 called $t$-distribution with $n$  degrees of freedom. Here $n&gt;0$  need not be an integer.
</fieldset>
Let us derive density of $t$-distribution with $n$  degrees of freedom. We shall do this step by step. 
<p></p>
From $Y$  we shall pass on to $Z = \sqrt{\frac Yn}$  and then to $\frac XZ.$  
<p></p>
Y has density $f_Y(y) =\left\{\begin{array}{ll}\mbox{const } e^{-\frac y2}y^{\frac n2-1}&\text{if }y&gt;0\\ 0&\text{otherwise.}\end{array}\right. $
<p></p>
The transform to go from $Y$  to $Z$  is $z = h(y) = \sqrt{\frac yn}$  with inverse $y = h ^{-1}(z) = n z^2.$ 
<p></p>
Hence, by the Jacobian formula, $Z$  has density
$$f_Z(z) = 2nz f_Y(nz^2) = \left\{\begin{array}{ll}\mbox{const } e^{-nz^2/2} z^{n-2}&\text{if }z&gt;0\\ 0&\text{otherwise.}\end{array}\right.$$
Next we shall employ the quotient formula to find density of $T = \frac XZ$  as 
$$\begin{eqnarray*}
f_T(t) 
&amp; = &amp; \int_0^\infty u f_X(tu) f_Z(u)\, du\\
&amp; = &amp; \mbox{const} \int_0^\infty u e^{-t^2u^2/2} e^{-nu^2/2} u^{n-2}\, du\\
&amp; = &amp; \mbox{const} \int_0^\infty  u^{n-1} e^{-(t^2+n)u^2/2}\, du.
\end{eqnarray*}$$
Substituting $v = \frac{u^2}{2}$  we have 
$$\begin{eqnarray*}
&amp; = &amp; \mbox{const} \int_0^\infty  v^{\frac{n-1}{2}} e^{-(t^2+n)v}\, dv\\
&amp; = &amp; \mbox{const} \int_0^\infty  v^{\frac{n+1}{2}-1} e^{-(t^2+n)v}\, dv\\
&amp; = &amp; \mbox{const } \Gamma\left(\frac{n+1}{2}\right) (t^2+n)^{-\frac{n+1}{2}}
&amp; = &amp; \mbox{const } (t^2+n)^{-\frac{n+1}{2}}.
\end{eqnarray*}$$
If you keep track of the constants, you will find that it is
$$\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}.$$
It should not be difficult to see that $t$-density is symmetric around 0. The densities are much like the $N(0,1)$ 
 density. They lie somewhere in-between the Cauchy density and $N(0,1)$  density. As the degrees of freedom increase
 to $\infty,$  the $t$-distribution approaches $N(0,1).$  For degrees of freedom more than 40, the $t$-density
 is virtually indisguishable from that of $N(0,1)$  density.
<h2><a
name="$F$  distributions">$F$  distributions</a></h2>

<fieldset>
<legend><b>Definition: $F$-distribution</b></legend>
If $X\sim \k m$  and $Y\sim \k n$  are independent random variables, then the distribution of $\frac{X/m}{Y/n}$  is
 called $F$ -distribution with numerator degrees of freedom $m$  and denominator degrees of freedom $n.$
</fieldset>
The density of $X$  is $f_X(x) =\left\{\begin{array}{ll}\mbox{const }x^{\frac m2-1}e^{-\frac x2}&\text{if }x&gt;0\\ 0&\text{otherwise.}\end{array}\right. $
<p></p>
Similarly, 
the density of $Y$  is $f_Y(x) =\left\{\begin{array}{ll}\mbox{const }y^{\frac n2-1}e^{-\frac y2}&\text{if }y&gt;0\\ 0&\text{otherwise.}\end{array}\right. $
<p></p>
Hence density of $Z = \frac XY$  is 
$$\begin{eqnarray*}
f_Z(z)
&amp; = &amp; \int_0^\infty u f_X(zu)f_Y(u)\, du\\
&amp; = &amp; \mbox{const}\int_0^\infty u (zu)^{\frac m2-1}e^{-\frac{zu}{2}} u^{\frac n2-1}e^{-\frac u2}\, du\\
&amp; = &amp; \mbox{const }z^{\frac m2-1}\int_0^\infty  u^{\frac{m+n}{2}-1}e^{-\frac{z+1}{2}u}\, du\\
&amp; = &amp; \mbox{const }z^{\frac m2-1}\Gamma\left(\frac{m+n}{2}\right) \left(\frac{z+1}{2}\right)^{-\frac{m+n}{2}}\\
&amp; = &amp; \mbox{const }z^{\frac m2-1}(z+1)^{-\frac{m+n}{2}}.
\end{eqnarray*}$$
Hence the density of $\frac nmZ$  is 
$$f(x) =\left\{\begin{array}{ll}\mbox{const }x^{\frac m2-1}(mx+n)^{-\frac{m+n}{2}}&\text{if }x&gt;0\\ 0&\text{otherwise.}\end{array}\right. $$
<p></p>

<h2><a
name="Problem set 4">Problem set 4</a></h2>

<h1><a
name="Sampling distributions for normal sample">Sampling distributions for normal sample</a></h1>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X_1,...,X_n$  are IID $N(\mu, \sigma^2),$  then for $a_1,...,a_n\in{\mathbb R}$   we have $\sum a_i X_i\sim N\left(\mu\sum a_i, \sigma^2\sum a_i^2\right).$
</fieldset>

<p>
<b><i>Proof:</i></b>Easy.<b><i>[QED]</i></b>
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X_1,...,X_n$  be IID $N(\mu, \sigma^2).$  Then 
<ol type="">

<li>$\overline X\sim N\left(\mu,\frac{\sigma^2}{n}]\right)$</li>

</ol>

<p></p>

</fieldset>

<h2><a
name="Problem set 5">Problem set 5</a></h2>

<p></p>

<h1><a
name="Miscellaneous problems">Miscellaneous problems</a></h1>

<p>
<b>EXERCISE 5:</b>&nbsp;If $X$  has a density of the form $f(x) \propto \exp(a+b+cx^2),~~x\in{\mathbb R},$  then
 find $E(X)$  and $V(X)$  in terms of $a,b,c.$  Also find median of $X.$</p>

<p>
<b>EXERCISE 6:</b>&nbsp;Construct $(X,Y)$ such that marginally $X$
and $Y$ have $N(0,1)$ distribution, but $(X,Y)$
is not bivariate normal.</p>

<p>
<b>EXERCISE 7:</b>&nbsp;Suppose that you have a software to generate IID replications from $N(0,1).$  Let
$\mu\in{\mathbb R}^n$  and  $\Sigma$  be any $n\times n$  PD matrix. Suggest how you can
 use the software to generate a single observation from $N_n(\mu,\Sigma).$  Assume that the
 software can perform matrix operations.</p>

<p>
<b>EXERCISE 8:</b>&nbsp;If $X,Y$  are IID $N(0,1)$, then what is the chance that the random point
 $(X,Y)$  lies in the annulus shown below?
<center>
<table width="100%">
<tr>
<th><img width="" src="image/annulus.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Express you answer in terms CDF of some standard distribution.
</p>

<p>
<b>EXERCISE 9:</b>&nbsp;Let $X_1,...,X_n$  be a random sample from $N(\mu,\sigma^2)$  for some
 $\mu\in{\mathbb R}$  and $\sigma^2&gt;0.$  Find $a&lt;b$  such that $P\left(a&lt; \frac{\bar
 X-\mu}{S/\sqrt{n}} &lt; b\right) = 0.95$  and $b-a$  is the least possible subject to this.</p>

<p>
<b>EXERCISE 10:</b>&nbsp;<font size="-2">[rossfcpnorm1.png]</font><img width="" src="image/rossfcpnorm1.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 11:</b>&nbsp;<font size="-2">[rossfcpnorm2.png]</font><img width="" src="image/rossfcpnorm2.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 12:</b>&nbsp;<font size="-2">[rossfcpnorm3.png]</font><img width="" src="image/rossfcpnorm3.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 13:</b>&nbsp;<font size="-2">[rossfcpnorm4.png]</font><img width="" src="image/rossfcpnorm4.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 14:</b>&nbsp;<font size="-2">[rossfcpnorm5.png]</font><img width="" src="image/rossfcpnorm5.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 15:</b>&nbsp;<font size="-2">[rossfcpnorm6.png]</font><img width="" src="image/rossfcpnorm6.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 16:</b>&nbsp;<font size="-2">[rossfcpnorm7.png]</font><img width="" src="image/rossfcpnorm7.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 17:</b>&nbsp;<font size="-2">[rossfcpnorm8.png]</font><img width="" src="image/rossfcpnorm8.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 18:</b>&nbsp;<font size="-2">[rossipmnorm1.png]</font><img width="" src="image/rossipmnorm1.png" style="vertical-align:text-top;"></p>

<p>
<b>EXERCISE 19:</b>&nbsp;<font size="-2">[rossipmnorm2.png]</font><img width="" src="image/rossipmnorm2.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 20:</b>&nbsp;<font size="-2">[hpstrans19.png]</font><img width="" src="image/hpstrans19.png" style="vertical-align:text-top;"></p>

<p></p>
<hr/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/></body></html>
