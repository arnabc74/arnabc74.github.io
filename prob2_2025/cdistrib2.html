<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE-BAN-OTF-new.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v19.0" nonce="Q7jTbrCq"></script>

<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else {
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script>
<style>
body,table {
  margin: 0;
  font-size: 40;
  //background: #000;
  //color: #fff;
}

.ans {
  display:none;
  background: #ccffcc;
}

.sticky {
  position: fixed;
  top: 0;
  width: 100%;
  background: #555;
  color: #f1f1f1;
}

.cu {
  background: #ffcccc;
}

.bu {
  background: #ccccff;
}

.scrpt {
  margin:10px;
  border-left: 5px solid black;
}

.box {
  background-color: yellow; 
  //border: 2px solid black;
  display: inline-block;
}

.hl {
  list-style-type: upper-alpha;
}
</style>
<script>
window.onscroll = function() {myFunction()};
window.onload = function() {myInit()};

var header, tphldr;
function myInit() {
  header = document.getElementsByClassName("header");
  tphldr = document.getElementById("topholder");
}

function myFunction() {
  var index = -1
  for(i=0;i<header.length;i++) {
    if (window.pageYOffset > header[i].offsetTop) {
       index = i
    }
    else {
       break
    }
  }

  if(index < 0) 
    tphldr.innerHTML = "";
  else
    tphldr.innerHTML = header[index].innerHTML
}
</script><script type="text/javascript" src="https://arnabc74.github.io/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="file:///home/asu/na/v/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head><body>
<div class="sticky" id="topholder"> </div>
<a href="http://www.isical.ac.in/~arnabc/">[Home]</a>
<h3>Some more standard densities</h3>
<ul>
<li>
<a href="#Beta function">Beta function</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 1">Problem set 1</a>
</li>
<li>
<a href="#Beta distribution">Beta distribution</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 2">Problem set 2</a>
</li>
<li>
<a href="#Bayesian machine learning">Bayesian machine learning</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 3">Problem set 3</a>
</li>
<li>
<a href="#Cauchy distribution">Cauchy distribution</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 4">Problem set 4</a>
</li>
<li>
<a href="#Cauchy baffles the laws of large numbers">Cauchy baffles the laws of large numbers</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 5">Problem set 5</a>
</li>
<li>
<a href="#Normal distribution">Normal distribution</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 6">Problem set 6</a>
</li>
<li>
<a href="#Central Limit Theorem">Central Limit Theorem</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 7">Problem set 7</a>
</li>
<li>
<a href="#Maxwell's derivation of the normal distribution">Maxwell's derivation of the normal distribution</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Problem set 8">Problem set 8</a>
</li>
<li>
<a href="#Problems for practice">Problems for practice</a>
</li>
</ul>
<hr/>
<title>Some more standard densities</title>

<p></p>

<h1><a
name="Beta function">Beta function</a></h1>
<a href="https://youtu.be/0PoToYKWusE">Video for this section</a>
<p></p>
The next distribution that we shall discuss is the Beta distribution. Just as we needed to know about the Gamma function
 in order to talk about the Gamma distribution, we need to know about the Beta function before we can get into Beta distribution.
<p></p>

<fieldset>
<legend><b>Definition: Beta function</b></legend>
The Beta function is the function $B:(0,\infty)\times(0,\infty)\rightarrow{\mathbb R}$  defined as
$$B(a,b) = \int_0^1 x^{a-1}(1-x)^{b-1}\, dx\mbox{ for } a,b&gt;0.$$
</fieldset>
Notice that if $a,b\geq 1,$  then the integrand is continuous, and so integrable over $[0,1].$  If, however, $a$ 
 or $b$  lies in $(0,1)$, then the integral is am improper one, and hence we have to worry about its existence.
 Fortunately, it is easy to establish convergence by comparison with $\int_0^1 x^\alpha\, dx$  for $\alpha&gt;-1.$ 
<p></p>
Most manipulations with the Beta function uses the following theorem to reduce the Beta function to the Gamma function:
<fieldset>
<legend><b><i>Theorem</i></b></legend>
For $a,b&gt;0$  we have 
$$B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}.$$
</fieldset> 

<p>
<b><i>Proof:</i></b>The proof is quite easy using a bivariate change of variable. However, since that result will
 be proved in Analysis III, we shall omit the proof of this theorem here.<b><i>[QED]</i></b>
</p>

<p></p>

<h2><a
name="Problem set 1">Problem set 1</a></h2>

<p>
<b>EXERCISE 1:</b>&nbsp;Find $B(4,5).$  Remember that $\Gamma(n) = (n-1)!$  for $n\in{\mathbb N}.$</p>

<p></p>

<p>
<b>EXERCISE 2:</b>&nbsp;Writing the factorials in terms of the Gamma function, express $\binom{10}{6}$  in terms of the Beta function</p>

<p></p>

<h1><a
name="Beta distribution">Beta distribution</a></h1>
<a href="https://youtu.be/8U3mGT60ZJQ">Video for this section</a>
<p></p>
Now that we see that for $a,b&gt;0$  the function $x^{a-1}(1-x)^{b-1}$  is  nonnegative and integrable 
over  $(0,1)$  with integral $B(a,b),$ 
 we can immediately manufacture a density out of it:
<fieldset>
<legend><b>Definition: Beta distribution</b></legend> 
The distribution with density
$$f(x) = \left\{\begin{array}{ll}\frac{1}{B(a,b)} x^{a-1}(1-x)^{b-1}&\text{if }x\in(0,1)\\ 0&\text{otherwise.}\end{array}\right.$$
for $a,b&gt;0$  is called the <b><font color="red" size="40">Beta distribution with parameters</font></b>  $a,b.$  
</fieldset>

<p></p>
The beta densities show a wide variety of shapes. 
<center>
<table width="100%">
<tr>
<th><img width="" src="image/betadens.png"></th>
</tr>
<tr>
<th>A variety of shapes from the Beta family</th>
</tr>
</table>
</center>


<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X\sim Beta(a,b)$  then $E(X) = \frac{a}{a+b}.$
</fieldset>

<p>
<b><i>Proof:</i></b>
$$E(X) = \frac{1}{B(a,b)}\int_0^1 x\times x^{a-1} (1-x)^{b-1}\, dx = \frac{1}{B(a,b)}\int_0^1  x^{(a+1)-1} (1-x)^{b-1}\, dx = \frac{B(a+1,b)}{B(a,b)}.$$
Now we shall express the Beta functions in terms of the Gamma function to get
$$\frac{B(a+1,b)}{B(a,b)} = \frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)}\times\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}.$$
We know that $\forall \alpha &gt;0~~\Gamma(\alpha+1) = \alpha \Gamma(\alpha).$
Hence $\frac{\Gamma(a+1)}{\Gamma(a)}= a$  and $\frac{\Gamma(a+b)}{\Gamma(a+b+1)}=\frac{1}{a+b}.$
Hence the result.
<b><i>[QED]</i></b>
</p>

<p></p>

<h2><a
name="Problem set 2">Problem set 2</a></h2>

<p>
<b>EXERCISE 3:</b>&nbsp;For particular values of $a,b$  we get the $Unif(0,1)$  distribution. Which values?</p>

<p>
<b>EXERCISE 4:</b>&nbsp;
If $X\sim Beta(a,b)$, then exactly one of the two statements is correct in general. Which one?
<ol class="hl">

<li>$E(X) = \frac{a}{a+b}.$</li>

<li>$E(X) = \frac{b}{a+b}.$</li>

</ol>
Answer by thinking about the density. Now prove it mathematically.
</p>

<p></p>

<p>
<b>EXERCISE 5:</b>&nbsp;If $X\sim Beta(a,b)$, then find $V(X).$</p>

<p></p>

<p>
<b>EXERCISE 6:</b>&nbsp;If $X\sim Beta(a,b)$  then show that $1-X\sim Beta(b,a).$</p>

<h1><a
name="Bayesian machine learning">Bayesian machine learning</a></h1>
<a href="https://youtu.be/u2KcYDA1yFs">Video for this section</a>
<p></p>
The main use of the beta distribution in statistics is as a "distribution of probability". To
 understand this, consider the large collection of coins, both biased and unbiased. If you pick
 one of them at random and let $\Pi$  denote its $P(head),$  then $\Pi$  is a
 random variable, just as the height of a random person is considered a random variable. Now
 $\Pi$, being a probability, must have a distribution supported  on $[0,1].$  Also,
 since $\Pi$  depends on various physical properties of the coin, it is natural to expect
 that it will have a density. The members of the Beta family   are suitable choices. 
This was a toy example. More often we have some event and we ask for expert opinion about its probability. Then $\Pi$ 
 denotes the opinion of a randomly selected expert. If there is a peak in its density, then that's the value most experts
 vote for. In this sense, $Beta(1,1)\equiv Unif(0,1)$  denotes absolute ignorance.
<p></p>
As a warm up towards the use of Beta distribution plays a role in Bayesian machine
 learning, consider the simple set up from last semester. We have a box containing 3 white and 2 black balls. An SRSWOR of
 size 3 is drawn from it, and transferred to a new box. Now an SRSWR of size 5 is drawn from the second box. If all the balls
 in the SRSWR are found to be black, then what is the (conditional) probability that all the balls
 in the first SRSWOR were black?
 I hope you remember the following diagrammatic representation for this:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/bayesdiag.png"></th>
</tr>
<tr>
<th>Diagram for the above problem</th>
</tr>
</table>
</center>
There are four paths. The $i$-th path has probability $p_iq_i$  for $i=0,1,2,3.$  The required probability
 is $\frac{p_3q_3}{\sum_i p_iq_i}.$
<p></p>
You may think of the first stage of this experiment as "constructing a random coin" whose $P(head)$  is $0$  or
 $\frac 13$  or $\frac 23$  or $1.$  These have probabilities $p_0,...,p_3,$  respectively. The second stage
 is tossing the coin 5 times. Given that we get 5 heads, we want to find the probability that our coin has $P(head)=1.$
<p></p>
In this toy example, our "random coin" could have only finitely many possible $P(head)$'s. Now let's make it more realistic.
 We shall start by picking a random coin from th population of all coins. Let $\Pi$  denote its $P(head).$  Just
 as the height of a randomly selected person is a random variable, similarly this $\Pi$  is a random variable. 
It can takes values in $[0,1].$  Let us assume that $\Pi\sim Beta(1,1)\equiv Unif(0,1).$  This means we consider
 any value of $\Pi$  in $[0,1]$  equally likely (this is one way to formalising "absolute ignorance"). 
We shall
now toss this coin 5 times. Given that all the tosses result in heads, what is the conditional distribution
 of $\Pi?$  We had started with a $Beta(1,1)$  distribution. This is often called the <b><font color="red" size="40">prior</font></b>  distribution,
 as it represented the state of our knowledge about $\Pi$  <i>prior</i>  to collection of data. In this sense, the conditional
 distribution we are seeking is the <b><font color="red" size="40">posterior</font></b>  distribution, which represents our state of knowledge after seeing
the data (the outcomes of the coin tosses). We plan to draw a diagram similar to the earlier one for this problem. But here
 the random coin can have any $P(head)$  in $[0,1].$  So we have a contnuum of arrows, which is hard to draw:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/bayesdiag2.png"></th>
</tr>
<tr>
<th>Diagram with continuous sprays of arrows</th>
</tr>
</table>
</center>
The 1 labelling the left hand arrow is the density of the prior $Beta(1,1).$  The $p^5$  labelling the right hand
 arrow is the (binomial) probability of obtaining $5$  heads out of 5 tosses of a coin with $P(head)=p.$  
Following the Bayes theorem idea as before we get the posterior density 
$$\frac{1\times p^5}{\int_0^1 1\times p^5\, dp}=6p^5,$$
which is the $Beta(6,1)$  density. 
<p></p>
It is instructive to draw the two densities side by side to understand how our state of belief has been updated in light
 of the data:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/priopost1.png"></th>
</tr>
<tr>
<th>After seeing the data our belief is concentrated more near 1.</th>
</tr>
</table>
</center>
In this way, whenever the prior is from the Beta family, our data is $X|\Pi=p\sim Binom(n,p)$,  the
 posterior will also be from the
 same family. We express this by saying:
<blockquote>
Beta is the conjugate prior family for Binomial.
</blockquote>
An exercise below
 asks you to prove this. For now let us toss the same coin 5 more times. Suppose now we get 4 tails and 1 head. If we carry
 out the same exercise again (but this time with $Beta(6,1)$  playing the role of the prior, and "1 head, 4 tails"
 as our data), we again get a Beta posterior that we plot below:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/priopost2.png"></th>
</tr>
<tr>
<th>Our belief is now peaked closer to the centre.</th>
</tr>
</table>
</center>
In this way the Beta family can express our ever-changing state of belief as more and more data stream in.
<p></p>

<h2><a
name="Problem set 3">Problem set 3</a></h2>

<p>
<b>EXERCISE 7:</b>&nbsp;Find the posterior for $\Pi$  if the prior is $Beta(6,1)$  and the data consist of
 1 head and 4 tails out of 5
 independent tosses of the coin. The answer should be a Beta distribution with parameters that you are determine.</p> 

<p></p>

<p>
<b>EXERCISE 8:</b>&nbsp;Show that if  $\Pi$  has prior $Beta(a,b)$  and our data consist of exactly
 $X$  heads out of $n$  tosses, then the posterior is again a Beta distribution. What are its parameters?</p>

<p></p>

<p>
<b>EXERCISE 9:</b>&nbsp;Suppose that we have coin with $P(head)$  having prior $Unif(0,1).$  We toss the
 coin $n$  times independently and obtain exactly $X$  heads. Let $f(p)$  be the
 (continuous) density of the posterior. It is natural to estimate $p$  using the value where
 $f$  is the maximum. This is called the (maximum a posteriori) MAP estimator. Derive its
 formula in terms of $n$  and $X.$ Is it the same as the "usual" estimator $\frac Xn?$</p>

<h1><a
name="Cauchy distribution">Cauchy distribution</a></h1>
<a href="https://youtu.be/qZPe1VVsWZ4">Video for this section</a>
<p></p>

<fieldset>
<legend><b>Definition: Cauchy distribution</b></legend>
By <b><font color="red" size="40">Cauchy distribution</font></b>  we mean the distribution with density
$$f(x) = \frac{1}{\pi(1+x^2)},\quad x\in{\mathbb R}.$$
Sometimes we also talk about $Cauchy(\mu,\sigma)$  distribution which is the distribution of $\mu+\sigma X$, where
 $X$  has the above density. In this notation, the density corresponds to the $Cauchy(0,1)$  distribution. 
</fieldset>

<p></p>
The most important reason for including this distribution in our discussion is that it has one notoriously bad property. It
 does not have any finite moment!  In particular, do not think that $Cauchy(\mu,\sigma)$  has
 mean $\mu$  and variance $\sigma^2.$   
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X$  has $Cauchy$  distribution, then $E(X)$  does not exist. As a result $E(X^n)$  does not exist
 for any $n\in{\mathbb N}.$  
</fieldset>

<p>
<b><i>Proof:</i></b>
$\int_0^\infty \frac{x}{1+x^2}dx\sim \int_0^\infty \frac 1xdx = \infty.$
<b><i>[QED]</i></b>
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
The characteristic function of the Cauchy distribution is $e^{-|t|}.$
</fieldset>

<p>
<b><i>Proof:</i></b>Needs techniques (complex contour integration/differentiation under intergal) beyond the present level.<b><i>[QED]</i></b>
</p>

<p></p>

<h2><a
name="Problem set 4">Problem set 4</a></h2>

<p>
<b>EXERCISE 10:</b>&nbsp;How can you generate a Cauchy random variable from a $Unif(0,1)$  random variable?</p>

<p></p>

<p>
<b>EXERCISE 11:</b>&nbsp;Consider the unit semicircle shown below.
<center>
<table width="100%">
<tr>
<th><img width="" src="image/caupt.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
 We pick a point at random on it, and extend the ray
  through it from the origin until it hits the $x=1$  line at some $(1,Y).$  Find the distribution of $Y.$</p>

<p></p>

<p>
<b>EXERCISE 12:</b>&nbsp;If $X$  is a Cauchy random variable, then show that $\frac 1X$  is also a Cauchy random variable.</p>

<p></p>

<h1><a
name="Cauchy baffles the laws of large numbers">Cauchy baffles the laws of large numbers</a></h1>
<a href="https://youtu.be/PpZTTaKJbbg">Video for this section</a>
<p></p>
The following R snippet will plot the running means of 10000 IID Cauchy random variables. The plot does not converge.
<font color="red">
<pre>
n = 10000
x= rcauchy(n)
y=cumsum(x)/(1:n)
plot(y,ty='l')
</pre>
</font>
This demonstration is theoretically justified using the following theorem.
<fieldset>
<legend><b><i>Theorem</i></b></legend>If $X$  and $Y$  are independent Cauchy random variables, then for any $a\in[0,1]$  
 $aX+(1-a)Y$  is also a Cauchy variate.
</fieldset>

<p>
<b><i>Proof:</i></b>
This may be proved using Jacobians, or more directly using characteristic function. The characteristic function of $aX+(1-a)Y$ 
 is 
$$E(e^{it(aX+(1-a)Y)}) = E(e^{itaX+it(1-a)Y}) = E(e^{itaX}\cdot e^{it(1-a)Y}) = E(e^{itaX})E(e^{it(1-a)Y}),$$
since $X,Y$  are independent. Now, we know that $E(e^{itX}) = E(e^{itY}) = e^{-|t|}.$  Hence
$$ E(e^{itaX})E(e^{it(1-a)Y}) =  e^{-|ta|})\times e^{-|t(1-a)|}) = e^{-|ta|-|t(1-a)|}  = e^{-|t|},$$
since $a\in[0,1].$
<p></p>
This completes the proof.
<b><i>[QED]</i></b>
</p>
The next theorem, which is a simple corollary to this theorem, shows why $\bar X_n$  failed to
 converge to a number in
 our simulation of the law
 of large numbers. 
<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X_1,...,X_n$  are IID
 Cauchy, then $\bar X_n= \frac 1n\sum_1^n X_i$  is also Cauchy.
</fieldset>

<p>
<b><i>Proof:</i></b>
See the exercise below.
<b><i>[QED]</i></b>
</p>

<h2><a
name="Problem set 5">Problem set 5</a></h2>

<p>
<b>EXERCISE 13:</b>&nbsp;Prove the above theorem using induction on $n.$  You may like to use the identity
$$\bar X_n = \frac{(n-1)\bar X_{n-1} + X_n}{n}.$$
</p>

<p></p>

<p>
<b>EXERCISE 14:</b>&nbsp;If $X,Y$  are independent Cauchy random variables, and we take $a\not\in[0,1],$ 
 then is it possible for $aX+(1-a)Y$  to have Cauchy distribution? </p>

<p></p>

<h1><a
name="Normal distribution">Normal distribution</a></h1>
<a href="https://youtu.be/YWHxhk3vok4">Video for this section</a>
<p></p>
This is the most commonly used distribution in statistics.
<p></p>

<fieldset>
<legend><b>Definition: Normal distribution</b></legend>
We say that $X\sim N(\mu,\sigma^2)$  to mean $X$  has density 
 $$\phi(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2} \right), \quad x\in{\mathbb R}.$$
</fieldset>
The density looks like the following: 
<center>
<table width="100%">
<tr>
<th><img width="" src="image/normdens.png"></th>
</tr>
<tr>
<th>$\mu$  controls centre, $\sigma$  controls spread</th>
</tr>
</table>
</center>

Proving that this is indeed a density is not entirely straightforward. Howeer, we may reduce
 the problem to a Gamma integral and use the following theorem.
<fieldset>
<legend><b><i>Theorem</i></b></legend>
$\Gamma\left(\frac 12\right)= \sqrt\pi.$
</fieldset>

<p>
<b><i>Proof:</i></b>Omitted.<b><i>[QED]</i></b>
</p>
To use this result in order to show that the total integral of the $N(\mu,\sigma^2)$  density is indeed 1, we proceed
 as follows.
$$\int_0^\infty e^{-x^2/2}\, dx= \frac{1}{\sqrt2}\int_0^\infty t^{-1/2} e^{-t} \, dt$$
by using $t = x^2/2.$  This new integral is just 
$$\int_0^\infty t^{\frac 12-1} e^{-t} \, dt = \Gamma\left(\frac 12\right) = \sqrt\pi.$$
So
$$\int_{-\infty}^\infty e^{-x^2/2}\, dx=2\int_0^\infty e^{-x^2/2}\, dx= \sqrt{2\pi}.$$
Hence we have shown that the $N(0,1)$  density integrates up to 1. To prove for any general $N(\mu,\sigma^2)$ 
 we simply use the substitution $y = \frac{x-\mu}{\sigma}$  to reduce it to the $N(0,1)$  case.
<p></p>
The letter lower case phi, $\phi$, is generally used for the $N(0,1)$  density, while its capital version $\Phi$ 
 is reserved for the CDF.
$$\Phi(x) = \int_{-\infty}^x \phi(t)\, dt$$
Louiville showed that $\Phi(x)$  cannot be expressed in terms of elementary functions (trigonometric, exponential, logarithmic,
 square root, cube root etc).  However, its value may be computed numerically for any given $x.$  
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X\sim N(\mu,\sigma^2),$  then for any $a\in{\mathbb R}$  and $b\neq 0$  we have
 $a+bX\sim N(a+b\mu,b^2 \sigma^2).$
</fieldset>

<p>
<b><i>Proof:</i></b>
Directly from Jacobian formula.
<b><i>[QED]</i></b>
</p>

<p></p>
A corollary is the following theorem.
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X\sim N(\mu,\sigma^2),$  then $\frac{X-\mu}{\sigma}\sim N(0,1).$
</fieldset>
The transformation from $X$  to $\frac{X-\mu}{\sigma}$  is called <b><font color="red" size="40">standardisation</font></b>.
<fieldset>
<legend><b><i>Theorem</i></b></legend>If $X\sim N(\mu,\sigma^2),$  then $E(X)=\mu$  and $V(X)=\sigma^2.$</fieldset>

<p>
<b><i>Proof:</i></b>Easy, and left as an exercise. Just one reminder. As a first step you should substitute
 $y=\frac{x-\mu}{\sigma}$  to arrive at $N(0,1).$  Now the expectation is given by the integral 
$$\frac{1}{2\sqrt\pi}\int_{-\infty}^\infty x e^{-x^2/2}\, dx.$$
Don't rush to the conclusion that this must be zero, because the integrand is an odd function. Here you are working with
 an improper integral. So you need to make sure that $\int_0^\infty x e^{-x^2/2}\, dx$  is finite before you can use
 the odd function argument.
<b><i>[QED]</i></b>
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
The characteristic function of $N(0,1)$  is $e^{-t^2/2}$  for $t\in{\mathbb R}.$
</fieldset>

<p>
<b><i>Proof:</i></b>
As you have not formally done complex integration yet, all our characteristic function derivations are heuristic. 
<p></p>
Here we can show directly that for any  $s\in{\mathbb R}$  we have $E(e^{sX}) = e^{s^2/2},$
 where $X\sim N(0,1).$   This is easily shown using a simple  substitution.  
<p></p>
Now, if you replace $s$  with $it,$  you get the result. This replacement is justified
 using arguments from complex analysis beyond the present scope. 
<b><i>[QED]</i></b>
</p>

<h2><a
name="Problem set 6">Problem set 6</a></h2>

<p>
<b>EXERCISE 15:</b>&nbsp;If $X\sim N(0,1),$  then express the following probabilities in terms of $\Phi(\cdot).$  
<ol type="">
<li>$P(X&lt;1)$</li>

<li>$P(|X|&lt;1)$</li>

<li>$P(|X|&gt;2)$</li>

</ol>

</p>

<p>
<b>EXERCISE 16:</b>&nbsp;If $X\sim N(2,3^2),$  then express the following probabilities in terms of $\Phi(\cdot).$  
<ol>
<li>$P(X&lt;1)$</li>

<li>$P(|X|&lt;1)$</li>

<li>$P(|X|&gt;2)$</li>

</ol>

</p>

<p></p>

<p>
<b>EXERCISE 17:</b>&nbsp;If $\Phi ^{-1}(0.95)=1.64$, then find $c\in{\mathbb R}$  such that $P(|X-1|&gt;c) = 0.1 $  where $X\sim N(1,1^2).$</p>

<p></p>

<h1><a
name="Central Limit Theorem">Central Limit Theorem</a></h1>
The Central imit Theorem (CLT) is possibly the most famous theorem in probability theory and statistics. Originally stated
 and proved by Gauss, the theorem has many variants due to other mathematicians. Here we shall state the simplest version.
<p></p>

<fieldset>
<legend><b><i>Central imit Theorem (CLT)</i></b></legend>
Let $X_1,X_2,...$  be IID with $E(X_i) = \mu$  and $V(X_i) = \sigma^2 &lt; \infty.$  Let 
$$\bar X_n = \frac{X_1+\cdots+X_n}{n}\mbox{ for } n\in{\mathbb N}.$$
Then the distribution of $\frac{\sqrt n(\bar X_n-\mu)}{\sigma}$  tends to $N(0,1)$  as $n\rightarrow \infty.$
More precisely, if $F_n(\cdot)$  denotes the CDF of $\frac{\sqrt n(\bar X_n-\mu)}{\sigma},$  and $\Phi(\cdot)$ 
 denotes the $N(0,1)$  CDF, then
$$\forall t\in{\mathbb R}~~F_n(t)\rightarrow \Phi(t) \mbox{ as } n\rightarrow \infty.$$
</fieldset>

<p>
<b><i>Proof:</i></b>Next semester.<b><i>[QED]</i></b>
</p>
 This theorem is a manifestation of statistical regularity. Whatever may the true
 distribution of the $X_i$'s be, if you average a large number of $X_i$'s you get close
 approximation to the normal distribution. This allows
statistician to deal with averages of a large number of IID observations without knowing the true
 underlying distribution. 
<p></p>
Let's look at a typical example. 
<p></p>

<p>
<b>EXAMPLE 1:</b>&nbsp;If 40% of the population of a city supports a poll candidate, then what is the approximate
 probability that a random sample of 500 persons from the city will have at least 250 supporters?
<p></p>
<b>SOLUTION:</b>
Here we think of the sampling procedure as 500 trials of the same  random experiment: Pick a person at random
 from the population of the city. 
<p></p>
We shall assume that the trials are IID. Now here we are introducing an approximation: the first
 membr of the sample was drawn from the entire population, but since we generally sample
 <i>without replacement</i>  in such a scenario, the second member of the sample was drawn from a
 population of size one less than in the case of the first member. So the radom experiment has
 actually changed, and they are not independent also. But since the population is large (much
 larger than 500), so we are ignoring both the non-identical and dependent nature and assuming IID. 
<p></p>
We also have a random variable: 
$$X(\omega) = \left\{\begin{array}{ll}1 &\text{if }\omega\mbox{ supports the candiate}\\ 0&\text{otherwise.}\end{array}\right.$$ 
Here$\omega$  is the person sampled. Each trial gives rise to one copy of this random variable, so we have 
$X_1,...,X_{500}$  IID $Bernoulli(0.4).$  This $0.4$  came from the 40% given in the problem. 
<p></p>
By CLT we have 
$$\frac{\sqrt n (\bar X_n-\mu)}{\sigma}\rightarrow N(0,1)$$
as $n\rightarrow \infty,$  where $\mu = E(X_i)$  and $\sigma^2 = V(X_i)&lt; \infty.$  We shall write this as 
$$\bar X_n \stackrel{\bullet}{\sim} N\left(\mu,\frac{\sigma^2}{n}\right)$$
for large $n.$  Here $\stackrel\bullet\sim$  means "approximately distributed as".
<p></p>
In our case, $\mu = 0.40$, $\sigma^2 = 0.4(1-0.4) = 0.24$  and $n=500.$  So 
$$\bar X_{500} \stackrel{\bullet}{\sim} N\left(0.40,\frac{0.24}{500}\right),$$
or 
$$\sum_1^n X_i \stackrel{\bullet}{\sim} N(0.40\times 500,0.24\times 500)\equiv N(200, 120).$$
Nowe we can find the required probability as
$$P(\sum_1^{500} X_i \geq 200) \approx 1-\Phi\left(\frac{250-200}{\sqrt{500}}\right).$$
This probability may be obtained by looking up standard $N(0,1)$  tables or using R as
<font color="red">
<pre>
1-pnorm((250-200)/sqrt(500))
</pre>
</font>
  ■
</p>
In this problem we knew the distribution of the $X_i$'s, but we never really made any use of it, except to compute $E(X_i)$ 
 and $V(X_i).$  
<h2><a
name="Problem set 7">Problem set 7</a></h2>
::<p>
<b>EXERCISE 18:</b>&nbsp;<font size="-2">[rossdistrib10.png]</font><img width="" src="image/rossdistrib10.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 19:</b>&nbsp;<font size="-2">[rossdistrib8.png]</font><img width="" src="image/rossdistrib8.png" style="vertical-align:text-top;">
<p><a
href="javascript:hideShow('lab1')"><b>[Hint]</b></a><div
class="ans" id="lab1">For the second part, just drop the 6's. This means you are rolling a 5-faced die 800 times.</div></p>

</p>
::<p>
<b>EXERCISE 20:</b>&nbsp;<font size="-2">[rossdistrib5.png]</font><img width="" src="image/rossdistrib5.png" style="vertical-align:text-top;"></p>

<p></p>

<h1><a
name="Maxwell's derivation of the normal distribution">Maxwell's derivation of the normal distribution</a></h1>
<a href="https://youtu.be/NYFoXVF7xkE">Video for this section</a>
<p></p>
Generally, Gauss is credited with the "discovery" of the normal distribution, which he derived via his famous central limit
 theorem. However, a lesser known derivation is due to James Clerk Maxwell, which we shall discuss now. 
<p></p>
Imagine a gas without any overall flow. We know that the molecules are moving tis way and that
 randomly. Maxwell was interested in
 finding the distribution of velocities. Of course, he had no way of grabbing a molecule and measuring its velocity. So he
 did a brilliant logical argument starting with little more than common sense notions available to anybody. 
<p></p>
His first step was to set up three axes $x$ , $y$  and $z.$  Then the velocity could be expressed as $(V_1,V_2,V_3).$ 
Since there is no overall flow and we have no reason to favour any one direction over the other hence $V_1,V_2,V_3$  must
 be IID. 
<p></p>
Now we make our first not-so-common-sense assumption: each $V_i$   has a density, say $f.$ 
Then the joint density of $(V_1,V_2,V_3)$  is  $f(v_1)f(v_2)f(v_3).$  Surely this cannot
 depend on the direction of $\vec v = (v_1,v_2,v_3).$ 
 So we must have 
$$f(v_1)f(v_2)f(v_3) = g(v_1^2+v_2^2+v_3^2)$$
for some function $g(\cdot).$
<p></p>
Now we make our second technical assumption: $f$  is differentiable. Then differentiating both sides of the above equality
partially           wrt $v_i$  we get
$$\begin{eqnarray*}
f'(v_1)f(v_2)f(v_3) &amp; = &amp; 2v_1g'(v_1^2+v_2^2+v_3^2),\\
f(v_1)f'(v_2)f(v_3) &amp; = &amp; 2v_2g'(v_1^2+v_2^2+v_3^2),\\
f(v_1)f(v_2)f'(v_3) &amp; = &amp; 2v_3g'(v_1^2+v_2^2+v_3^2).
 \end{eqnarray*}$$
So 
$$\frac{f'(v_1)f(v_2)f(v_3)}{v_1}=\frac{f(v_1)f'(v_2)f(v_3)}{v_2},$$
or
$$\frac{f'(v_1)}{v_1f(v_1)}=\frac{f'(v_2)}{v_2f(v_2)}.$$
Since $v_1,v_2$  are arbitrary, hence this means 
$$\frac{f'(x)}{xf(x)}=k$$
for some constant $k.$   Solving the differerntial equation we get 
$\log f(x) = \frac{k x^2}{2}+$ constant,   or 
$$f(x) \propto e^{k x^2/2}.$$
Since $f$  is a density, hence its total intergral must be 1. Hence $k &lt; 0$  and we get
 the density of $N\left(0,-\frac{1}{2k}\right).$
<p></p>

<h2><a
name="Problem set 8">Problem set 8</a></h2>

<p>
<b>EXERCISE 21:</b>&nbsp;If $X$  is a random variable with density proportional to $\exp((1-x)(3+4x))$  for
 $x\in{\mathbb R}$,  then find the distribution of $X.$ </p>

<p></p>

<p>
<b>EXERCISE 22:</b>&nbsp;Let $\vec V = (V_1,V_2,V_3)$  have the joint distribution as in Maxwell's derivation.
 Consider $\vec U = \frac{\vec V}{\|\vec V\|}$, the unit vector along $\vec V.$  Describe
 the distribution of $\vec U.$</p>

<p></p>

<p>
<b>EXERCISE 23:</b>&nbsp;If $X$  has density proportional to $e^{ax^2+bx+c}$  for $x\in{\mathbb R}$, for some
 constants $a,b,c$, then find $E(X)$  and $V(X).$</p>

<p></p>

<h1><a
name="Problems for practice">Problems for practice</a></h1>
You may leave the answers in terms the $N(0,1)$  CDF $\Phi(x)$  or its inverse $\Phi ^{-1}(x).$
<p></p>
::<p>
<b>EXERCISE 24:</b>&nbsp;<font size="-2">[rossdistrib1.png]</font><img width="" src="image/rossdistrib1.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 25:</b>&nbsp;<font size="-2">[rossdistrib2.png]</font><img width="" src="image/rossdistrib2.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 26:</b>&nbsp;<font size="-2">[rossdistrib3.png]</font><img width="" src="image/rossdistrib3.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 27:</b>&nbsp;<font size="-2">[rossdistrib4.png]</font><img width="" src="image/rossdistrib4.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 28:</b>&nbsp;<font size="-2">[rossdistrib6.png]</font><img width="" src="image/rossdistrib6.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 29:</b>&nbsp;<font size="-2">[rossdistrib7.png]</font><img width="" src="image/rossdistrib7.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 30:</b>&nbsp;<font size="-2">[rossdistrib9.png]</font><img width="" src="image/rossdistrib9.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 31:</b>&nbsp;<font size="-2">[rossdistrib11.png]</font><img width="" src="image/rossdistrib11.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 32:</b>&nbsp;<font size="-2">[rossdistrib12.png]</font><img width="" src="image/rossdistrib12.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 33:</b>&nbsp;<font size="-2">[rossdistrib18.png]</font><img width="" src="image/rossdistrib18.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 34:</b>&nbsp;<font size="-2">[rossdistrib28.png]</font><img width="" src="image/rossdistrib28.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 35:</b>&nbsp;<font size="-2">[rossdistrib29.png]</font><img width="" src="image/rossdistrib29.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 36:</b>&nbsp;<font size="-2">[rossdistrib30.png]</font><img width="" src="image/rossdistrib30.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 37:</b>&nbsp;<font size="-2">[rossdistrib31.png]</font><img width="" src="image/rossdistrib31.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 38:</b>&nbsp;<font size="-2">[rossdistrib32.png]</font><img width="" src="image/rossdistrib32.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 39:</b>&nbsp;<font size="-2">[rossdistrib33.png]</font><img width="" src="image/rossdistrib33.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 40:</b>&nbsp;<font size="-2">[rossdistrib34.png]</font><img width="" src="image/rossdistrib34.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 41:</b>&nbsp;<font size="-2">[rossdistrib35.png]</font><img width="" src="image/rossdistrib35.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 42:</b>&nbsp;<font size="-2">[rossdistrib37.png]</font><img width="" src="image/rossdistrib37.png" style="vertical-align:text-top;"></p>

<p></p>
::<p>
<b>EXERCISE 43:</b>&nbsp;<font size="-2">[rosspdf15.png]</font><img width="" src="image/rosspdf15.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 44:</b>&nbsp;<font size="-2">[hpspdf23.png]</font><img width="" src="image/hpspdf23.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 45:</b>&nbsp;<font size="-2">[hpspdf27.png]</font><img width="" src="image/hpspdf27.png" style="vertical-align:text-top;"></p>
---
::<p>
<b>EXERCISE 46:</b>&nbsp;<font size="-2">[hpspdf28.png]</font><img width="" src="image/hpspdf28.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 47:</b>&nbsp;<font size="-2">[hpspdf29.png]</font><img width="" src="image/hpspdf29.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 48:</b>&nbsp;<font size="-2">[hpspdf30.png]</font><img width="" src="image/hpspdf30.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 49:</b>&nbsp;<font size="-2">[hpspdf31.png]</font><img width="" src="image/hpspdf31.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 50:</b>&nbsp;<font size="-2">[hpspdf32.png]</font><img width="" src="image/hpspdf32.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 51:</b>&nbsp;<font size="-2">[hpspdf33.png]</font><img width="" src="image/hpspdf33.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 52:</b>&nbsp;<font size="-2">[hpspdf34.png]</font><img width="" src="image/hpspdf34.png" style="vertical-align:text-top;"></p>

<p></p>
::<p>
<b>EXERCISE 53:</b>&nbsp;<font size="-2">[hpspdf42.png]</font><img width="" src="image/hpspdf42.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 54:</b>&nbsp;<font size="-2">[hpspdf43.png]</font><img width="" src="image/hpspdf43.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 55:</b>&nbsp;<font size="-2">[hpspdf44.png]</font><img width="" src="image/hpspdf44.png" style="vertical-align:text-top;"></p>
----
::<p>
<b>EXERCISE 56:</b>&nbsp;<font size="-2">[hpstrans3.png]</font><img width="" src="image/hpstrans3.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 57:</b>&nbsp;<font size="-2">[hpstrans10.png]</font><img width="" src="image/hpstrans10.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 58:</b>&nbsp;<font size="-2">[hpstrans11.png]</font><img width="" src="image/hpstrans11.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 59:</b>&nbsp;<font size="-2">[hpstrans13.png]</font><img width="" src="image/hpstrans13.png" style="vertical-align:text-top;"></p>

<p></p>
::<p>
<b>EXERCISE 60:</b>&nbsp;<font size="-2">[hpstrans17.png]</font><img width="" src="image/hpstrans17.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 61:</b>&nbsp;<font size="-2">[hpstrans20.png]</font><img width="" src="image/hpstrans20.png" style="vertical-align:text-top;"></p>

<p></p>
::<p>
<b>EXERCISE 62:</b>&nbsp;<font size="-2">[hpstrans26.png]</font><img width="" src="image/hpstrans26.png" style="vertical-align:text-top;"></p>

<p></p>
::<p>
<b>EXERCISE 63:</b>&nbsp;<font size="-2">[wilks7.png]</font><img width="" src="image/wilks7.png" style="vertical-align:text-top;"></p>
::<p>
<b>EXERCISE 64:</b>&nbsp;<font size="-2">[wilks10.png]</font><img width="" src="image/wilks10.png" style="vertical-align:text-top;"></p>

<p></p>
<hr/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/></body></html>
