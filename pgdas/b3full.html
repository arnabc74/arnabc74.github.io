<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE0592.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script>
<style>
body {
  margin: 0;
}


.sticky {
  position: fixed;
  top: 0;
  width: 100%;
  background: #555;
  color: #f1f1f1;
}

.cu {
  background: #ffcccc;
}

.bu {
  background: #ccccff;
}

.scrpt {
  border-left: 5px solid black;
}
</style>
<script>
window.onscroll = function() {myFunction()};
window.onload = function() {myInit()};

var header, tphldr;
function myInit() {
  header = document.getElementsByClassName("header");
  tphldr = document.getElementById("topholder");
}

function myFunction() {
  var index = -1
  for(i=0;i<header.length;i++) {
    if (window.pageYOffset > header[i].offsetTop) {
       index = i
    }
    else {
       break
    }
  }

  if(index < 0) 
    tphldr.innerHTML = "";
  else
    tphldr.innerHTML = header[index].innerHTML
}
</script><script type="text/javascript" src="https://www.isical.ac.in/~arnabc/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">

The world around us is full of unpredictable variations. Unpredictable, yet
the unpredictability has a pattern in it. Man has been studying this
pattern ever since his earliest existence.
---
 There are different ways to deal
with such patterned unpredictability, and statistics is one of them. In
this lesson we shall learn what sets statistics apart from the other
approaches.
---
 This difference is the key to appreciating statistical methods,
their strong points as well as their weak points. 
---
This key concept in a nutshell is this: Whatever data we collect is like a
cup of water from a vast ocean. The cup of water is all that we have to
base our inference on, but it is not the water in the cup that we want to
draw inference about.
---
 The  target of our inference is the entire ocean. The
statistical term for the cup of water is a sample, the ocean being
called the population. 
---
Any serious statistical exercise starts with a precise and clear picture of
the population and its relation with the sample.

Population and sample

We shall start with a simple example. The very term population
conjures up the vision of the totality of all the people living in a
country. While statistics uses the term in a broader sense, this
nevertheless remains a good picture to keep in mind. 
---
Here is our toy example. Imagine a population consisting of all the people living in a
large country. [men shown] We want to know the height of the
tallest person in our population.[line shown]  
---
Of course, if we had a list of the heights of all the
members of the entire population, then it would have been just a matter of
looking up the maximum, something that a computer could have done easily
for us.
---
 But the point is that we do not have such a list to start with.
 All that we can do  is to draw a random
sample of individuals and measure their heights
only.[highlighted]
---
 There is no guarantee
of course that this sample will contain the tallest person in the
population,...
---
 and so there cannot exist any sure  way to find the maximum
height in the population by looking at just the heights in the sample. The
best that we can hope for is only a reasonable guess.
---
 The precise statistical
term for obtaining  such a reasonable guess
is estimation. Let's try to come up with a good estimation procedure.

Estimation

 Suppose the heights
of the people in our sample are 5'2'', 5'5'', 5'0'', 5'9'' and
6'3''.[list shown] 
----
Based on this somone estimates the maximum population to be 6'.[6'?] Is this a
good estimate? No! Because even in our sample we have a person taller than
this![finger]
---
 OK, let's revise our estimate to 6'3'' then, the maximum in our
sample. That is not as absurd as the last estimate, but still this assumes
that just by randomly selecting 5 persons out of an enormous population we
have managed to get one of the tallest guys in the entire
country!
---
 Not impossible, but not very probable either! So we should better allow some
margin above the sample maximum. How to choose the margin? Here are two ad
hoc suggestions: 
---
* One method could be to use the gap between the top two tallest persons in
the sample as the margin. So we shall compute maximum+gap between
the tallest two.[shown], we have called this gap the top gap.
 In our case it is
[finger] 6'3''-5'9''=6''.
---
 So we estimate the population maximum to be 6'3''+6'' = 6'9''.[shown]

* Another method could be to scale up the sample maximum by a factor 5/4,
which is sample size/(sample size-1),[shown].
---
These are, as I said, ad hoc suggesions, and one can come up with many such
suggestions. This scenario is indeed common to all
statistical methods. We can think of many ad hoc procedures, and
need to choose the best, or at least a good one among them.
---
 Instead of just blurting out a single number as our
estimate, we are instead trying to come up with a  rule or formula to
produce the estimate from the sample. Like[finger] "sample max +
top gap" or "size/(size-1) * max".
---
Such formulae are called estimators, as opposed to
an estimate which refers to the numerical value that the formula takes for a
given sample. The target quantity that we are trying to estimate
is called a parameter.
---
 Indeed, any unknown quantity regarding the
underlying population is called a parameter, whether or not we
are trying to estimate it. 
---
Our parameter of interest was the population maximum. Had we possesed a complete list
of all the heights in the population, then this would have been a dumb
clerical exercise. No scope of creativty there.
---
 But since we do not have
such a master list available, we can propose and compare between different
estimators. Lots of scope for creativity here! 
---
So here is the take away message that you should never forget. The ultimate
aim of statistics is not to look at data, but through data at
the underlying reality. Sample corresponds to the data, population to
the underlying reality.

</script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head>
<body>
<div class="sticky" id="topholder"> </div>
<a href="http://www.isical.ac.in/~arnabc/">[Home]</a>
<h3>Table of contents</h3>
<ul>
<li>
<a href="#Module 1: Estimation">Module 1: Estimation</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 1: The concept">Module 1: Lesson 1: The concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 1: The concept (population,
sample)">Module 1, Lesson 1, Video 1: The concept (population,
sample)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 2: The concept (sampling distribution)">Module 1, Lesson 1, Video 2: The concept (sampling distribution)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 3: The concept (sampling distribution)">Module 1, Lesson 1, Video 3: The concept (sampling distribution)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 4: The concept (sampling distribution)">Module 1, Lesson 1, Video 4: The concept (sampling distribution)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 5: The concept (sampling distribution)">Module 1, Lesson 1, Video 5: The concept (sampling distribution)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 2: Estimating mean">Module 1: Lesson 2: Estimating mean</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 2, Video 1: Estimating mean">Module 1, Lesson 2, Video 1: Estimating mean</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 2, Video 2: Lab">Module 1, Lesson 2, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 2, Video 3: Sampling distributions of mean">Module 1, Lesson 2, Video 3: Sampling distributions of mean</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 3: Estimating proportions">Module 1: Lesson 3: Estimating proportions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 3, Video 1: Estimating proportions">Module 1, Lesson 3, Video 1: Estimating proportions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 4: Estimating dispersion">Module 1: Lesson 4: Estimating dispersion</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 4, Video 1: Estimating dispersion">Module 1, Lesson 4, Video 1: Estimating dispersion</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 4, Video 2: Lab">Module 1, Lesson 4, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 4, Video 3: Sampling distributions">Module 1, Lesson 4, Video 3: Sampling distributions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 4, Video 4: Alternatives">Module 1, Lesson 4, Video 4: Alternatives</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 5: Review">Module 1: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 5 , Video 1: Review">Module 1, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 5 , Video 2: Review">Module 1, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 2: Test of hypotheses">Module 2: Test of hypotheses</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 1: Concept of test">Module 2: Lesson 1: Concept of test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 1, Video 1: The concept of test (sound of water)">Module 2, Lesson 1, Video 1: The concept of test (sound of water)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 1, Video 2: The concept of test (sound of water)">Module 2, Lesson 1, Video 2: The concept of test (sound of water)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 1, Video 3: Lab">Module 2, Lesson 1, Video 3: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 1, Video 4: Critical value method">Module 2, Lesson 1, Video 4: Critical value method</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 2: One sample t-test">Module 2: Lesson 2: One sample t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 2, Video 1: one-sample t-test">Module 2, Lesson 2, Video 1: one-sample t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 2, Video 2: one-sample t-test">Module 2, Lesson 2, Video 2: one-sample t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 2, Video 2: Lab">Module 2, Lesson 2, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 3: Paired t-test">Module 2: Lesson 3: Paired t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 4: Two-sample t-test">Module 2: Lesson 4: Two-sample t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 5: Review">Module 2: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 5 , Video 1: Review">Module 2, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 5 , Video 2: Review">Module 2, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 3: Goodness of fit and independence">Module 3: Goodness of fit and independence</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 1: Contingency tables">Module 3: Lesson 1: Contingency tables</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1, Video 1: Contingency tables">Module 3, Lesson 1, Video 1: Contingency tables</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1,  Video 2: Contingency table">Module 3, Lesson 1,  Video 2: Contingency table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1,  Video 4: Contingency table">Module 3, Lesson 1,  Video 4: Contingency table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1,  Video 5: Contingency table">Module 3, Lesson 1,  Video 5: Contingency table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1,  Video 6: Contingency table">Module 3, Lesson 1,  Video 6: Contingency table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 2: Independence test">Module 3: Lesson 2: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 3: Permutation test">Module 3: Lesson 3: Permutation test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 3, Video 1: Permutation test">Module 3, Lesson 3, Video 1: Permutation test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 3, Video 2: Permutation test lab">Module 3, Lesson 3, Video 2: Permutation test lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 4: Goodness of fit test">Module 3: Lesson 4: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 5: Review">Module 3: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 5 , Video 1: Review">Module 3, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 5 , Video 2: Review">Module 3, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 4: ANOVA">Module 4: ANOVA</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 1: ANOVA concept">Module 4: Lesson 1: ANOVA concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 2: ANOVA terms and data layout">Module 4: Lesson 2: ANOVA terms and data layout</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 2, Video 1: ANOVA terms">Module 4, Lesson 2, Video 1: ANOVA terms</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 2, Video 1: ANOVA terms">Module 4, Lesson 2, Video 1: ANOVA terms</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 3: ANOVA table">Module 4: Lesson 3: ANOVA table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 4: Interaction">Module 4: Lesson 4: Interaction</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 5: Review">Module 4: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 5 , Video 1: Review">Module 4, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 5 , Video 2: Review">Module 4, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 5: Regression">Module 5: Regression</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 1: Regression concept">Module 5: Lesson 1: Regression concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 1, Video 1: The concept">Module 5, Lesson 1, Video 1: The concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 1, Video 2: Mathematical formulation">Module 5, Lesson 1, Video 2: Mathematical formulation</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 2: Least squares">Module 5: Lesson 2: Least squares</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 2, Video 1: Least squares">Module 5, Lesson 2, Video 1: Least squares</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 2, Video 1: Least squares">Module 5, Lesson 2, Video 1: Least squares</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 3: Real life data">Module 5: Lesson 3: Real life data</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 3, Video 1: Real life data">Module 5, Lesson 3, Video 1: Real life data</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 4: Residuals, outliers, leverage">Module 5: Lesson 4: Residuals, outliers, leverage</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 1: Residuals (theory) ">Module 5, Lesson 4, Video 1: Residuals (theory) </a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 2: Regression diagonsotics">Module 5, Lesson 4, Video 2: Regression diagonsotics</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 3: Residuals (lab) ">Module 5, Lesson 4, Video 3: Residuals (lab) </a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 4: Outiers, leverage (theory) ">Module 5, Lesson 4, Video 4: Outiers, leverage (theory) </a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 5: Outiers, leverage (lab) ">Module 5, Lesson 4, Video 5: Outiers, leverage (lab) </a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 5: Review">Module 5: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 5 , Video 1: Review">Module 5, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 5 , Video 2: Review">Module 5, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 6: Time series analysis">Module 6: Time series analysis</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 1: Time series concept">Module 6: Lesson 1: Time series concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 1, Video 1: Concept: What it is">Module 6, Lesson 1, Video 1: Concept: What it is</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 1, Video 2: Concept: Estimation, prediction, smoothing">Module 6, Lesson 1, Video 2: Concept: Estimation, prediction, smoothing</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 1, Video 2: Concept: Estimation, prediction, smoothing">Module 6, Lesson 1, Video 2: Concept: Estimation, prediction, smoothing</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 2: Plotting">Module 6: Lesson 2: Plotting</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 2, Video 1: Plotting">Module 6, Lesson 2, Video 1: Plotting</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 3: Analysis">Module 6: Lesson 3: Analysis</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 3, Video 1: Trend+Seasonal+Error">Module 6, Lesson 3, Video 1: Trend+Seasonal+Error</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 3, Video 1: Trend+Seasonal+Error">Module 6, Lesson 3, Video 1: Trend+Seasonal+Error</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 3, Video 1: Trend+Seasonal+Error">Module 6, Lesson 3, Video 1: Trend+Seasonal+Error</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 4: Real life examples">Module 6: Lesson 4: Real life examples</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 4, Video 1: Real life example">Module 6, Lesson 4, Video 1: Real life example</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 5: Review">Module 6: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 5 , Video 1: Review">Module 6, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 5 , Video 2: Review">Module 6, Lesson 5 , Video 2: Review</a>
</li>
</ul>
<hr/> -*- eval: (folding-mode t) -*-
<title>Course: Basic 3: Statistical methods</title>


<h1><a
name="Module 1: Estimation">Module 1: Estimation</a></h1>

<h2><a
name="Module 1: Lesson 1: The concept">Module 1: Lesson 1: The concept</a></h2>

<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 1: The concept (population,
sample)">Module 1, Lesson 1, Video 1: The concept (population,
sample)</a></h3>
</div>

<b>Total lesson duration
= 29.5</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 6.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.30</i></legend>
The world around us is full of unpredictable variations. Unpredictable, yet
the unpredictability has a pattern in it. Man has been studying this
pattern ever since his earliest existence.
---
 There are different ways to deal
with such patterned unpredictability, and statistics is one of them. In
this lesson we shall learn what sets statistics apart from the other
approaches.
---
 This difference is the key to appreciating statistical methods,
their strong points as well as their weak points. 
---
This key concept in a nutshell is this: Whatever data we collect is like a
<b>cup of water from a vast ocean</b>. The cup of water is all that we have to
base our inference on, but it is not the water in the cup that we want to
draw inference about.
---
 The  target of our inference is the entire ocean. The
statistical term for the cup of water is a <b>sample</b>, the ocean being
called the <b>population</b>. 
---
Any serious statistical exercise starts with a precise and clear picture of
the population and its relation with the sample.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Population and sample</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.70</i></legend>
We shall start with a simple example. The very term <b>population</b>
conjures up the vision of the totality of all the people living in a
country. While statistics uses the term in a broader sense, this
nevertheless remains a good picture to keep in mind. 
---
Here is our toy example. Imagine a population consisting of all the people living in a
large country. <b><font color="red">
[[men shown]] </font></b> We want to know the height of the
tallest person in our population.<b><font color="red">
[[line shown]] </font></b>  
---
Of course, if we had a list of the heights of all the
members of the entire population, then it would have been just a matter of
looking up the maximum, something that a computer could have done easily
for us.
---
 But the point is that we do not have such a list to start with.
 All that we can do  is to draw a random
sample of individuals and measure their heights
only.<b><font color="red">
[[highlighted]] </font></b>
---
 There is no guarantee
of course that this sample will contain the tallest person in the
population,...
---
 and so there cannot exist any sure  way to find the maximum
height in the population by looking at just the heights in the sample. The
best that we can hope for is only a reasonable guess.
---
 The precise statistical
term for obtaining  such a reasonable guess
is <b>estimation</b>. Let's try to come up with a good estimation procedure.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Estimation</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=3.50</i></legend>
 Suppose the heights
of the people in our sample are 5'2'', 5'5'', 5'0'', 5'9'' and
6'3''.<b><font color="red">
[[list shown]] </font></b> 
----
Based on this somone estimates the maximum population to be 6'.<b><font color="red">
[[6'?]] </font></b> Is this a
good estimate? No! Because even in our sample we have a person taller than
this!<b><font color="red">
[[finger]] </font></b>
---
 OK, let's revise our estimate to <b>6'3''</b> then, the maximum in our
sample. That is not as absurd as the last estimate, but still this assumes
that just by randomly selecting 5 persons out of an enormous population we
have managed to get one of the tallest guys in the entire
country!
---
 Not impossible, but not very probable either! So we should better allow some
margin above the sample maximum. How to choose the margin? Here are two ad
hoc suggestions: 
---
* One method could be to use the gap between the top two tallest persons in
the sample as the margin. So we shall compute maximum+gap between
the tallest two.<b><font color="red">
[[shown]] </font></b>, we have called this gap the top gap.
 In our case it is
<b><font color="red">
[[finger]] </font></b> <b>6'3''-5'9''=6''.</b>
---
 So we estimate the population maximum to be 6'3''+6'' = 6'9''.<b><font color="red">
[[shown]] </font></b>

* Another method could be to scale up the sample maximum by a factor 5/4,
which is sample size/(sample size-1),<b><font color="red">
[[shown]] </font></b>.
---
These are, as I said, ad hoc suggesions, and one can come up with many such
suggestions. This scenario is indeed common to all
statistical methods. We can think of many ad hoc procedures, and
need to choose the best, or at least a good one among them.
---
 Instead of just blurting out a single number as our
estimate, we are instead trying to come up with a  rule or formula to
produce the estimate from the sample. Like<b><font color="red">
[[finger]] </font></b> "sample max +
top gap" or "size/(size-1) * max".
---
Such formulae are called <b>estimators</b>, as opposed to
an <b>estimate</b> which refers to the numerical value that the formula takes for a
given sample. The target quantity that we are trying to estimate
is called a <b>parameter</b>.
---
 Indeed, any unknown quantity regarding the
underlying population is called a parameter, whether or not we
are trying to estimate it. 
---
Our parameter of interest was the population maximum. Had we possesed a complete list
of all the heights in the population, then this would have been a dumb
clerical exercise. No scope of creativty there.
---
 But since we do not have
such a master list available, we can propose and compare between different
estimators. Lots of scope for creativity here! 
---
So here is the take away message that you should never forget. The ultimate
aim of statistics is not to <b>look at data</b>, but <b>through data at
the underlying reality</b>. Sample corresponds to the data, population to
the underlying reality.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 2: The concept (sampling distribution)">Module 1, Lesson 1, Video 2: The concept (sampling distribution)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.20</i></legend>
In the last video we saw the simplest scenario of <b>sample</b> and
<b>population</b>. Indeed, what we discussed there is what most people would
anyway associate with these concepts. But statisticians use the terms
population and sample in a much more general sense.
---
In most branches of science, we need to make measurements. Now if you measure
the same quantity repeatedly under as identical conditions as you can manage,
still the measurements fluctuate to some extent.
---
 It is hard to pin point the cause behind the variation. But
something imponderable beyond our control changes somewhere.
---
 One cannot avoid noticing the uncanny resemblance of
this with a coin toss. The outcome is random.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Statisticians' perspective</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.80</i></legend>
Statisticians like to invoke the same population and sample analogy even in
this case. It may require some effort to visualise this at first, but it is
well worth getting used to.
---
 Think of each measurement as picking a random
sample from a population of all possible measurements. As if nature has in
her secret store all these slightly differing values.
---
That is the population of measurements. When you are
making 5 measurements, it is as if you are picking 5 of the stored
measurements and making them public.
---
 To distinguish this abstract usuage of
the term population, people sometimes call it an <b>infinite
population</b>. 
</fieldset>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.90</i></legend>
Admittedly this might look like a rather contrived way of looking at
things.  However, there is a theoetical justification behind this
approach. And it comes from the concept of <b> statistical
regularity</b>. 
---
We had discussed this in our Basic Statistics
course. But it boots repetition. 

In certain situations we see that lots of randomness piled together somehow
cancel each other out and a more or less regular behaviour emerges.
---
  Well, this concept makes a somewhat unexpected
appearance in the context of measurements. Suppose that you measure the
same quantity repeatedly keeping the set up as unchanged as
possible. Thus you get lots of numbers.
---
 If you create a histogram<b><font color="red">
[[start shown]] </font></b> of these numbers, then you'll see that
a particular shape emerges as you make more and more
measurements. <b><font color="red">
[[anim]] </font></b> 
---
The shape will depend on the quantity being measured
and the measurement procedure being followed. This fixed shape corroborates
the idea that there is something fixed sitting unseen beyond our immediate perception
and all our measurements are coming from that.
---
 The shape of the histogram as captured by this curve is
basically a rough picture of that unseen master process. 
---
The more data we collect the better we get an idea of that process. We could
have known it completely if only we could collect an infinite amount
of data. Hence the notion of an <b>infinite population</b>.
---
 We often use the term <b>distribution</b> to mean the same thing. Thus we talk about a
random sample from an infinite population, and sometimes refer to the same
thing as a random sample from a distribution. 
</fieldset>

<fieldset>
<legend>
(s4)[a] <i>Duration=1.40</i></legend>
You often hear statements like <b>Let's consider a random sample
from such-n-such distribution.</b> It is important to understand what this
means. 
---
In this context, a distribution, or rather <b>probability
distribution</b> to be precise, refers to a particular shape of the
underlying population histogram.
---
 The shape is often depicted as a curve<b><font color="red">
[[shown]] </font></b> for a continuous
variable, and a bar chart<b><font color="red">
[[shown]] </font></b> for a discrete one. We usually
capture the shape using math formula, called...
---
a <b>probability
density function (pdf)</b> for the continuous case,
and a <b>probability mass function (pmf)</b> for the discrete case.
---
 Suppose that we have some
variable in a data set.  If we
make a histogram of all its values, then the histogram will have that
particular shape.
---
 The shape need not be apparent if we have too few
cases. But as the number of cases grows the histogram will go closer and
closer to that shape. 
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 3: The concept (sampling distribution)">Module 1, Lesson 1, Video 3: The concept (sampling distribution)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.9</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.70</i></legend>
In the last video we have introduced the all important concept
that sets statistics apart from other approaches to analysing
data.
---
 Statistics, as I have already said, looks through data
instead of looking at data. We shall see an important consequence
of this concept now. 
---
In the Basic Statistics course as well as in our  school days we
have learned about computing the mean of a bunch of numbers. You
do not need to be a statistician to do that.
---
 But the way a statistician looks at  mean is not exactly how a layman considers
it. You are given a bunch of numbers. You crunch them with a
formula, and out pops the mean, which is
just another number situated more or less near the centre.
---
 From a layman's perspective the story ends there. But 
for a statistician it is a path leading
towards the mean of the unknown distribution from which the
sample has come.
---
  The observed sample  is just one possible sample that has turned up
randomly. Any conclusion based on that also suffers from the same
randomness. It is the underlying distribution that is the
unwavering truth worth seeking.
---
 When you hear this for the first
time it might not make much sense, besides sounding a bit
theatrical. But the germ of this idea is already there in our
everyday lives, as a little thought experiment will show.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">A little thought experiment</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=2.10</i></legend>
Suppose that you are comparing between two fertilisers for a
certain variety of crop. The aim is to see which variety produces
the greater yield.
---
 For this purpose two plots<b><font color="red">
[[shown]] </font></b> are chosen as
identical as possible. The same variety of crop is sown in
both. Fertiliser 1 is used in the first, fertiliser 2 in the second.
The yields turn out to be
234 bushels and 235 bushels<b><font color="red">
[[shown]] </font></b>.
---
 Here 235 &gt; 234. There can't be any
doubt about that. But is this evidence enough to clearly conclude
that the fertiliser used in the second plot is the better one?
 No, because the margin is so low here.
---
 Just a single bushel, a single bushel out of over 230 bushels. One might say that such
a small difference could very well have resulted from pure chance
variations. That's commonsense.
---
The idea is that if we repeat the
entire experiment all over again but using the same fertiliser
for both the plots,...
---
 even then some minor difference between the
two yields is quite likely, say a bushel or two this way or
that. That is like an inevitable error margin.
---
 One fertiliser can be called better than the other in terms of yield only when
it produces a difference significantly larger than this
margin. Thus you see our common sense has this idea built into
it:
---
 If we repeated the same experiment, some fluctuation is inevitable, and our conclusion
 must not get caught in this fluctuation.
---
Statistics formalises this notion into what is called the
<b>sampling distribution</b>. That's what we shall discuss in the next video.
</fieldset>

</div>



<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 4: The concept (sampling distribution)">Module 1, Lesson 1, Video 4: The concept (sampling distribution)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.20</i></legend>
The last video introduced the very important concept of <b>sampling
distribution</b>. Having a clear idea about sampling distributions is the key
to understanding the working of most statistical procedures.
---
We shall explain with a familiar example:
estimating the maximum height of people in a vast population.<b><font color="red">
[[shown]] </font></b> We
shall compare between two contending procedures.
---
 In both cases we shall start by drawing a sample of size 5
randomly from our population.<b><font color="red">
[[highlighted]] </font></b> 
---
 The first procedure will use just the <b>sample
maximum</b>, while the second procedure will multiply the sample max
by  5/4, which is the ratio sample size/(sample
size-1).<b><font color="red">
[[formula shown]] </font></b>
---
Once a particular sample of size 5 is drawn, these two estimators
will yield two numbers or estimates. Of course, just by staring at
those two estimates, we can never decide which estimator is
better. That's where sampling distribution comes to our
help.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=4.7</i></legend>
Show population of size 10000. Show true max. Draw sample. Find
estimates. Mention repeating this 1000 times. Show the results
already in a different sheet. Show histogram already made. Compare.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 5: The concept (sampling distribution)">Module 1, Lesson 1, Video 5: The concept (sampling distribution)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.5</b>
<p></p>


<fieldset>
<legend>
(s1)[a] <i>Duration=2.50</i></legend>
We have learned quite a few terms. A <b>parameter</b> is any unknown
quantity related to the underlying distribution. It is a fixed
quantity, part of the ultimate truth.
---
 To estimate it we need an
<b>estimator</b>, which is a formula. It is like a machine that takes a
sample in and produces a number out<b><font color="red">
[[shown]] </font></b>. In general any such formula
is called a <b>statistic</b>.
---
 It is a singular noun, no 's' at the
end. An estimator is a just a statistic that is being
used for the purpose of estimation.
---
 Every statistic has its own <b>sampling distribution</b>, its
behaviour when different random samples from the same population
are fed into it.  
---
This is basically what we showed in the last lab session. We
played god. We generated many random samples, evaluated the
statistic for them and made a histogram to provide a visual
representation of the sampling distribution. 
---
While a visual representation is a good thing to have, we can use
techniques like <b>mean</b> and <b>standard deviation</b> to describe the
sampling distribution quantatively. 
---
The mean of the sampling distribution is the value around which the values of the
statistic are expected to hover. This is often called the 
expected value or <b>expectation</b>.
---
 So if the statistic is an
estimator, then the diffrence between this expectation and the true
value of the parameter is a useful thing to have. This is called
the <b>bias: mean value of estimator - parameter</b>.
---
 Of course, you may not be able to compute this since the parameter value is
unknown. But sometimes you can even without knowing the true
parameter value! We shall take this issue up in
the next lesson. 
---
Then you can also consider the standard deviation of the sampling
distribution. That has a special name here: the<b> standard error</b> of
the statistic. Unlike bias, which makes sense only for
estimators, standard error is meaningful for any statistic. 
---
The next video will give us an idea about bias and standard
error using LibreOffice.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Start with a population. Compute min. Draw sample, compute sample
min. Show already computed values. Compute their mean, subtract
true min. Also compute standard error. 
</fieldset>

</div>

<fieldset>
<legend>M1L1</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Two statisticians have drawn two random samples from the
population of all households in a village. Data have been
collected about monthly expenditure of each sampled household. Can the sample means
be different? Can the poplation means be different?<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;If we collect two samples of sizes 10 and 50, and compute
the sample mean, then which will have the higher
dispersion?<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;All the values in a population are known to be more or less
the same. Then would you go for a large sample or a small one?<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Suppose that we want to estimate the maximum income of a
population. Can the sample maximum be more than the population
maximum?<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Same question about mean.<img src="../image/box.png"></p>

</pre>
</fieldset>



<h2><a
name="Module 1: Lesson 2: Estimating mean">Module 1: Lesson 2: Estimating mean</a></h2>

<b>Total lesson duration
= 27.9</b>
<p></p>

<div class="header">
<h3><a
name="Module 1, Lesson 2, Video 1: Estimating mean">Module 1, Lesson 2, Video 1: Estimating mean</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 1.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.80</i></legend>
 In each of this and the next two lessons we shall take up a different parameter and
learn about estimating it. 

In the current lesson the parameter to be estimated is the
<b>mean</b>.
---
 Remember that here we are talking about the <b><font color="red">
[[shown]population
mean] </font></b>, and not
the sample mean, which we anyway know for sure for the sample at
hand, and hence we do not need to estimate.
---
There is an underlying
distribution. It's best to think
of it as the shape of a histogram<b><font color="red">
[[shown]] </font></b>.
---
 This distrbution has a mean<b><font color="red">
[[arrow shown]] </font></b>, a fixed number, which is also
unknown. It is this unknown number that we are 
trying to estimate. That is our parameter of interest.
---
 All that we have is a random sample from this distribution,  a bunch of
numbers whose histogram roughly resembles this shape.
---
 Our job is to
obtain a good estimator in terms of this random sample.
---
Here is a simple solution. If the sample histogram closely
resembles the population histogram, and our aim is to estimate
the centre of the population, then why not use the centre of the
sample histogram for that purpose?
---
 In other words, just use the sample mean to estimate the
population mean. 
---
we shall now see the sample mean in action using LibreOffice.
So in the next video we shall go to
the lab to explore
the use of sample mean for estiating the population mean.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 1, Lesson 2, Video 2: Lab">Module 1, Lesson 2, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Normal data already present. Draw a Sample. Find mean. Show lots
of means and histogram already in different sheets. Mention how
adding a constant to population shifts the sample mean.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 1, Lesson 2, Video 3: Sampling distributions of mean">Module 1, Lesson 2, Video 3: Sampling distributions of mean</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.80</i></legend>
The lab session in the last video gave us some idea about the
sampling distribution of the sample mean. A more quantitative idea may
be had using its bias and standard error.
---
 Recall that the bias of
an estimator is the <b>E(estimator) - parameter</b>. In
general, this may not be computable, since the parameter value is
unknown.
---
 Here, however, we can compute the bias, and it just turns out to
be zero! So we call sample mean an <b>unbiased</b> estimator of the
population mean.
---
Let's see why sample mean is unbiased for poplation mean.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Proof</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=4.5</i></legend>
Proof of unbiasedness.
</fieldset>

</div>



<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.10</i></legend>
In the last video we looked at the bias of sample mean as an estimator
of the poplation mean. 
Next let's turn to the <b>standard error</b> of sample mean.
---
 This is the standard
deviation as computed from the sampling distribution of the sample
mean. 
---
 As may be expected this is closely connected with the standard deviation
of the underlying distribution. If the population standard
deviation is less, so should be the standard error of sample
mean.
---
 Also another thing should be intuitively obvious: the
larger the sample size, the better we can guess the underlying
distribution. So the standard error should decrease with
increasing sample size.
---
 In fact, we can show that the standard
error is 
<b>$$
\frac{\sigma}{\sqrt n},
$$</b>
where $\sigma $ is the population standard deviation,
and $n$ is the sample size. Let's look at the proof.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Proof</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
Proof.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 4.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.30</i></legend>
There is a very important theorem in statistics called the <b>Central
Limit Theorem</b> about the sampling distribution of the mean.
---
 It says that even if you do not know the  population distribution,
but just know its mean and standard deviation, ... then you
basically know the distribution of the sample mean!

</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
CLT statement.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.20</i></legend>
Let's look at a practical example. <b>Confidence interval</b>. Let us see ... the
mathematical formulation.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
CI example.
</fieldset>

</div>



<fieldset>
<legend>M1L2</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Generate random sample. Compute mean repeatedly.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Explore standard error.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Explore bias.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Estimate mean using trimmed mean.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Estimate mean using median.<img src="../image/box.png"></p>

</pre>
</fieldset>




<h2><a
name="Module 1: Lesson 3: Estimating proportions">Module 1: Lesson 3: Estimating proportions</a></h2>

<b>Total lesson duration
= 25</b>
<p></p>

<div class="header">
<h3><a
name="Module 1, Lesson 3, Video 1: Estimating proportions">Module 1, Lesson 3, Video 1: Estimating proportions</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.00</i></legend>
In the first lesson of this week we had learned about the concept
of estimation. We are now learning how to apply this concept to
various parameters of interest.
---
 In the last lesson we learned how to estimate
the population mean. In this lesson we shall estimate <b>population proportion</b>. 
---
First let's understand the importance of the problem with a real
life example. Suppose that an <b>election</b> is imminent in a
country. There are two major contending parties. Each striving
its best to win the election.
---
 What will they not give to know the result of the election
beforehand!  If only they knew where their
support base is weaker, they would be able to focus more effort
in those regions.
---
 In abstract terms this is the problem of estimating a proportion, 
the proportion of the people who would vote for that
party. <b><font color="red">
[[proportion of supporters]] </font></b>
---
As yet another application, consider  a <b>casino</b>. They use
various gambling devices there, slot machines, roullette wheels
etc. They need to be very carefully tuned in order to rake money
for the house.
---
 For instance, if a roullette wheel is slightly
tilted resulting in certain numbers showing up more often and the
gamblers get a hint of the that, then that means ruin for the
house.
---
 So the casino owner has to keep an eye on the unbiasedness of
his devices. How can he do that?  Let's consider the simplest
gambling device, a coin<b><font color="red">
[[actually show]] </font></b>.
---
 Given a coin how do you find its <b>probability of
showing head</b>? That is an example of a population proportion. Unlike the radius, thickness or mass of the coin,
its probability of head is not a directly observable feature. 

That's where estimation of a population proportion becomes important.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">A simple solution</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=2.20</i></legend>
How would you estimate the <b>probability of head</b> of a coin? What is
the commonsense method? Just toss the coin a large number of
times and find the proportion of times you get head.
---
 Say I toss the coin <b>1000</b> times, and find <b>465</b> heads. Then I estimate the
probability of head as <b>$\frac{465}{1000]=0.465.$</b> Of course,
what you get in this way is the <b>sample proportion</b>.
---
 But if the sample is large enough, then it should be close to the population
proportion, thanks to statistical regularity.
---
Similarly for the election problem we can draw a sample of voters
and try to determine their political leanings. Then find the
proportion based on the sampled voters.  
---
There is another equivalent way of looking at estimating
population proportion that may be understood using this voter
example.
---
 Associate a number 0 or 1 with each voter in the
population according as the voter is against or for the party in
question.<b><font color="red">
[[shown]] </font></b> This creates a new variable.
---
 Notice that the population
mean of this variable is precisely the proportion we are trying
to estimate. Because when you sum, you are basically counting the
1's. 
---
Thus estimation of population proportion is just a
special case of estimation of population mean. So we may use
sample mean of this variable as before. It is easy to see that
this sample mean is just the sample proportion. 
---
The advantage of this line of thought, considering proprtion as a
special case of mean, may not be readily apparent. The main
advantage is that  sample proportion inherits all the
properties of the sample mean.
---
 This will make our life easier when we shall try to work with the sampling distribution of
sample proportion.

But first let's look at a little hands on computation.
</fieldset>

</div>

<div class="header">
<h3><a
name="Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Voting data already present. Draw a Sample. Find mean after ifelse. Show lots
of proportions and histogram already in different sheets. 
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.80</i></legend>
The lab session in the last video gave us some idea about the
sampling distribution of the sample proportion. As in the case
sample mean, a more quantative idea may
be had using its <b>bias</b> and <b>standard error</b>.
---
 We know  that the bias of an estimator is the <b>expectation of
the estimator - parameter</b>. In 
general, this may not be computable, since the parameter is
unknown.
---
 But in certain special cases the bias may be computed
indirecly via mathematical reasoning even without knowing the
value of the parameter. This was the case for sample mean, and
this is also going to be the case with sample proprtion.
---
 And that is hardly unexpected because as we have already
mentioned that sample proportion
is a special case of sample mean, it is the mean of a 0-1 veriable. 
---
Sample mean was <b>unbiased</b> for population mean. So here sample
proportion is unbiased for population proportion. Thus, while the
sample mean is not guaranteed to equal the population proportion
but it is likely to hover centred around the true value.
---
Next let's turn to the <b>standard error</b>. This is the standard
deviation of the sampling distribution of the sample
proportion. Again we shall invoke the corresponding result for sample
mean.
---
 There the standard error was 
<b>$$
\frac{\sigma}{\sqrt n},
$$</b>
where $\sigma $ is the population standard deviation,
and $n$ is the sample size.
---
 Now our variable takes only the values 0 and 1. 
So $\sigma$ may be... simplified further. 
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=3</i></legend>
Let
the population proportin be $\theta.$ Then $\sigma $ is 
$$
\frac1N\sum x_i^2-(\bar x)^2 = \theta - \theta^2 = \theta(1-\theta).
$$
Becuase $x_i^2 = x_i$ since $x_i$ is either 0 or
1. Thus the standard error is 
$$
\frac{\theta(1-\theta)}{\sqrt n}.
$$
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.80</i></legend>
We have a large store full of items,<b> $\theta$ proportion
of which are defective</b>. This $\theta$ is unknown. We want to
estimate it.
---
 For this we have drawn a <b>random sample of 100</b> items,
and found <b>73 defective</b> items. This gives us a point
estimate $\frac{73}{100}.$<b><font color="red">
[[anim]] </font></b>
---
 But we want a $95\%$ CI, ie,
two numbers <b>$L$ and $U$</b> based on our sample such that 
<b>$P(L\leq \theta \leq U ) = 0.95.$</b> Let us see how the central
limit theorem ... helps us here.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>

</fieldset>

</div>

<div class="header">
<h3><a
name="Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.60</i></legend>
In case of estimating population  mean we had used
sample mean. Now sample mean is not robust against outliers. So
if we suspect presence of outliers, a robust altenative like
trimmed mean may be preferable.
---
Now sample proportion is a special case of sample mean.
But we do not have to worry about outliers here, because here our
variable takes only the values 0 and 1 by design. So no question of
extreme values creeping in.
---
 However, there is a problem from a
different direction, as we are about to discuss. 

We had been pretty close to this problem when we talked about the
election scenario.
---
 There our suggestion was to sample some voters
and find out their political leanings. Now a voter is entitled to
privacy. He/she may not agree to reveal his/her political
leaning.
---
 Even worse, a voter may simply lie. This poses a
different problem, the problem of respondents trying to hide
sensitive information. 

The  problem also occurs in a rather different context. 
---
Suppose we have a <b>multiple choice question with 4 options</b>
exactly one of which is known to be correct. We want to estimate
the proportion of students who know the correct answer.
---
 If we
simply take a sample of students, ask the question, and find the
proprtion of correct answers, then that won't be a good estimator
[qn: biased up or down?] as a student may have answered correctly
by chance.
---
So here we need to model the students' thought process. Either a student
knows the answer or not.<b><font color="red">
[[tree shown]] </font></b> In the first case, she answers
correctly<b><font color="red">
[[shown]] </font></b>, but in the second case she makes a
guess<b><font color="red">
[[shown]] </font></b>, 
say a random
guess<b><font color="red">
[[tree grows]] </font></b>.
---
Let the true probability of her knowing the
correct answer be $\theta.$<b><font color="red">
[[shown]] </font></b> So this is $1-\theta.$
When she guesses,  she has $\frac 14$ chance of getting the
correct answer<b><font color="red">
[[shown]] </font></b>, and $\frac 34$ of getting it wrong.
---
 Then the probability of her
giving the correct answer is <b>$\theta + (1-\theta)/4.$</b>
<b><font color="red">
[[finger expl]] </font></b> It is
this thing that is estimated well by the sample
proportion, $p$ say, of correct answers. 
---
So we may say 
<b>$$
\theta + \frac 14(1-\theta) \approx p,
$$</b>
hence <b>$\theta\approx \frac{4p-1}{3}.$</b>
---
This is not entirely intuitive. Also, if this turns out to be
negative, we should of course take the estimated $\theta $ to be 0. 
---
 A similar
application of this idea is used to estimate population
proportion of sensitive issues like political leanings. The
method is called randomised response.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Randomised response</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.30</i></legend>
<b>Randomised response</b> refers to situations where the respondents
add a layer of extra randomness to hide the truth. The student
guessing in an MCQ was one example where the randomisation was
unplanned. Here is a planned version of the same.
---
 Again consider the election scenario with two contending parties. We pick a
random sample of voters and ask them: <b>"Will you vote for party
A?"</b> But in order to protect their privacy you also hand them a
fair die<b><font color="red">
[[shown]] </font></b>...,
---
 ask them to roll it in private, and <b><font color="red">
[[scheme shown]] </font></b> answer truthfully if
the die shows 6, and lie if the die shows something else.
---
 Thus, let's say I am a voter favouring party A.<b><font color="red">
[[finger]] </font></b> I
roll my die, get a <b>5</b>, 
so I answer "No". My friend is against party A, she gets a 6, and
she also answers "<b>No</b>". 
---
Let's see how we may estimate the proportion of A supporters from
the responses we thus obtain.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Estimation</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.00</i></legend>
 A voter is either for or against party A, with probabilities $\theta$
and $(1-\theta),$<b><font color="red">
[[tree top shown]] </font></b>
respectively.
---
 Suppose he is for A. <b><font color="red">
[[finger]] </font></b> Then the die may show a 6 or not,
probabilities $\frac 16$ and $\frac 56.$<b><font color="red">
[[tree bot left
shown]] </font></b> Similarly if he is against A.<b><font color="red">
[[bot right shown]] </font></b>
---
Accordingly there
are two ways one may answer "Yes".<b><font color="red">
[[finger]] </font></b>
---
 The total probability is 
<b>$$
\theta \times \frac 16 + (1-\theta)\times \frac 56.
$$</b>
This is estimated by the sample proportion of "yes".
---
 If this
proportion is $p,$ then 
<b>$$
\theta \times \frac 16 + (1-\theta)\times \frac 56\approx p.
$$</b>
So <b>$\theta \approx \frac{5-6 p}{4}.$</b>
---
This is called <b>Warner's method</b>. There are different variations of
the same idea in use.
</fieldset>

</div>

<fieldset>
<legend>M1L3</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Estimate by drawing a random sample.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Line diagram that converges.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Explore standard error.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Explore bias.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Combining two estimates from samples of different sizes.<img src="../image/box.png"></p>

</pre>
</fieldset>



<h2><a
name="Module 1: Lesson 4: Estimating dispersion">Module 1: Lesson 4: Estimating dispersion</a></h2>

<b>Total lesson duration
= 27.1</b>
<p></p>


<div class="header">
<h3><a
name="Module 1, Lesson 4, Video 1: Estimating dispersion">Module 1, Lesson 4, Video 1: Estimating dispersion</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.60</i></legend>
We are continuing with our plan of estimating various parameters
following the idea presented in the very first lesson of this
module. We have already learned how to estimate population mean,
and population proportion.
---
 In this lesson we shall attack
<b>population dispersion</b>. Now dispersion is just an intuitive
concept, it cannot be a parameter. We need some conrete measure
of dispersion to act as our parameter.
--- 
<b>Population variance</b> and <b>population standard deviation</b> are the two
most popular choices.
---
The need for this estimation could be felt even when we learned
to estimate population mean using sample mean. There the standard
error was <b>$\frac{\sigma}{\sqrt n}.$</b>
---
Here $\sigma $ is the population standard deviation,
and $n$ is the sample size.

 This quantitiy gave us an idea about the precision of our estimator. But how
can it be useful, because it involves $\sigma,$ which is
unknown? 
---
So we need to estimate $\sigma$ or
equivalently $\sigma^2.$ 
---
Throughout this lesson keep this picture at the back of your mind.
There is an underlying
distrbution (the unknown truth). It's best to think of it
as the shape of a histogram. The spread of this histogram is what
we have in mind.
---
 A population with low  scatter is often easier to
work with, as all the members are more or less similar. A
population with wide scatter requires much more effort to
explore. 
---
So prior to any detailed exploration of a
population, we need to estimate its variance to get an idea of
how much work lies ahead.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">A simple solution</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.30</i></legend>
We already know a simple solution from our experience so far with
estimating population mean and proportion. Just use the sample
analog.
---
 By the way, this idea of estimating a population
parameter by using the corresponding sample quantity has a name. It
is called the <b>plugin principle</b>. 
---
It is justified on the ground
that the sample histogram closely resembles the population
histogram. Here this principle suggests
using sample variance as an estimator of population
variance, and...
---
 sample standard deviation as an estimator of
population standard deviation. 
---
By the way, we had noticed earlier in the Basic Statistics course
that there are two alternative definitions of sample variance
(and hence of sample standard deviation). One is<b><font color="red">
[[move]] </font></b> 
<b>$$
\frac 1n\sum (x_i-\bar x)^2
$$</b>
and the other is 
<b>$$
\frac{1}{n-1}\sum (x_i-\bar x)^2.
$$</b>
---
LibreOffice, as well as most other standard statistical softwares,
use the second formula by default. Earlier we had glossed over this point
in our basic Statistics course. But here we shall give a more detailed
exposition. 
---
But first it is time to go to the lab in the next video.
</fieldset>

</div>

<div class="header">
<h3><a
name="Module 1, Lesson 4, Video 2: Lab">Module 1, Lesson 4, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Normal data already present. Draw a Sample. Find variance. Show lots
of variances and histogram already in different sheets. Mention how
adding a constant to population shifts leaves the result unaffected.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 1, Lesson 4, Video 3: Sampling distributions">Module 1, Lesson 4, Video 3: Sampling distributions</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.10</i></legend>
The lab session in the last video gave us some idea about the
sampling distribution of the sample variance. A more quantitative idea may
be had using its bias and standard error.
---
 The bias of
an estimator is the <b>mean of the estimator - parameter</b>. In
general, this may not be computable as a number, since the parameter is
unknown.
---
 But in case of sample mean as well as sample proportion it
turned out to be zero. So there the estimators were unbiased. But
the situation somewhat different for sample variance.
---
 For one thing the estimator 
<b>$$
\frac 1n\sum (x_i-\bar x)^2
$$</b>
is not unbiased.
---
 We really cannot compute the bias as a
number, only express it as a formula involving the unknown parameter.
Let's do so.
---
The expected value of this
estimator is 
<b>$$
\frac{n-1}{n}\times \sigma^2.
$$</b>
---
So the bias is <b>$-\frac 1n \sigma^2.$</b> [IVQ: over-est or
under-est?]

Let's prove this fact.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Proof</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Proof
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 1.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.70</i></legend>
The proof of this fact <b><font color="red">
[[shown]] </font></b>
that we saw in the last video may be too technical
for your taste. 
---
It has one implication, however, that may 
be appreciated without going too deep into mathematics.
---
Notice that the expected value<b><font color="red">
[[finger]] </font></b> is a known multiple
of $\sigma^2.$ This $\frac{n-1}{n}$ is known because the
sample $size$ $n$ is known.
---
 Also, it is not random, though the sample itself is random,
because the sample size is known even
before the sample has been drawn.
---
  So we may just divide our estimator by this known constant
to make the expected value exactly equal to $\sigma^2.$ When
we do this we arrive at
<b>$$
\frac{1}{n-1}\sum (x_i-\bar x)^2.
$$</b>
---
So now you know where that alternative estimator came from.

Next we should turn to the standard error. Unfortunately, the
formula is rather complicated. So we shall not torture ourselves
with that formula here.
---
 However, as we may guess, the standard
error will go down as the sample size increases.

Incidentally, it might interest you to know the shape of the
sampling distribution.
---
 Even though we assume a nice symmetric
bell shape for the population, the sampling distribution is
asymmetric, or skewed to the right, to use a precise mathematical
term. We have already seen the shape in the last lab session.
</fieldset>

</div>



<div class="header">
<h3><a
name="Module 1, Lesson 4, Video 4: Alternatives">Module 1, Lesson 4, Video 4: Alternatives</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.40</i></legend>
Sample variance (whether with an $n$ or a $n-1$ in the
denominator) is a natural estimator of population variance.
But it suffers from lack of robustness.
---
Suppose that there is a little spike in our underlying distribution.<b><font color="red">
[[shown]] </font></b> 
A distribution, as we have mentioned earlier, is basically the
shape of a histogram.
---
The spike<b><font color="red">
[[finger]] </font></b> indicates a little contaminating
value, an extreme value that just does not fit with the rest of
the values.
---
Now because it is just one bad point as opposed to many many good
points, the population variance is not influenced much by it. But imagine
what may happen when you draw a random sample of much smaller
size.
---
 If that spike somehow gets into your sample it might wreak
havoc. Because there are only a handful of good values to counter
it. As a result the sample variance will move far from the
population value. 
---
So we need something more robust. Something that does not care
much about extreme points. <b>Median Absolute Deviation (MAD)</b>
is one good option. Let's quickly recall the definition
and computation of MAD.
</fieldset>

<fieldset><legend
    class="cu">
[r] <i>Duration=3.5</i></legend>
[Computation of MAD]
</fieldset>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.90</i></legend>
 However, it is rather difficult to compute its bias, because its formula
involves abolute value, and that makes mathematical computation of its
expected value difficult. 
---
Now there is one point that must be bourne in mind. Our aim is to
estimate population dispersion, and not necessarily population
variance.
---
 Do not consider MAD as an estimator of population
variance or population standard deviation. Think of it as an
estimator of population MAD. 
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 4.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.30</i></legend>
In the last video we talked about using sample MAD to estimate
population dispersion, as captured by the population MAD. 
Similar is the case with
<b>interquartile range</b>. Let us quickly recall its computation. 
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=4</i></legend>
Show IQR computation.
</fieldset>

</div>



<fieldset>
<legend>M1L4</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Estimate by drawing a random sample.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Explore standard error.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Explore bias.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Combining two estimates from samples of different sizes.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Estimating using range and semi IQR<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Compare standard errors of different techniqes.<img src="../image/box.png"></p>

</pre>
</fieldset>



<h2><a
name="Module 1: Lesson 5: Review">Module 1: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 1, Lesson 5 , Video 1: Review">Module 1, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 1, Lesson 5 , Video 2: Review">Module 1, Lesson 5 , Video 2: Review</a></h3>
</div>



<h1><a
name="Module 2: Test of hypotheses">Module 2: Test of hypotheses</a></h1>


<h2><a
name="Module 2: Lesson 1: Concept of test">Module 2: Lesson 1: Concept of test</a></h2>

<div class="header">
<h3><a
name="Module 2, Lesson 1, Video 1: The concept of test (sound of water)">Module 2, Lesson 1, Video 1: The concept of test (sound of water)</a></h3>
</div>

<b>Total lesson duration
= 27.3</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 5.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.50</i></legend>
In this module we shall learn about <b>test of statistical
hypotheses</b>. What is that? You may ask. Well, it's a long story
with many details that might scare beginners away.
---
 We shall go into
those details later. For now let's start with a simple
non-mathematical example. 
---
Here I have a sealed container, and I want to know if it is half
filled with water. How do I ascertain that? Remember: my aim is
not to find the actual content of the container,...
---
 but just to
provide a yes/no answer to the question: Is it partly filled with
water?

Why not just open it and see?
---
 Well, it  is sealed, and I
cannot open it. So guess work is my only option. 
---
Let's start by feeling  the weight. It's not empty. But it could
be water or wood or something else.
---
 Let's shake it, and listen to the sound. It does not sound like water at all. It actually
sounds like...any way who cares! That it does not sound like
water is all I need to know. That's enough info for me to
conclude that it cannot be half filled with water.
---
Test of statistical hypotheses is just a formal way of carrying
out this same common sense procedure.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">More formally</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.40</i></legend>
The set up started with two possibilities: either the container
is <b>partly full of water</b> or <b>not that</b>. In statistical parlance we call
these  two <b>hypotheses</b>. 
---
One of them is called the <b>null hypothesis</b>
the other is called the <b>alternative hypothesis</b>. We shall learn
later how to choose which one to call the null and which one to
call alternative.
---
 But to keep going let's say "partly filled with
water" is our null hypothesis. We denote the null hypothesis by
the symbol <b>$H_0$</b> and the alternative one
by <b>$H_1$</b>. 
---
The two hypotheses must cover all possibilities, and must not
overlap. Thus, it should not be possible to have a situation
where neither hypothesis holds or both the hypotheses
hold. 
---
 Deciding in favour of one of these hypotheses is called a
<b>test of hypotheses</b>. Opening the container and peeping inside
would have given the answer for sure, but that was impossible by
the rule of the game.
---
 So guessing based on imperfect indirect
knowledge is the only option. Hence we have <b>statistical</b>
hypotheses testing.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Test statistic</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=2.00</i></legend>
When we picked up the container, felt its weight and gave it a shake,
and listened to the sound,
we were collecting <b>data</b>, some useful and some not. The weight did
not help us at all, but the sound did.
---
 The relevant aspect of the data is called the <b>test
statistic</b>. 
Here we meet the term
statistic again, a quantity computed based on the sample.
---
 Since
it is being used for testing, so the term test statistic. Its
choice crucially depends on the two hypotheses we are trying to
distinguish between.
---
 For instance, had we tried to distinguish
between water and milk, sound would have been of little value,
because either would have sounded about the same.
---
  If the two
hypotheses were water versus empty, then weight itself would have
provided enough clue.
---
The test statistic should be some aspect of the data that behaves
very differently under the two hypotheses. In order to decide if
the null hypothesis is true we need to know the behaviour of the
test statistic under the null hypothesis,...
---
 ie. the sound of water
in our toy example. This is called the <b>null distribution</b> of the
test statistic.
---
We compare the observed value of the test statistic with its
null distrbution. If the observed value is too extreme compared
to what is expected for the null distrbution, then we naturally
cast our vote in favour of the alternative hypothesis.
---
 This decision is customarily called <b>Rejecting $H_0$</b>.
If, on the other hand, the observed value
of the test statistic is nothing incongruous with the null
distrbution, then we <b>Accept $H_0.$</b>

</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 1, Video 2: The concept of test (sound of water)">Module 2, Lesson 1, Video 2: The concept of test (sound of water)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.60</i></legend>
In the last video we talked about comparing the observed value of
the test statistic with its null distribution, ie, how the test
statistic is expected to behave if the null hypothesis were true.
---
This comparison may be done in a couple of ways:

* using <b>p-value</b>
* using <b>critical value</b>
---
The p-value technique is what is used by most standard
statistical softwares and is the easier to explain. So we shall
start with that.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">p-value</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.40</i></legend>
In the Basic Statistics course we had already discussed <b>p-value</b> as a means of checking if a
given value is too extreme compared to a bunch of numbers. This
is precisely what we need here.
---
 We know the <b>null distribution</b>,
ie, the type of values that the test statistic is expected to
take if the null hypothesis were true.
---
 And we are to compare the
<b>observed value of the test statistic</b> with these. If the value is
too extreme, then we shall reject $H_0$, else we shall
accept $H_0.$ 
---
As we have already learned in the Basic Statistics course, the
p-value is basically the chance that the test
statistic may be more extreme  than the observed value assuming
the null hypothesis is true.
---
 If this is small, then the observed
value is already too extreme. and we reject $H_0.$ Clearly
smaller p-values favour rejection of $H_0.$
---
 How small is
small enough? A commonly used threshold is <b>5%</b>. If you want to be
more liberal use 1%. Whatever threshold you use is called
the <b>level of significance</b> of the test. 
---
It is possible to use a mathematical curve
to find p-values. Let's understand this...using pictures.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
Show points along a numberine explain p-value. Draw
histogram. Approx by curve. Explain by area.
</fieldset>

<fieldset>
<legend>
(s3)[a] <i>Duration=0.50</i></legend>
Thus the pvalue technique for test of statistical hypotheses is
to <b>compute the test statistic</b>, <b>compute the p-value</b> and
reject/accept $H_0$...
---
 according as the p-value is below or
above 0.05<b><font color="red">
[[shown]] </font></b>. Statistical softwares generally just give you 
the p-value, leaving the choice of the cut off to you.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 1, Video 3: Lab">Module 2, Lesson 1, Video 3: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.20</i></legend>
In this video we shall take a closer look at the concept of
p-value in the context of test of hypotheses.
</fieldset>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Toy example: start with data and some arbit test
statistic.  Give lots
of typical values in a different sheet. Find p-value based on
them. Accept or reject. Mention that this is an exploded view,
not recommended in practice.
</fieldset>

</div>


<fieldset>
<legend>M2L1V1</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;We have a random sample of patients. We have found the
proportion of COVID infections among them. Which of the
following is a valid hypotesis to e tested?
<ol type="">

<li>The sample proportion is 0.5</li>

<li>The population proportion is 0.5</li>

<li>The  sample proportion is less than the population proportion.</li>

</ol>

<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;A test has been conducted at 5% level of significance. This
means
<ol type="">

<li>we make a mistake with at most 0.05 probability.</li>

<li>A correct null hypothesis is rejected with probability at
most 0.05.</li>

<li>A wrong null hypothesis is accepted with probability at most 0.05.</li>

</ol>
 
<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Power is 
<ol type="">

<li>P(type 1)</li>

<li>1-P(type 1)</li>

<li>P(type 2)</li>

<li>1-P(type 2)</li>

</ol>

<img src="../image/box.png"></p>

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 2, Lesson 1, Video 4: Critical value method">Module 2, Lesson 1, Video 4: Critical value method</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.80</i></legend>
The <b>p-value method</b> is simple, but leaves one important question
unanswered. How do we choose the test statistic? This
question is not terribly important for us in this course,...
---
 because we shall be using standard softwares where the test statistic is
already built in. But still it is good to have an idea. And that
is where the <b>critical value method</b> will help us. 
---
Incidentally, it will also expose us to some more statistical
jargon. 

In a statistical hypothesis testing scenario our conclusion is
based only on indirect evidence, and hence liable to
errors. There are two types of errors. Let's discuss them.
---
 There are two hypotheses that cover all possibilities without any overlap
<b><font color="red">
[[H0 and H1 shown in column headed truth]] </font></b>. Accordingly we have two possible verdicts:
<b>accept $H_0$ and reject $H_0$</b><b><font color="red">
[[shown as row
with head verdict]] </font></b>.
---
So we have a tabular structure with 4 cells.<b><font color="red">
[[shown]] </font></b> These two cases
correspond to correct decisions.<b><font color="red">
[[finger]] </font></b> $H_0$ correct, and we have
rightly accepted it. Here $H_0$ does not hold, and we have
indeed rejected it.
---
 These two cases correspond to errors. Here we
have rejected $H_0$ even though it is actually correct, and here
we have accepted a wrong $H_0.$ These two are called type
I and type II errors.<b><font color="red">
[[shown]] </font></b>
---
 Notice that if we swap the
labels $H_0$ and $H_1$, then the  types are also
swapped. The convention is to call the <b>more serious</b> of the two
errors Type I, an accordingly label the hypotheses as $H-0$
and $H_1.$
---
Let's understand this with an example.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">An example</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.20</i></legend>
Suppose that I have a tumour in my hand. I am worried about its
being cancerous. So there are two possibilities: it is cancerous
or it is not. These are my two hypotheses.
---
 I visit a doctor, who
makes diagnostic measurements (ie collects data) and either says
"Yes, it is cancer" or "No, it isn't".<b><font color="red">
[[table shown]] </font></b> Now let's look at the two
errors.
---
 Here<b><font color="red">
[[finger]] </font></b> I do not have cancer, but the doc says cancer. So I
pass a few sleepless nights, curse my fate, and spend some money
to get my harmless tumour treated.
---
 Now let's see the consequence
of the other error. I do have cancer, but the doc says, "Don't
worry, you're just fine!" I feel very happy, go home, cancer
spreads, and RIP.
---
I hope you'll agree that the latter error is the more serious of
the two. So we shall call that our Type I error. Now type I error
is the error of rejecting a correct $H_0.$ So we label the
two hypotheses accordingly. 
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Level of significance</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.20</i></legend>
Since Type I error is the more serious, we naturally want to
guard against that first. We set an upper bound on the
probability of committing that error. This upper bound is called
the <b>level of significance</b> or the <b>size</b> of the test.
---
 Common choices are 5% and 1%. Subject to this we choose a test that minimises
the probability of the other error. This minimisation dictates
the choice of the test statistic, as well as of the cut off
value, or <b>critical value</b>, as it is called.
---
In this approach we have to  compute the test statistic from the data and
compare it against the critical value. Depending on the side of
the critical value our test statistic lands, we either accept for
reject $H_0.$
---
However, in our course we do not need to worry about all
these. The softwares already know the best test statistic to use
in each case, and so the simpler p-value approach will give the same
result as the more ambitious critical value approach.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
A huge lot of items. Proportion of defective items is \theta
unknown. 
The lot is is good if H_0: \theta \leq 0.2  vs H_1: \theta &gt; 0.2. Accept a
lot based on a sample 
of 10 iff all ok. Compute type I and type 2 prob.
</fieldset>

</div>

<fieldset>
<legend>M2L1V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Estimate  the upper-tailed p-value of ... w.r.t. the
typical values ..., ..., ... <img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Estimate  the lower-tailed p-value of ... w.r.t. the
typical values ..., ..., ... <img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Estimate  the two-tailed p-value of ... w.r.t. the
typical values ..., ..., ... <img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 2: Lesson 2: One sample t-test">Module 2: Lesson 2: One sample t-test</a></h2>


<div class="header">
<h3><a
name="Module 2, Lesson 2, Video 1: one-sample t-test">Module 2, Lesson 2, Video 1: one-sample t-test</a></h3>
</div>

<b>Total lesson duration
= 22.3</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 6.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.00</i></legend>
There are many different types of tests of statistical hypotheses
suited for different purposes. We shall talk about quite a few of
them this week.
---
 Let's start with the one that is possibly
the easiest to understand and has wide application. We shall
start with an example. 
---
We all use bottled liquids, milk, shampoo, oil, medicines. The
container bears a label telling us the amount of the content. How
does the manufacturer ensure that the amount is as it should
be?
---
 Of course, if we fill each bottle manually we may
painstakingly measure out the right amount and pour it in. But
these bottles are not packaged manually. The amounts are poured
out by some automated device, and churned out by the hundreds.
---
 If some setting somewhere goes off a bit, it will have a disastrous
effect. So the manufacturer has to keep a constant eye on the
process. And test of statistical hypotheses plays the role of
that eye.
---
First, we must understand that even when the bottling plant is working
satisfactorily, there is bound to be some inevitable random variation among the
amounts of the contents in the bottles. 
---
 The extent of this variation is typically a property of the machine
itself,  its precision, and may be expected to hold through out. 
But even with the precision in place, the setting may get shifted 
over time. And that's what we need to guard against. 
---
This calls for a <b>test of mean</b>. Again,
be careful here: this mean is the population mean. 

The procedure is this: We take some bottles from the production line, open
them and actually measure their contents.
---
 That's our data. We want toy know if the population
mean is shifted from its advertised value or not. Let's
understanding this: using some concrete numbers.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=3.5</i></legend>
Start with a population. Draw a sample. Draw another
sample. Shift population. Draw
another sample.  Compute means of all three samples. 
Point out that the difference between the first two is ignorable,
but not so for the third.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Back to theory</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.00</i></legend>
So we take our null hypothesis <b>$H_0: \mu = 50$</b> and the alternative
hypothesis <b>$H_1: \mu \neq 50.$</b>
 Be careful here, this $\mu $ is the population mean,
 the unknown quantity depending on the current possibly faulty condition of the
machine. 
---
Had it been just the sample mean, then we could just
have computed it and checked if it is equal to 50. 

Even here, we do need to compare the sample mean with 50, but since the sample mean is
just an approximation of the population mean, we should allow some
margin.
--- 
A sample mean just a little off from 50 should not be an
excuse to stop the production process and call for the
repairman. 
---
The question is how to choose the margin. And that's what we
shall discuss in the next video.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 2, Video 2: one-sample t-test">Module 2, Lesson 2, Video 2: one-sample t-test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.40</i></legend>
We are learning test of population mean. In the last video we
discussed a real life scenario where such a test is called
for. Let's quickly recall the set up, but this time in an
abstract way. 
---
Our set up consists of a population, an underlying distribution
with an unknown <b>mean $\mu.$</b> In our bottling plant example
this is the population of all bottles that the machine can churn
out in its current possibly faulty setting.
---
 We have an <b>advertised value $\mu_0$</b> for $\mu.$ We want to
test <b>$H_0: \mu =\mu_0$ versus $H_1: \mu \neq \mu_0.$</b>
---
 In certain other situations it may be known a priori that $\mu$ cannot fall
below $\mu_0.$ Then we can just test againt <b>$H_2: \mu &gt;
\mu_0.$</b>
---
 Similarly, other situations may call for testing against <b>$H_3: \mu &lt;
\mu_0.$</b>
 These are called respectively
<b>two-tailed, upper-tailed and lower-tailed</b> alternatives.
---
We have a random sample from the population <b>$X_1,...,X_n.$</b>
We compute <b>$\bar X$</b> and are about to compare this
with $\mu_0.$
---
 We intuitively feel that we should
reject $H_0$ in favour of $H_1$ not just if $\bar X$ is slightly
different from $\mu_0,$ but only if $\bar
X$ is too far away from $\mu_0.$.
---
 Similarly we should reject $H_0$ in favour of $H_2$
(or $H_3$) if $\bar X$ is much larger (or smaller)
than $\mu_0$. 
---
The question now is "how large is too large?" The answer, as you
might have guessed already, depends on the dispersion present in
the population.
---
 When you are measuring an agricultural plot a
shift of a few centimetres during measurement is nothing serious,
but during an eye operation using laser beams, a shift of even a
10th of a millimetre may be disastrous. 
---
Hmmm...dispersion present in the population. How can we know
that? The population is that underlying unknown truth. Well,
yes. But not everything has to be unknown about the
population.
---
 For instance, many machines come with a rating of its
inherent precision, and even though we suspect that the mean has
shifted, we may have reason to believe that the dispersion is
still at its advertised value.
---
 If that is the case, then we are in luck. Let's call this <b>known 
population standard deviation $\sigma.$</b> Then our test statistic is 
<b>$$
Z = \frac{(\bar X - \mu_0)}{\sigma/\sqrt{n}}.
$$</b>
---
Its null distribution is <b>$N(0,1)$</b> which involves no unknown quantity,
and may be used to compute critical values or p-values.
---
Don't bother too much about this formula or the null distribution. 
These are already built
into all standard statistical softwares. However, you should
quickly recognise the denominator as the standard error
of $\bar X.$
---
 Notice the letter $Z.$ Well, this test is
called the $Z$-test of mean.
</fieldset>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.20</i></legend>
So far we have discussed the case where we are in luck: the
population standard deviation is known. Our machine has possibly
slipped in the mean, but still holding its advertised
precision.
---
 In most situations, however, we are not this lucky. If
we suspect that our machine has drifted away from its advertised
mean, we are equally unsure about its current standard
deviation.
---
 Well, nothing to despair even then. If you do not
know <b>$\sigma,$</b> just estimate it!
 So our test-statistic now becomes 
<b>$$
t = \frac{(\bar X - \mu_0)}{\hat \sigma/\sqrt{n}},
$$</b>
---
Here <b><font color="red">
[[finger]] </font></b>
$\hat \sigma$ is the sample standard deviation
with $n-1$ in the denominator. The rest is basically same as
the test with known $\sigma.$ 
This is the <b>one sample $t$-test.</b>
---
Of course, for the sake of complteness, we should also mention the null
distribution. 
 For z-test it was <b>$N(0,1)$</b> and for
the $t$-test it is something called $t_{(n-1)}$<b><font color="red">
[[shown]] </font></b>.
---
Anyway, they are built into standard softwares. So
let's not torture ourselves with their not too apetising
mathematical forms. Instead it is time to see these things in action.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 2, Video 2: Lab">Module 2, Lesson 2, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Toy data. Full computation. Mention t-distribution. df.
<font color="red">
<pre>
set.seed(23261)
x = as.numeric(format(rnorm(6,mean=10),digit=2))
m=mean(x)
v=var(x)
n=length(x)
(m-11)*sqrt(n)/sqrt(v)
t.test(x,mu=11)
t.test(x,mu=11,alt="greater")
</pre>
</font>

</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
z test and one sample t-test.
</fieldset>

</div>



<fieldset>
<legend>M2L2V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Identify the test statistic for t-test.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;If we have a sample of size 50, then what is the d.f.?<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;An example, where summary statistics are given, and
t-distrn cut-off are given. Ask to accept/reject.<img src="../image/box.png"></p>

</pre>
</fieldset>


<fieldset>
<legend>M2L2V2</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Upper tailed.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Lower tailed.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Two tailed.<img src="../image/box.png"></p>

</pre>
</fieldset>



<h2><a
name="Module 2: Lesson 3: Paired t-test">Module 2: Lesson 3: Paired t-test</a></h2>

<b>Total lesson duration
= 22.9</b>
<p></p>

<div class="header">
<h3><a
name="Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=4.30</i></legend>
As we have already said, there are many different types of tests
of statistical hypotheses 
suited for different purposes. We have already talked about the
t-test, or one-sample t-test, to be more precise. 
---
It was used to compare mean of a population with some given
value. Like here is a sample, do you think that the population it
comes from has mean equal to 50? That was pretty useful as it
stood. But we can do more with the idea. 
---
In this lesson we shall talk about a related
test, which is basically an application of what we have learned in
the last lesson. As usual, we shall start with an example. 
---
Often measurements are done in pairs. Like you do something, and
want to see the effect. Then you make two measurements, one before
and one after that activity. 
---
You have a medication for <b>insomnia</b>, say. Then you randomly
select some <b>patients</b>, and measure their
<b>daily average amounts of  sleep</b>.
<b>before</b> the medication starts, and then
again for the same set of patients <b>after</b> the medication is over.
---
 Another scenario that leads to paired
observations is where you have two similar, but different aspects of
something that you want to compare.
---
 Like you want to compare
 <b>salaries</b> of husbands and wives. You randomly select some <b>households</b>
with earning couples.
and compare the 
<b>husband's salary with the  wife's salary</b> within the same household.
---
 Or may be you have two different ways to measure the amount of
<b>active ingredient</b> in a tablet. You want to compare them. So you
take a random asortment of <b>tablet</b>s, break each tablet into two
halves and apply the <b>two methods</b>, one to each half, and record
the measurements.
---
There are many other possible scenarios. But we shall continue to use
the insomnia medication scenario as our running example. 
Imagine that there is a random sample of such patients, and we have two
measurements for each, one before and one after the medication.<b><font color="red">
[[shown]] </font></b>
---
 We are interested in knowing if the medication had any effect. Our
null hypothesis is <b>$H_0: $ no good effect vs $H_1:$ some
good effect.</b>
---
 Clearly, we should look at the amount of increase in
the sleep. So we should subtract the before values from the after
values.
---
 We can express the hyptoeses mathematically like this. The
null hypthesis says:<b><font color="red">
[[move]] </font></b> on an
average the after values are the same as the before values.<b><font color="red">
[[shown]] </font></b> The
alternative says: the  after values are larger.<b><font color="red">
[[shown]] </font></b>
---
 Again, be careful here. The hypotheses are in terms of
the population means<b><font color="red">
[[finger]] </font></b>, these $\mu$'s are the
population means, and not the sample means. 

We have a population of patients.
---
 As each patient produces two measurements, one before
and one after, each patient is like a pair of numbers. So we have
a population of pairs.
---
 If in each pair we subtract the first from
the second, we get a population of differences. We want to to
know that if the mean of this population is zero or positive.
---
 In this case we are ruling out the possibility that the mean may be
negative, ie, the medication actually backfires, and reduces the
amount of sleep. 
---
We can now perform a t-test on the difference. This is called <b>paired
t-test</b>. 
---
Notice the adjective "paired". This is important. Here we have
two sets of measurements, the ones made before the medication and
the ones made afterwards.
---
 But these are not just like two
unordeeed sets. The first measurement in this set and the first
measurement of that set both correspond to the same patient. This
common aspect provides the pairing between them. 
---
Now it is time to take a look at paired t-test in practice.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6.5</i></legend>
Medication data. Peform the test.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.10</i></legend>
In the last video we saw paired t-test in action. Here we shall
look at it from a more abstract viewpoint, and also consider the
assumptions on the underlying distribution that are needed. 
---
We start with a data matrix with 2 continuous variables whose
values are paired. Let's pause for a moment and understand this
concept.
---
 Suppose we have two sets of agricultural plots, each set
has 5 plots.<b><font color="red">
[[rectangles shown]] </font></b> In the first set we have sown paddy and in the
second we have sown wheat.<b><font color="red">
[[shown]] </font></b>
---
 We measure the yields of all the
plots. Thus we have 5 paddy yields and 5 wheat yields. We
create a 5x2 data matrix out of them.<b><font color="red">
[[shown]] </font></b>
---
 Is this suitable for
paired t-test? No, because the two columns are not paired. You
might as well shuffle the values within a column without
losing any information. 
---
Now compare this with  a similar situation. A new chemical
spray has been been proposed that is supposed to enhance the
yield of paddy.
---
 To test its efficacy 5 plots are taken<b><font color="red">
[[shown]] </font></b>, 
and paddy is sown in them.
---
 Then each plot is divided into
two equal halves<b><font color="red">
[[shown]] </font></b>, and the spray is applied to only one half.

---
 The yields from the two halves are measured separately, resulting in 5
with-spray yields and 5 without-spray yields. Again we get a
5x2 data matrix.<b><font color="red">
[[shown]] </font></b> Is this data set suitable for a paired t-test?
---
The answer is Yes. The fact that the same plot is split into two
halves provide the pairing. The two halves in the same pair share
common soil type or irrigation.
---
Now let's come back to ... the mathematical set up.
</fieldset>

<fieldset><legend
    class="bu">
[n] <i>Duration=4</i></legend>
Coming back to the mathematical set up we have two continuous variables of
the same type (like both are measurements of the same quantity),
and their values are paired. We shall denote them as 
<pre>
x1 y1
...
xn yn
</pre>
There is a populaton mean for the xi's and a populaton mean for
the yi's. Call them $\mu_x$ and $\mu_y.$ We are trying
to test null hypothees like $\mu_y = \mu_x$ or $\mu_y \leq
\mu_x$ against alternatives like $\mu_y \neq \mu_x$
or $\mu_y &gt; \mu_x.$ 

The assumption that we make here is the $w_i=x_i-y_i$ values
have a normal distribution. This of course has
mean $\mu_x-\mu_y=\mu,$ say, and some unknown standard
deviation $\sigma.$ Then the null hypothees become sometime
like $H_0: \mu = 0$ or $H_0: \mu \leq 0$ with
alternative hypothees like $H_1: \mu \neq 0$ or $H_1: \mu&gt;
0.$ 

We use the test statistic
$$
t = \frac{\sqrt n \bar w}{sd(w)}.
$$
Its null distribution is $t$ with degrees of freedom $n-1.$
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Toy data. Full computation. Mention t-distribution. df.
<font color="red">
<pre>
set.seed(23263)
x = as.numeric(format(rnorm(6,mean=3),digit=2))
y = as.numeric(format(rnorm(6,mean=3),digit=2))
z = y - x
m=mean(z)
v=var(z)
n=length(x)
(m-0)*sqrt(n)/sqrt(v)
t.test(z,alt="greater")
</pre>
</font>


</fieldset>

</div>


<fieldset>
<legend>M2L3V1</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Identify the test statistic for paired t-test<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Find d.f. based on sample size.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;An example, where summary statistics are given, and
t-distrn cut-off are given. Ask to find p-value.<img src="../image/box.png"></p>

</pre>
</fieldset>




<fieldset>
<legend>M2L3V2</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Upper tailed.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Lower tailed.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Two tailed.<img src="../image/box.png"></p>

</pre>
</fieldset>



<h2><a
name="Module 2: Lesson 4: Two-sample t-test">Module 2: Lesson 4: Two-sample t-test</a></h2>

<b>Total lesson duration
= 21.2</b>
<p></p>


<div class="header">
<h3><a
name="Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.9</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.80</i></legend>
In the last lesson we learned about the paired sample
t-test. What we are going to learn in this lesson is deceptively
similar to that. It is called <b>two sample t-test</b>, or sometimes
called the <b>independent samples t-test</b>.
---
 This latter terminology is more suggestive, and clearly expresses its difference from a
paired sample t-test. Here also we have two sets of numbers, but
instead of being paired they are indepenedent. 
---
Let's spend some time appreciating the difference. While
discussing paired t-test we had mentioned an example where we
were assessing the effectiveness of a spray in increasing
agricultural yields. The set up there was like this.<b><font color="red">
[[shown]] </font></b>
---
 We had halved a number of plots, and applied the spray to only one half
and measured the yields of the halves separately. Now this is pretty cumbersome
to carry out in practice. So instead here is a different more
easily implementable set up.
---
 We start with a bunch of more or
less identical plots, say 9.<b><font color="red">
[[shown]] </font></b> Then we pick 4 plots randomly out
of them and apply the spray only to those.<b><font color="red">
[[shown]] </font></b> So we have 4
yields from these, and 5 from the others.<b><font color="red">
[[finger]] </font></b>
---
The resuting data look like this.<b><font color="red">
[[shown]] </font></b> 
So now we have to compare 4 numbers with 5 numbers. Clearly there is no pairing between
elements of these two sets. They are indepenedent. Even the sizes of these two sets are
different, one is 4, the other 5.
---
 Here we need a different type
of t-test, the <b>two-sample t-test or the independent samples
t-test.</b> 
---
The first step  is to compute the
average yield from both the groups, call them $\bar x$
and $\bar y$ and subtract one from the other.<b><font color="red">
[[shown]] </font></b>
---
 It is not enough to  check that it is more than or less than
zero.  We must remain
aware that our conclusion is to hold for the population means,
for which the sample means are just approximations.
---
 So we need to
have an idea about how good the approximations are, ie, the
variability of the yields within the two populations must be taken
into account. 
---
Here we face an unexpected problem. It may be the case that the
variability within the two populations are the same. Or it could be
that there is no such guarantee. Accordingly we have two different tests:
<b>homoscedastic</b> and <b>heteroscedastic</b>. 
---
Let's consider the cases one by one.
</fieldset>


<center>
<img src="image/jingle.png"><font color="blue" size="+3">Homoscedastic and heteroscedastic</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[n] <i>Duration=3</i></legend>
In the homoscedastic case,  the dispersion is the same in
both the groups. Here we use this test statistic.<b><font color="red">
[[shown]] </font></b>
---
 For the
heteroscedastic case the dispersion may be different
in the two  groups. There we use a this test statistic.
<b><font color="red">
[[shown]] </font></b>
---
 The formulae are somewhat complicated, but their null
distributions are simpler. They are 
always $t$-distributions with different degrees of
freedom.
---
 In the homoscedastic case it is $m+n-2,$<b><font color="red">
[[shown]] </font></b>
In the hetescedastic
case it is complicated. 
They are complicated,
but they have a pattern. In both the cases the numerator is the
difference of the two sample means, ad the denominator is an
estimator of its standard error.

So let's see how to perform two-sample $t$-test using LibreOffice.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Homo and hetero 2-sample t-test
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.30</i></legend>
We have seen how there are two versions of
2-sample $t$-tests based on <b>homo and heteroscedastic</b> set
ups. 
---
Both these tests seek to achieve the same basic aim,
comparing the means of two populations. Then why are there two
different tests? Well, because they work under two different set
ups. 
---
So the important question now is: given a real life problem, how
on earth are we to know which t-test we should use? The homoscedastic one
or the heteroscedastic one?
---
The answer depends on the <b>standard deviations of the two
populations</b>. If the two standard deviations are known to be equal, then we
are in the homoscedastic set up, otherwise (ie if the equality is not
known for sure) then we are in the heteroscedastic
set up.
---
 Now the population standard deviations are typically unknown. So there is no sure
way of knowing if we are in a homoscedastic set up or a
heteroscedastic one.
---
However, we may guess, or to use a more precise statistical term,
test the hypothesis of homoscedasticity against that of
heteroscedasticity. And there is indeed a standard test for that,
the <b>$F$-test</b>. We shall learn about this in this video.
---
First we need some new symbols. We are talking about equality of
two population variances (or equivalently equality of standard deviations).
---
Remember that here we have two populations, and one sample has
been drawn from each, independently of the other. So each
population has its own standard deviation.
---
 We call these <b>$\sigma_1$ and $\sigma_2.$</b> Then we are trying to
test <b>$H_0: \sigma_1 = \sigma_2$ vs $H_1: \sigma_1 \neq
\sigma_2.$</b> 
---
We have already mentioned in the first lesson of this module, how
the best test statistic may be obtained by restricting the
probability of type I error, and minimising the probability of
type 2 error subject to that.
---
 Well, if we follow that rather
mathematical process, we finally arrive at a test statistic, which
is quite intuitive. It is 
<b>$$
\frac{\sum_i (x_i-\bar x)^2/(m-1)}{\sum_j (y_j-\bar y)^2/(n-1)}.
$$</b>
---
Here $m$ is the size of the sample from the first
population, and $n$ is that for the second.

The numerator is just the sample variance for the first sample,
and the denominator is the sample variance  for the second
sample.
---
 If the two population variances are indeed the same, ie,
if $H_0$ holds, then this quantity should be close to 1. How
close? That answer is given by the null distribution.
---
 Again, that is a complicated beast, but fortunately people have already
computed it. It is called an <b>$F$-distribution with degrees
of freedom $m-1$ and $n-1.$</b>

Let's see it in practice in the next video.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
F-test.
</fieldset>

</div>

<fieldset>
<legend>M2L4V1</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Identify the test statistic for paired t-test<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Find d.f. based on sample size.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;An example, where summary statistics are given, and
t-distrn cut-off are given. Ask to find p-value.<img src="../image/box.png"></p>



<p>
<b>EXERCISE:</b>&nbsp;Husband wife data. Is 2-sample appropriate?<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Identify test statistic.<img src="../image/box.png"></p>

</pre>
</fieldset>




<fieldset>
<legend>M2L4V2</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Upper tailed.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Lower tailed.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Two tailed.<img src="../image/box.png"></p>

</pre>
</fieldset>



<h2><a
name="Module 2: Lesson 5: Review">Module 2: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 2, Lesson 5 , Video 1: Review">Module 2, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 2, Lesson 5 , Video 2: Review">Module 2, Lesson 5 , Video 2: Review</a></h3>
</div>




<h1><a
name="Module 3: Goodness of fit and independence">Module 3: Goodness of fit and independence</a></h1>


<h2><a
name="Module 3: Lesson 1: Contingency tables">Module 3: Lesson 1: Contingency tables</a></h2>


<div class="header">
<h3><a
name="Module 3, Lesson 1, Video 1: Contingency tables">Module 3, Lesson 1, Video 1: Contingency tables</a></h3>
</div>

<b>Total lesson duration
= 18.2</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 5.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.30</i></legend>
In the Basic Statistics course we talked about pivot tables, and
had mentioned how one special 
 type of pivot table
 has a much more important use than the rest.
---
 These are the <b>contingency table</b>s. We had only touched upon the subject
in that course. Now is the time for a fuller exposition.
---
Let's start with a brief recap.
A pivot table requires specification of two things:
 * One or more <b>categorical</b> variables in the data set.
 * Some <b>summary</b> measure.
---
To get a contingency table, we need to choose  two
categorical variables, and count as the summary measure. Let's
take an example.
---
Here is a table<b><font color="red">
[[shown]] </font></b> with two categorical variables gender
(Male/Female) and Handedness (Left/Right). The corresponding
contingency table is 2 by 2<b><font color="red">
[[shown]] </font></b>, where each cell has the
corresponding count. <b><font color="red">
[[Explain by pointing].] </font></b>
---
It should be quite easy to create such a contingency table in
LibreOffice, as we have already learned how to create a general
pivot table. Anyway, here is a little lab session, in case you
need one.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=4</i></legend>
[Screencast: Make contingency table.]
lr.csv: Don't use iq
</fieldset>

</div>



<div class="header">
<h3><a
name="Module 3, Lesson 1,  Video 2: Contingency table">Module 3, Lesson 1,  Video 2: Contingency table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.10</i></legend>
Contingency tables are very important objects in statistics. How
do we use them? The most important way is to  explore relation
between two categorical 
variables. We had a taste of this in the Basic Statistics
course. An example would help to clarify this.
---
We often want to answer questions like <b>"Is smoking related to
cancer?"</b>. Or what is the relation between <b>education level and
income group</b>? Does the chance of <b>admission to an educational
institute depend on one's gender</b>? 
---
All these questions are trying
to connect two categorical variables. In the first case smoking
habit (Y/N) with cancer (Y/N).
---
 In the second case, educational
level (preschool, school,  college and above)
and income level (low, middle, high). In the third case admission
status (admitted / rejected) and gender (male /
female). 
---
Contingency tables help us to understand such relations.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Joint distribution</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.00</i></legend>
We have learned about frequency distributions in our Basic
Statistics course, and seen how they lead to probability
distributions via statistical regularity. 
---
Well, contingency tables do the same thing too. They are also
frequency distribution tables of categorical variables, not of just one categorical
variable, but multiple ones simultaneously. 
---
Thus we do not merely
ask questions like "how many males are there in my sample?", or "how
many lefthanded persons?" We ask both the questions together, like
"how many lefthanded males are there in the sample?" We call
these  <b>joint frequency distributions</b>.
---
 Just as we could
construct bar charts based on frequency distributions, we can
create bar charts based on joint frequency distrbutions<b><font color="red">
[[shown]] </font></b>. Here
each bar is like a 3D pillar, one pillar per cell.
---
Suppose we make the height of each bar depict not the
 frequencies, but the <b>relative frequencies</b>, ie the cell frequncies
 divided by the total sample size.
---
 Then statistical
 regularity will kick in. As the sample size increases, the bar
 plot will converge to a fixed shape. This shape is called the
 <b>joint probability distributions</b> of categorical
variables.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 1,  Video 4: Contingency table">Module 3, Lesson 1,  Video 4: Contingency table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.00</i></legend>
We have mentioned how we may construct  a contingency table from
raw data. When we collect raw data with the aim of creating a
contingency table, there are three major ways to go about it. Let's
understand this with an example. 
---
Suppose that our population consists of employees from a
particular job sector. We are interested in two variables <b>gender
and responsibility</b> levels. Let's say there are four
responsibility levels: <b>Manager,
Engineer, Clerk and  Support staff</b>.
---
 Our final aim is to get a
contingency table<b><font color="red">
[[emptytab]] </font></b> like this, that shows genders in the rows
and the responsibilities in the columns. 
---
How should we go about collecting data for this? One method could
be to select 100 employees randomly from the entire population,
and observe the gender and responsibility levels for
each.
---
 In this scheme we have no control on how many men and how
many women will be in our sample. Neither do we have any control
on the counts for the responsibility levels. So both the row
marginals and the column marginals are random.
---
An alternative approach<b><font color="red">
[[gentab]] </font></b> is to draw a random sample of 50 men and
50 women, and then observe the responsibility levels for each of
them. Here there are 50 men and 50 women by design. So the row
marginals are fixed, not random. However,  the row marginals
continue to be random as before.
---
Of course, we could have done it the other way around, fixing the
column marginals first<b><font color="red">
[[restab]] </font></b>. Then we should have chosen in advance the number of
persons in each responsibility level, say 25 each, and randomly
chosen that many employees from each level.
---
 Finally, we observe
the genders of these selected employees. 

Why are we bothering about these different sampling schemes? Because the
different  schemes will let us make different types of
inferences. 
---
Suppose that we want to
know the proportion of male employees versus female
employees. Then the second scheme is not suitable, because there
we deliberately chose an equal number of men and women.
---
Similarly,
if we are interested in comparing the responsibility levels for
the two genders, <i>i.e.</i>, trying to check if the responsibility
level distribution for men...
---
 differ significantly from that for
women, then the third scheme is not suitable, as there we are
starting by fixing the responsibility level counts.
---
  The first
scheme may also not be suitable if there happen to be too few men (or two
few women) in the sample. Here the second scheme is the best
suited for the purpose.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 1,  Video 5: Contingency table">Module 3, Lesson 1,  Video 5: Contingency table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 1.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.60</i></legend>
Contingency tables tell us how multiple categorical variables
vary together. So far we have been considering the simplest
possible scenario, just two categorical variables.
---
 As the result our contingency tables were all 2-way or
2-dimensional. While these are indeed the most commonly used, it
is quite possible to have higher dimensional 
contingency tables, as well.
---
 A 3-way contingency table will not look like a flat rectangle
drawn on a piece of paper, but as a 3-dimensional block. Or like
a stack of rectangles, one rectangle per layer.
---
 Let's take an example. Suppose we want to study the relation
between 3 categorical variables, say, <b>gender</b>, <b>educational
level</b>
and <b>income group,</b>
---
 We consider  two levels
under gender <b><font color="red">
[[shown]] </font></b>, 4 under educational
level <b><font color="red">
[[shown]] </font></b>, and 3 under income group<b><font color="red">
[[shown]] </font></b>.
---
So we shall have a <b>3-way
contingency table</b>,which is <b>$2\times4\times 4$</b> in
size.
---
Here we commonly refer to the first variable as the
<b>layer</b>, the second as the <b>row</b>, and last as
 the <b>column</b>. It is as if a 3-way contingencytable consists of
 layers, each layer holding a 2-way contingency table.
---
Thus, gender is the layer variable, educational level is the
row variable, and income group is the column variable. Such
 tables are geberally printed  layer by layer, where each layer is
a 2-way table. 
---
Most of the concepts that we have learned about 2-dimensional contingency
tables readily generalise for higher dimensional contingency
tables, as well. For example, we may talk about association
between the variables.
---
However, a higher dimensional contingency table allows more
variety. For instance,  income group and educational level may
be associated in one way in the male layer, but  differently in
the female layer.  
---
We had some brush with such strange behaviours of
 multi-dimensional contingency tables in our Basic Statistics
 course already. 
---
In the next video we shall remind ourselves how to construct a 3-way contingency
table from raw data using LibreOffice.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 1,  Video 6: Contingency table">Module 3, Lesson 1,  Video 6: Contingency table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
[Screencast: 3-way simpson's paradox example.]
simp.csv: victim, defendant, death penalty
</fieldset>

</div>



<fieldset>
<legend>M4L1V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Fill in missing cells based on totals.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Make table by hand based on small data.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Give table, guess association<img src="../image/box.png"></p>

</pre>
</fieldset>



<fieldset>
<legend>M4L1V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Make a table from data, and ask for two cell
frquencies.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Make a table, guess association.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 3: Lesson 2: Independence test">Module 3: Lesson 2: Independence test</a></h2>

<b>Total lesson duration
= 30.3</b>
<p></p>

<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 6.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=6.10</i></legend>
In this video we shall discuss the most important application for
contingency tables: <b>testing independence between two categorical
variables.</b> 
---
Let's work through an example to build up the idea. We had
already seen this in the Basic Statistics course. Here we shall
develop the idea further. 
---
Let's start with an example. We want to see if there is any
association between <b>educational level</b> and <b>income group</b>.
---
 We have four educational
levels (<b>Preschool, School, College and Univ</b>) and 
three income groups (<b>High, Mid and Low</b>) and  we have a
population of people in mind,...
---
we draw a random sample from it, and
ask the people in the sample about their income groups and educational
levels. This will fill up a 3x4 contingency table.<b><font color="red">
[[empty table trans]] </font></b>
---
Notice that  I have not told you the entries in the cells. All
that I have given are the marginals. So you know the proportions of the
three income groups in the sample,...
---
 and also the proportions of
the four educational levels. Assuming that the sample is
representative enough, these are good approximations for the
population proportions.
---
Now suppose I pick a random person from the population, then
what is the chance that he/she belongs to the high income group? It
is<b><font color="red">
[[finger]] </font></b> this 60 by this 300, which is 1/5.
---
 Now suppose, just for the sake of argument,  I tell you that income group
is independent of educational level. What does that mean? That
means even if I whisper in your ear that the randomly selected
person has never been to school,...
---
 you'll have no reason to change your answer to the first
question. You'll still say that the 
probability of that person's being in the high income group is 1/5.
---
 This is true about all the
persons. In particular about<b><font color="red">
[[finger]] </font></b> all the 41 persons in this preschool
group.
---
 So out of these 41 persons each has 1/5 chance  of being
in the high income group. So the expected number of persons in
this cell is 41/5 = 8.2.<b><font color="red">
[[shown]] </font></b>
---
 Am I making sense? It's like I toss a coin with
1/5 chance of head. If I toss it 41 times, then the expected
number of heads is 1/5 of 41, ie, 8.2.
---
 Don't worry about it's not being a whole number, it is just the result of the numeric
computation. We mean the frequency in that cell should be near
about that number. Now notice how we arrived at this number.
---
 It is <b>row total times column total by grand total</b>. This holds for all the
cells. So we can compute the expected frequencies for all the
cells. <b><font color="red">
[[shown]] </font></b>
---
All these are under the assumption that income group and
educational level are independent. Now we also have some observed
frequencies in the cells.<b><font color="red">
[[shown]] </font></b>
---
 If these observed frequencies are more
or less like the expected frequencies, then we do not have reason
to suspect any association between the two variables. Otherwise,
this table itself is an evidence against the assumed
independence. 
---
The next question therefore is about how to compute the
difference between the expected and the observed
frequencies. Computing the difference between two numbers is easy,
you just subtract on from the other.
---
 But here we have two sets
of numbers. Some of them may be close, while some may be far
apart. We need some kind of a pooling across all the cells. Here
is our first attempt. 
---
<b>$$
\sum_{ij} (e_{ij} - o_{ij}).
$$</b>
where $e_{ij}$'s are the expected frequencies
and $o_{ij}$'s are the observed ones.
---
Unfortnately this will just give a zero, because this is just
subtracting the grand total from itself. The problem is that
positive differences are cancelling off negative differences.
---
 But
for us a difference is a difference, we do not care about the
sign. So we get rid of the sign by squaring:
<b>$$
\sum_{ij} (e_{ij} - o_{ij})^2.
$$</b>
---
That's better, but should all the cells be considered with equal
importance? If for a cell we have expected frequency 20000 and
observed frequency is just 10 more than that, then the squared
difference is 100.
---
 If for another
cell the expected frequency is 5 and observed is 10 more than that, then also
the squared difference is 100. Shouldn't these 100's be treated
differently?
---
 In the first case the 100 is where the expected
frequency was large, 20000, while in the second case the same
amount came from a smaller expected frequency, just 5.
---
 So, relatively, the second case is a more serious departure from
independence. So we update the formula to 
<b>$$
\sum_{ij} \frac{(e_{ij} - o_{ij})^2}{e_{ij}}.
$$</b>
---
We have already met this quantity in our Basic Statistics
course. It is called the <b>$\chi^2$-statistic</b>. Large values of
this makes us go against the null hypotheses of independence.
---
How large is large? We shall get into those questions. But first
a little lab session is in order. That's what we shall do in the
next video.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Hand computation (whorl)
<font color="red">
<pre>
obs = matrix(c(35,129,10,33),2)
rtot = c(45,162)
ctot = c(164,43)
gtot = sum(obs)
expc = matrix(0,2,2)
expc[1,1] = rtot[1]*ctot[1]/gtot
expc[1,2] = rtot[1]*ctot[2]/gtot
expc[2,1] = rtot[2]*ctot[1]/gtot
expc[2,2] = rtot[2]*ctot[2]/gtot
expc
(obs-expc)^2/expc
sum((obs-expc)^2/expc)
</pre>
</font>

</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Compute $\chi^2$ by raw computation and automatically.
</fieldset>

</div>



<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=5</i></legend>
Idea of null distribution of $\chi^2.$ Show a
population. Draw sample compute. Redraw and compute again. Show
difference. Show precomputed values in a different sheet. Make hist.
</fieldset>

</div>
[qn: null distrn is sampling distrn]

<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.20</i></legend>
In the last lab session we have got a taste of the null
distribution of the $\chi^2 $ statistic. The process we used
there was cumbersome. Statisticians have found a smarter method,
albeit an approximate one.
---
 They have seen that if the sample size
is large, and so are all the expected frequencies (say <b>sample
size at least 30 and each expected frequency at least 5</b>, as a
rule of thumb),...
---
 then the shape of the histogram looks like this<b><font color="red">
[[shown]] </font></b>. 
Of course this is just the general shape, there could be
variations. 
---
To be precise the general shape gives a family of distributions,
called the <b>$\chi^2 $ distributions</b>. Each member of the
family is identified by a number called the <b>degrees of
freedom</b>.
---
 The larger the degrees of freedom, the more spread out
the shape. For example this<b><font color="red">
[[shown]] </font></b> has higher degrees of freedom than
this.
---
Given a contingency table there is a simple way to work out the
degrees of freedom of the null distribution. It is just 
<b>(nrows -1) times (ncols - 1)</b>.
---
 When you compute the numbers of
rows and columns, don't include the totals. We are counting only
the numbers of categories of the two variables.
---
 For instance, in the educational level vs income group example<b><font color="red">
[[shown]] </font></b>, the degrees of
freedom are (4-1)(3-1) = 6.
---
What happens if you have just one row or only one column? Is the
degree of freedom 0 then? Well, you do not need to worry about
that case.
---
 A caegorical variable should have at least two
categories, because otherwise it is just a constant! So you'll
always have at least 2 rows and 2 columns.
---
Of course, as a practicing statistician in the modern age, you
really do not have to remember all these. All standard
statistical softwares have these things built in. 

The next video will demonstrate this in action.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=5</i></legend>
$\chi^2 $ test.
</fieldset>

</div>

<fieldset>
<legend>M4L2V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Give marginals. Ask to fill in the cells assuming
independence.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Give table. Ask to compute expected freqs for two
cells.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Give table ask to compute chisq.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Give set up. Ask d.f.<img src="../image/box.png"></p>

</pre>
</fieldset>



<h2><a
name="Module 3: Lesson 3: Permutation test">Module 3: Lesson 3: Permutation test</a></h2>

<b>Total lesson duration
= 23.3</b>
<p></p>

<div class="header">
<h3><a
name="Module 3, Lesson 3, Video 1: Permutation test">Module 3, Lesson 3, Video 1: Permutation test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.70</i></legend>
We have seen in the last video how the $\chi^2$ statistic
may be used for testing independence between two categorical
variables.
---
 We first compute the $\chi^2 $ statistic using a
special formula and then check if it is too large. If it is, then
we suspect that the two categorical variables are not
independent, else our verdict goes for independence.
---
 How do we
check for the value being too large? We employed
the $p$-value method. Now, $p$-value compares a given
number to a distribution, the null distribution in our case.
---
 That is, how the $\chi^2 $ statistic is expected to behave had
the two categorical variables really been independent. 

Thus there are two steps:
---
* First, <b>finding the $\chi^2 $ value</b>
* Second, <b>comparing it with the null distribution.</b>
---
How are we computing the null distribution? Well, we as
practising statisticians are not computing it ourselves, but
letting the computer do it for us using mathematics.
---
 Now most mathematical reasoning requires certain assumptions, and
<b>statistical regularity</b> is one of the major assumptions
here.
---
 Statistical regularity creates regular behaviour out
of randomness, when a large amount of randomness is piled
together carefully.
---
 So we need a large amount of randomness, ie,
a large sample. In particular for the $\chi^2$ distribution
to hold, each of
the expected frequencies must be large as well.
---
 How large? Well, the larger the better, but a commonly used  rule of thumb sets
the threshold as <b><font color="red">
[[ineqs shown]] </font></b>low as 5. If the total sample size is at least
30  and expected frequencies in all
the cells in the table are at least 5,...
---
then we may safely use the null distribution provided by the
computer, says the rule. But 
what happens if some expected frequency drops below 5? Well, the
first step is still meaningful. 
---
The $\chi^2 $ statistic still makes sense. 
It is sort of a distance between the observed
frequencies and the frequencies expected under independence.
---
 The only problem is that the null distribution prescribed for it by
the computer is not accurate any more.

In such a situation we use a different technique called
<b>permutation test</b>.
---
 The idea behind the test is quite intuitive,
though the final procedure is a bit too sophisticated for
LibreOffice. But still let me explain with an example. 
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">An example</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[n] <i>Duration=3</i></legend>
[Go fast]
Suppose we have 20 mother child pairs and we have classified the
fingerprints of the 20 mothers and their 20 children as either a
whorl pattern or a no whorl pattern. This results in a 2x2
contingency table. We want to test if the mother having a whorl
pattern is independent of her child's having a whorl pattern. For
this we think of the 20 mothers standing in a row. Now we shall
play god and distribute the children among them. 
Let's say mothers with whorls are more likely to
produce babies with whorls. Then god is more likely to choose a
whorl child for a whorl mother, and a non-whorl child for a
non-whorl mother. On the other hand if there is no association
between mothers' whorl and children's whorl, then the god just
assigns the children at random to the mothers. We shall do
precisely this, artificially. Assign the 20 children randomly to
the 20 mothers without caring about whorls. This produces a new
data set which identical to the original data set in terms of the
numbers of whorl and non-whorl mothers and numbers of whorl and
non-whorl children. But unlike in the original data, here we know
that there is no association between the mothers' patterns and
the children's patterns. This new data set may again be
summarised as a 2x2 contingency table, which has the same
marginal totals, but possibly different cell entries. If we now
compute the $\chi^2 $ value from this table, then we get a
taste of how the $\chi^2 $ should look like in case of
independence, ie. an idea about the null distribution. Well, just
a single value  won't give us a clear picture about the null
distribution. But then we can repeat this procedure a large
number of times, say 1000 times. Each time assigning the same 20
children randomly to the same 20 mothers (in possibly different
orders), making a 2x2 table and computing $\chi^2 $
values. In this way we get 1000 different $\chi^2 $ values
all under the assumption of no association. Now we may compute
the $p$-value of the $\chi^2 $ value from the original
table w.r.t. these 1000 numbers. 

Of course, we are making heavy use of the computer, repeating the
entire process 1000 times, but conceptually it is not that
demanding. The next video will show this in action. 
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Randomise using random permutation.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.5</i></legend>
We have seen one complicated method. Now we shall see a simpler but
crude method.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Merging.
<font color="red">
<pre>
set.seed(135317)
obs = matrix(c(46,34,5,2,47,78),2)
rtot = apply(obs,1,sum)
ctot = apply(obs,2,sum)
gtot = sum(obs)
(exp = outer(rtot,ctot)/gtot)
obs = matrix(c(51,36,47,78),2)
rtot = apply(obs,1,sum)
ctot = apply(obs,2,sum)
gtot = sum(obs)
(exp = outer(rtot,ctot)/gtot)
sum((exp-obs)^2/exp)

</pre>
</font>

</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 3, Video 2: Permutation test lab">Module 3, Lesson 3, Video 2: Permutation test lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Permutation test: do one set of randomisation. Then show
many $\chi^2 $ values in a different sheet. 
</fieldset>

</div>



<fieldset>
<legend>M4L3V1</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Identify formula.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Interpret.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Find one from another.<img src="../image/box.png"></p>

</pre>
</fieldset>



<fieldset>
<legend>M4L3V2</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Compute from data.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Compute from data.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 3: Lesson 4: Goodness of fit test">Module 3: Lesson 4: Goodness of fit test</a></h2>

<b>Total lesson duration
= 26.9</b>
<p></p>

<div class="header">
<h3><a
name="Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 6.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.60</i></legend>
In statistics and probability we often talk about an <b>unbiased
coin</b>, a coin that when tossed is equally likely to show either a
head or a tail.
---
 We use such coins to make impartial decisions, like which
team should bat first in a cricket tournament. Now here is a
coin<b><font color="red">
[[real]] </font></b>. I want to know if it is unbiased. How do I go
about it?
---
Well, no amount of staring at it will take me anywhere. I can
start by tossing it a large numbr of times, say <b>1000</b>
times. Let's say I get <b>523 heads and 477 tails</b>. May I conclude
from it that the coin is unbiased?
---
 Well, ideally I should have
obtained <b>500</b> heads and as many tails. But a coin toss is random
after all, so we can never guarantee an exact 50% even for an
unbiased coin.
---
 So close enough to 500 is what we should look for. So the
all important question now is: Are 523 and 477 close enough to
500?  
---
Note the similarity of this situation with the independence test
that learned in the last lesson. We have<b><font color="red">
[[finger]] </font></b> observed frequncies 523
and 477 and expected frequncies 500 and 500.
---
 And we are trying to
test if the observed frequncies are too far away from the
expected ones. Let's try ... the $\chi^2 $ statistic here.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
 We compute the differences between the  the observed and the
expected frequncies, square them divide by the expected
frequencies and sum. 

Clearly, if this too large then we should think that the coin is
biased. Well, we are doing a statistical test here. So let's
spell out the two hypotheses cleary. The null hypothesis
is $H_0:$ unbiased and the altrnative is $H_1:$ biased.

Now we need the null distribution. As before we do not really
need to know the name or form of it, as the software should have
that built in. But here there is a slight twist coming up
soon. In order to prepare for that let me tell you the name of
the null distribution. It is called
the $\chi^2 $-distribution. It is not a single distribution,
but a family of distributions, where each member of the family is
indexed by a parameter called the degrees of freedom. Here are
some examples.

Our null distribution is a member of this family. Which member?
That will be given by the degrees of freedom. There is a formula
for this. If you are checking for a fixed distribution as in this
case (unbiased coin means both the probabilities are given to
be $\frac 12$) then the degrees of freedom is one less than
the number of categories. In our case we had just two categories,
head and tail. So the degree of freedom is 1.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Die roll test. Any Hugli data categories?
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.80</i></legend>
In the last two videos we learned to perform a goodness-of-fit test
for some completely specified distribution, ie, a distribution
where the probabilities for all the categories were specified as
numbers.
---
 Sometimes we need to fit a distribution with unspecified parameters in
it. Then the procedure is similar but not entirely the same. 

Let's start with an example. 
---
You should have learned about the <b>Poisson distribution</b> in your
probability course, and have possibly also learned there that it
is used for modelling the probabilities of <b>rare events</b>. In case this
sounds new to you, here is a quick refresher. 
---
Consider this event:  a soldier
getting killed by a horse kick. Fortunately for the soldiers,
such events  occur pretty rarely. They usually get to die more
glorious deaths.
---
 Even if you count all such
unfortunate deaths in a band of soldiers for a whole year, the
number should be quite low, often just zero. This is an example
of a rare event.
---
 Now there is a belief supported by some
mathematical arguments that the probability of observing exactly $k$
such events  is ...
---
<b>$$
e^{-\lambda} \frac{\lambda^k}{k!}
$$
for $k=0,1,2,...$</b> This is called the Poisson distribution
with parameter $\lambda$, which is (population) average number of events.
---
We want to test if this belief holds true for our data
set. <b><font color="red">
[[Prussian horse kick data raw]] </font></b>
[Describe data]
---
We first summarise the data in the form of a frequency
distribution table.<b><font color="red">
[[shown]] </font></b>
---
 We need to know the probabilities, which
depend on the unknown $\lambda $. Of course, that is not a
problem, as we can estimate it by the sample average from the raw data. 
<b>$$
\hat \lambda = 0.7
$$</b>
---
 Then we have the probabilities or, rather, the estimated
probabilities<b><font color="red">
[[shown]] </font></b>. From these the expected frequencies are found
easily, by multiplying these with this total.
---
Now we compute
the $\chi^2 $ statistic as usual.

So far so good. Next we need the null distribution. It is a chisq
distribution. The df is 
<b>#cat - 1 - #est param</b>.
---
 Here we have
just 4 categories. So the degrees of freedom should be 4-1=3. But
no, here we have estimated one parameter, and so the degrees of
freedom will be one less than 3, ie, 2. 

The rest is as before. We shall see the computational details in the next video.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Prussian data lab.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.5</i></legend>
Did you notice one thing in the last video? The last category was
"3 or more". In the raw data we had 4 as the maximum number of
deaths. So why not have one category for 3 and another for 4?
---
Because then the expected frequncies become too small for the
chisq null distribution to hold. We shall
discuss this issue in this video with a different data.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Merging.
<font color="red">
<pre>
set.seed(135317)
x = rpois(50,lambda=1)
table(x) 
50*(1-sum(dpois(0:4,lam=1)))
# 0     1     2     3     4    5 
#19    22     4     2     2    1
#18.39 18.39  (9.20  3.07  0.77 0.18)=13.21
ob = c(19,22,9)
ex = c(18.39,18.39,13.21)
k=sum((ob-ex)^2/ex)
1-pchisq(k,df=2) #0.36
</pre>
</font>

</fieldset>

</div>

<fieldset>
<legend>M4L4V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Find expected frequencies under the assumption of
uniformity.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Find d.f<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Do chisq test by hand.<img src="../image/box.png"></p>

</pre>
</fieldset>


<fieldset>
<legend>M4L4V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Real data. Perform test of uniformity. Accept.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Real data. Perform test of uniformity. Reject.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Real data. Perform test of given probability.<img src="../image/box.png"></p>

</pre>
</fieldset>




<h2><a
name="Module 3: Lesson 5: Review">Module 3: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 3, Lesson 5 , Video 1: Review">Module 3, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 3, Lesson 5 , Video 2: Review">Module 3, Lesson 5 , Video 2: Review</a></h3>
</div>



<h1><a
name="Module 4: ANOVA">Module 4: ANOVA</a></h1>

<h2><a
name="Module 4: Lesson 1: ANOVA concept">Module 4: Lesson 1: ANOVA concept</a></h2>

<b>Total lesson duration
= 22.6</b>
<p></p>

<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.80</i></legend>
Suppose that you have entered a room, that you haven't been in
before<b><font color="red">
[[shown]] </font></b>. There  you find a lamp that is on. And two switches like
these, one turned on, the other off. 
---
Just by looking at these can
you conclude which switch controls the  lamp? Since only this
switch is on, and so is the lamp, it is natural to guess that it
is this switch that controls the lamp.
---
 OK, let's now try to turn
the lamp off. We flick the switch<b><font color="red">
[[shown]] </font></b>. Well, nothing happens. We now
try the other switch<b><font color="red">
[[shown]] </font></b>. Aha, now the lamp responds.
---
So what's the conclusion now? Which switch really controls the
lamp? This second switch of course!
---
This simple example contains an important maxim: when we want to
understand the relation between input and output it is more
reliable to link changes in the input with changes in the output
than value of the input to the value of the output. 
---
The switch whose state (on/off) matched the state of the lamp
need not be the one controlling it. The switch whose change of
state corresponded to the lamp's change of state is the actual
controller. 
---
There are plenty of real life examples demonstrating this maxim. I
am ill, I take a drug and get cured in a week<b><font color="red">
[[shown]] </font></b>. Does that prove
that the drug is effective?
---
 Not until I compare this with a
similar situation where the drug is not taken.<b><font color="red">
[[shown]] </font></b> May be then also I
would have been cured in a week<b><font color="red">
[[shown]] </font></b>. 
---
In that case, the input changed from drug to no drug, but the
output remained the same. So the drug is not effective.  That's why it is
important to see how the output changes when the input changes. 
---
This basic principle sits at the heart of what we shall discuss
in this module. In many branches of science as well as in
everyday life we often have to understand the input-output
relation of a system that we treat as a blackbox
unit.<b><font color="red">
[[shown]] </font></b>
---
 If the unit is a blood pressure patient<b><font color="red">
[[shown]] </font></b>, then the input could be a drug
that may be administered in different doses and output could be
the  blood pressure measured after medication.
---
If the unit is a spring<b><font color="red">
[[shown]] </font></b>, then the input could be the load hung
from it, and the output would be the length of the spring.
---
 There could be multiple inputs as well. In a typical agricultural
experiment<b><font color="red">
[[shown]] </font></b>, the unit is a plot,  the inputs could be the variety of crop, the
fertiliser used,  and
the output would be the yield.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">What are we trying to find?</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.60</i></legend>
The main aim here is not really to assess the input-output
relation but to check which inputs influence the output and which
do not. 
---
This is like a preliminary step before embarking upon a
formal analysis to estimate the relation.
---
 And it is here that our
maxim plays an important role. Change the inputs one at a time
and see if the output also changes accordingly.

The next video will illustrate this with a concrete example.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.30</i></legend>
The example that I am going to discuss here is painfully
real. Indeed so much so, that I have to hide behind symbolic
notation lest I hurt anybody's personal feeling. 
---
In a certain country the people are of two religions. Call them
<b>Religion 1 and Religion 2</b>. A commonly held belief in that country
is that Religion 2 is <b>against education</b>. 
---
And even a cursory glance at the populace seems to provide ample evidence in favour
of this belief. If you take a random sample of people following
Religion 1, and another random sample of people following
Religion 2,...
---
 then indeed the educational achievements of the
latter group would be far below that for the first group. Since
education is a good thing, and people following Religion 2 have
less education,...
---
 so it is claimed that Religion 2 is bad, and
keeps its followers backward. But is this conclusion indeed justified?
---
Well, here we can visualise this as a blackbox system.<b><font color="red">
[[shown]] </font></b> Religion
is the input and education is output.
---
 We see that a change in
Religion (ie, comparison between people of different religions)
leads to change in education. So apparently the conclusion seems
justified.
---
Now in that country Religion 1 used to be only religion, and there used to be
much oppression against the poor class. This class sought to
build a separate identity and fight back, and that's how Religion
2 gained currency.
---
 Even now  Religion 2 is embraced chiefly by
the poorer people of the country. Let's take a second look at our
blackbox in light of this new information. 
---
So now we have  another input into our blackbox, <b>income</b>. 
This opens up a whole new interpretation. Is it really
religion that determines the educational level or is it the
income?
--- 
It is quite possible that poorer people get
less education, and since followers of Religion 2 are mostly
poor, we see a greater number of less educated persons among
them.
---
 A conclusion can be drawn by changing only a single factor
at a time, ie by comparing the educational levels of the rich and poor within the same
religion, and comparing religions among people of similar
financial status.
---
 Our earlier conclusion was that most the
variation in the education was due to diference in
religions. Let's show variation with a double arrow,<b><font color="red">
[[shown]] </font></b> the
thicker the arrow the more the variation.
---
 The religion arrow was thicker meaning it accounted for  the lion's share of the output
variation, while this arrow is much thinner. 
---
In the new interpretation<b><font color="red">
[[shown]] </font></b> this arrow becomes big and this
becomes smaller. 
---
This idea of accounting for different part of the variability in
the output by variation in the inputs is called ANalysis Of
VAriance or ANOVA. Because we are splitting or analysing the
variation and assigning the parts to different heads.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.40</i></legend>
When we draw a blackbox like this<b><font color="red">
[[shown]] </font></b> it looks like a machine that
takes this input and produces that output. Now the output of a machine is
supposed to be completely determined by its inputs.
---
 However, consider this example, where the unit is a plot, input is crop
variety and  output is yield.<b><font color="red">
[[shown]] </font></b> Even if I take two
identical plots, sow the same variety,
is it guaranteed that the yield will be the same?
---
 No, there are
bound to be many imponderable factors that will make the two
yields slightly different. In order to show this schematically in
our box diagram we add this input<b><font color="red">
[[shown]] </font></b> and call it the random
error.
---
 Now when we ascribe parts of the output variation to
different inputs, the part that we could not explain using the
other inputs, all those imponderable effects are put under this
head.
---
To appreciate let's consider comparing the yields of three
different varieties of the same crop. There are say 5 fields under each variety, so 15 fields in
all. We measure the yield of each plot, and that's our output.<b><font color="red">
[[shown]] </font></b> 
---
Consider all the output values in a number line.<b><font color="red">
[[shown]] </font></b> So we have 15
points. We have shown the varieties using different
colours. There are 5 points of each colour.
---
 Here the points of
the same colour are all tightly together with ample gap between
the points of different colours. What should our conclusion be
here? So the varieties are really different in terms of yield.
---
 Why are the points of the same colour are not exactly together?
That is because of random error. Thus, random error has its
contribution no doubt, but the effect of the varieties is far
stronger.
---
 We denote this using arrows like this. 
A fat arrow for the variety input, and a thin one for the random
error.<b><font color="red">
[[shown]] </font></b>
---
Compare this with this situation.<b><font color="red">
[[shown]] </font></b> Now the points of
different colours are all mixed together. Shall we say here that
the varieties really have much effect on the yield?
---
 No, here chance plays the dominating role. So now the random error gets
the fatter arrow<b><font color="red">
[[shown]] </font></b>, and to maintain the same total the variety
input gets the slender arrow.
---
This simple example introduces possibly the most important
concept in the whole of ANOVA: using the variation due to the random
error input as the yard stick.
---
 We say that an input has significant effect on the output if and
only if its effect is appreciably larger than what we anyway expect from mere
chance. 
---
Thus ANOVA is not only just about ascribing variablities
in the output to the different inputs (including random error),
but also expressing the contributions of all the inputs in terms
of that due to random error. 
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.30</i></legend>
We have already presented the two important pillars of
ANOVA. Let's start by reminding ourselves.<b><font color="red">
[[box shown]] </font></b>

* One, splitting the variation<b><font color="red">
[[shown]] </font></b> in the output into components
  ascribable to the various inputs including the random error.
* Two, using the random component of the variation as the yard
  stick of how much variability we must endure, and measuring the
  components relative to that.

Traditionally this idea is presented in the form of a table
called the <b>ANOVA table</b>. These tables have many intricate details
that need not concern us here. But it is important to know the
basic structure.<b><font color="red">
[[shown]] </font></b> We have shown only the most
important columns: source, this SS stands for Sum of Squares,
then F and finally p.

 The table has one row for each of the arrows (including
the random error and output). The total row stands for the
output. 
 The source column tells us which of the other rows are
for which inputs<b><font color="red">
[[anim shown]] </font></b>.
 In our schematic box diagram, these double arrows denote
variabilities, fatter means higher variability, thinner means
lower variability. These are quantified as something called the
Sums of Squares, or SS  for short. These populate the second
column in our table<b><font color="red">
[[anim shown]] </font></b>. This total sum of squares is the
total variablity in the output, which is being decomposed into
sum of squares due to the different sources. After this come a few more
columns which are of technical nature and hence would be skipped
here. Finally we want to express the different sums of squares
relative to this error sum of squares. In other words, we divide
these by this. Again, there are some technicalities that I am
skipping here. But the F column does basically this. Now we check
if any of these numbers is too large. If any of them is, then the
corresponding source has a significant effect on the output.  

As you can see, I am glossing over certain details, partly
because the math is a bit involved, and mostly because they play
no role in the interpretation of the result. And interpretation
of the table is what we, as practising statisticians, are
primarily interested in. Still let me give you a taste of the
mathematics in a particularly simple case.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">A simple mathematical analysis</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[n] <i>Duration=4</i></legend>
Consider our number line example once again. We had three
varieties of the same crop and measured the yields from 15 plots,
5 for each variety.<b><font color="red">
[[shown]] </font></b> 
---
The variability among the points of the same colour indicate the
contribution of the random error. The variability among point
clusters of the different colours is due to the contribution of
the varieties.
---
 To express these variabilities mathematically, let
us introduce a notation system. We shall denote the yields by the
letter $y.$<b><font color="red">
[[shown]] </font></b> Also we shall attach a numbering scheme to the
15 plots.
---
 Each plot will be indexed by a pair $(i,j),$<b><font color="red">
[[shown]] </font></b>
where $i$ denotes the <b>variety</b> sown in it ($i=1,2,3$)
and $j$ is like a <b>roll number</b> that runs from 1 to 5 within
the same variety.
---
 Then the yield from the $(i,j)$-th plot
will be denoted by $y_{ij}.$ 
---
Thus the dots of the same colour correspond to $y_{ij}$'s
with the same $i$ and different $j$'s. Let's mark the
centres of each colour cluster.<b><font color="red">
[[shown]] </font></b>
---
The centre of the $i$-th cluster is <b><font color="red">
[[finger]] </font></b> $\bar
y_{i\bullet}.$ Note the dot in place of the $j$ index,
over which which we have averaged.
---
The variability
in the first cluster therefore may be quantified as 
<b>$\sum_j(y_{1j}-\bar y_{1\bullet})^2.$</b>
---
 Similarly for the other
colours. So the total contribution of chance is the
sum <b>$\sum_i\sum_j (y_{ij}-\bar y_{i\bullet})^2.$ </b>
---
On the other hand, when we want to measure the contribution due to
the varieties, we consider dots of each color as a single class
and measure the variability between the classes.
---
 For this imagine all the points in the same class to be concentrated at the center
of the class.<b><font color="red">
[[finger]] </font></b> Let the overall mean be $\bar
y_{\bullet\bullet}.$<b><font color="red">
[[shown]] </font></b>
---
 Then the variability between the classes
corresponds to <b>$\sum 5 (\bar y_{i\bullet}-\bar
y_{\bullet\bullet})^2.$ </b>
---
Finally, the total variability present in the output, ie.,
all $y_{ij}$'s is <b>$\sum_i \sum_j (y_{ij}-\bar y_{i\bullet})^2.$ </b>
---
It comes as a pleasant surprise that we have the following
algebraic identity:
<b>$$
\sum \sum (y_{ij}-\bar y_{i\bullet})^2 = \sum J (\bar y_{i\bullet}-\bar
y_{\bullet\bullet})^2 + \sum_i\sum_j (y_{ij}-\bar y_{i\bullet})^2.
$$</b>
---
We shall prove this in the next video, and is exactly what we expected
intuitively. It is one of those moments where intuition is
borne out by mathematics exactly. 
---
When we write an ANOVA table we write precisely these quantities:
<b>
<pre>
------------
Source    SS
------------
Variety  BSS
Error    ESS
------------
Total    TSS
------------
</pre>
</b>

</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Proof of the 1-factor ANOVA identity.
</fieldset>

</div>


<fieldset>
<legend>M3L1V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Describe set up. Draw box diagram with unlabelled input and
output. Ask to label.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Ask to choose from the proper layout of data.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Two 1-way ANOVA numberline examples. One significant, other
not. Ask to identify.<img src="../image/box.png"></p>

</pre>
</fieldset>



<fieldset>
<legend>M3L1V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Describe set up. Write different models. Choose the correct one.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 4: Lesson 2: ANOVA terms and data layout">Module 4: Lesson 2: ANOVA terms and data layout</a></h2>

<b>Total lesson duration
= 22.5</b>
<p></p>


<div class="header">
<h3><a
name="Module 4, Lesson 2, Video 1: ANOVA terms">Module 4, Lesson 2, Video 1: ANOVA terms</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 7.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.80</i></legend>
When dealing with an ANOVA problem it is important to think about
the blackbox diagram<b><font color="red">
[[shown]] </font></b>. There must be<b><font color="red">
[[finger]] </font></b>
at least one input apart from 
the inevitable random error input, and exactly one output.
---
 The traditional theory of ANOVA requires the output to be <b>continuous,</b>
and the random error input to be continuous as well. The other
inputs may be categorical or continuous. 
---
If all the <b>non-random inputs are categorical we call it an ANOVA</b>
set up. If all the non-random inputs are <b>continuous we generally
call it a regression set up</b>. If there is at least one categorical
and one continuous non-random input, then it is an ANCOVA set
up.<b><font color="red">
[[shown]] </font></b> Here ANCOVA means <b>ANalysis of COVAriance</b>.
---
The categorical inputs are called <b>factors</b> and the continuous
inputs are called <b>covariates</b> in ANOVA parlance. 
If there are exactly $k$ categorical inputs and no
continuous input, then we
have a $k$-factor ANOVA.
---
 If we also have at least one
continuous input, then it is $k$-factor ANCOVA. The number
of covariates does not feature in the nomenclature.
---
 This
terminology however, is not entirely standard. Some people use
the term $k$-way ANOVA only when each factor combination
occurs exactly once in the data set. This is what Libreoffice
uses as well. It will be clear when we see examples in the lab.
---
In this module we shall focus on only 1-factor and 2-factor
ANOVA. 

Next let us understand the nature of data that we need in order
to carry out ANOVA.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Data</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=2.30</i></legend>
The main aim in ANOVA is to explore a blackbox system to see
which inputs have any appreciable effect on the output. For this
it is important to keep two  basic principle in mind: 

* First, <b>change each input</b>

* Second, <b>change them one at a time. </b>

Let's take a closer look at the first<b><font color="red">
[[shown]] </font></b>. If you are interested in
assessing if the drug is effective, it is not enough to apply the
drug to some patients and see its effect. You must also not give
the drug to some other similar patients, and see the difference
between the patients who got the drug and those who
didn't. This gives rise to the wellknown concept of <b>placebo</b> in
clinical studies, where a dummy drug is given to one group and
the true drug is given to the another group. The dummy drug has
no medical value, it is just to keep the patients happy so that
no psychological effect creeps in. The group of patients who did
not get the real drug is usually called the <b>control group</b>. 
The other group that gets the real drug is the <b>treatment
group</b>.

Next we come to the second point: changing the inputs one at a time.
If you always change multiple inputs simulateneously it
will be impossible to figure out which input caused the output to
change. It's like I have a fever. My wife suggests visiting an
alopathc doctor while a freind suggests seeking homeopathic
medication. Unwilling to displease either, I take both types of
medicine, and get cured. Now which doctor should get the credit?
Or may be the medicines reacted together to form a new chemical
which actually cured me! Such a situation where two or more inputs have
changed together so that the change in output cannot be clearly
ascribed to either is called <b>confounding</b>. And this should better
be avoided. 
</fieldset>

<fieldset>
<legend>
(s3)[n] <i>Duration=2.90</i></legend>
To avoid confounding we have to collect our data carefully. Let's
say we have a medicine whose effectiveness we are trying to test,
and we suspect that smoking habit might influence the
effectiveness. So we have two inputs drug and smoking habit. We
know that each input should be changed. Accordingly I have two
possible values for the drug input: real drug and placebo, and
two values for the smoking habit input: smoker and non-smoker. 
So I have a 2x2 layout. It is important that I have
representatives from all the 4 cells. That is I must give the
real drug to some smokers, placebo to some other smokers, the
real drug to some non-smokers and also the placebo to some
non-smokers. Is the drug effective? To answer this we can then
compare this group with this, and that group with that. But if we
just confine our study to these cells, ie, give the real drug to
only smokers and placebo to only non-smokers then drug effect and
smoking effect will be confounded. This way of laying out the
parients in a rectangular array allowing for all possible
combination is called a factorial design, and the cells are often
called blocks. 

The same principle of changing only one input at a time gives
rise to another concept called replication. Remember that random
error is also one input, that we have no control on. So it is
always changing. In order to let it change alone we just have to hold all
the other inputs fixed for some time. Let's understand this with
the drug example. We had the 2x2 layout. In each cell we must
have at least two patients (though more patients per cell would
be better). When we compare the outputs for the patients in the
same cell their variation must be solely due to the random error
input, and so we can form an idea about the inevitable amount of variability
that we may expect due to chance alone. As we know this amount is
very important, as this is going to be yard stick w.r.t which all
the other variabilities are to be measured.

If the units are human beings, then we often assign them to
different groups blindly, ie, without telling them their true
groups. If there are two drugs, they are given identical external
appearance. Sometimes even the experimenter is not allowed to
know the true groups until the data collection and analysis is
finished. This is called double blind. 
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 2, Video 1: ANOVA terms">Module 4, Lesson 2, Video 1: ANOVA terms</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0</i></legend>
In order to perform ANOVA we need our data to be laid out in a
particular format. While most softwares use a standardised form,
LibreOffice and MS-Excel use a slightly simpler format. Let's
understand the standard format first.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Standard format</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=4.5</i></legend>
When collecting data it is good to think in terms of the balckbox
diagram. Each input arrow (excluding the random error) and output
arrow corresponds to a variable, ie, a column in the data
matrix. Each time you measure the values for the inputs
(excluding the random error, of coure) and the output, you get a
case. 

Let's consider a concrete example. Each unit in the blackbox is
an insomnia patient. The inputs are drug, age group and
gender. The amount is the increase in amount of sleep after
medication. Let's say drug has three levels Drug 1, Drug 2 and
Placebo. There are three age groups, Young, Midle, Old. Two
genders Male and Female. So imagine that we approach each patient
with a blank form where the variable names are fields. We fill in
the blanks approapriately for each patient. Collating all the
filled in forms we get a data matrix like this. This is the data
layout required by most standard statistical softwares like R, SAS,
SPSS, Systat, Stata. 
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.60</i></legend>
In the last video we learned about the data required by most
standard statistical software for ANOVA. However, LibreOffice expects a
different layout, which is somewhat more intuitive.
---
 First let me
admit that LibreOffice cannot handle ANOVA in its full
generality. It can handle only 1-factor and 2-factor ANOVA.
---
 The simple data layout that LibreOffice expects is possible only for
these simple cases. So let's simplify our example to retain only
the Drug input. Then we have a 1-factor ANOVA set up. <b><font color="red">
[[box
diag shown]] </font></b>
---
Here we have just a single input (except the random error
input), and a single output. Now LibreOffice expects the output
values for the different values of the input in different
columns. Let's see this in LibreOffice.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[n] <i>Duration=5</i></legend>
One-way LibreOffice layout.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5.1</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=2</i></legend>
Explain 2-factor layout.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[n] <i>Duration=3</i></legend>
Two-way in Libre Office
</fieldset>

</div>

<fieldset>
<legend>M3L2V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Describe set up. Ask to classify. One-way.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Describe set up. Ask to classify. One-way, ANCOVA<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Describe set up. Ask to classify. two-way.<img src="../image/box.png"></p>

</pre>
</fieldset>




<h2><a
name="Module 4: Lesson 3: ANOVA table">Module 4: Lesson 3: ANOVA table</a></h2>

<b>Total lesson duration
= 36.8</b>
<p></p>

<div class="header">
<h3><a
name="Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 1.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.50</i></legend>
We have aleady seen <b>ANOVA table</b>s. They sit at the heart of ANOVA
and is the traditional way to present the result of ANOVA. Let us
take an example. We are comparing <b>three varieties</b> of paddy and <b>two
fertilisers</b> in terms of the yield.
---
So we have 6 plots<b><font color="red">
[[shown]] </font></b>, these <b><font color="red">
[[finger]] </font></b> are under fertiliser 1,
these under 2. Similarly, these are under variety 1, these under
variety 2, these under 3.
---
To understand ANOVA tables it is important to visualise the set up
as a blackbox diagram<b><font color="red">
[[shown]] </font></b>.
---
 In the basic version of the ANOVA table there are as many rows as the
number of input arrows (including the random error).<b><font color="red">
[[shown]] </font></b>  Plus there
is one total row for the output arrow.<b><font color="red">
[[shown]] </font></b>
---
 The very first column is called the <b>Source</b> column and
describes each input arrow. The last 
non-total<b><font color="red">
[[finger]] </font></b> row is always reserved for the random
error.
---
The next column is for the variabilities called <b>sum of squares</b>. To understand this we need
to go back to our basic identity. And that's what we shall do in
the next video.
</fieldset>


</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0</i></legend>
Now we shall learn about the ANOVA table. Many text books present
it as a crowd of formulae, which often look pretty
scary. Actually the 1-way ANOVA table is just a fancy way of
expressing...the basic identity. 
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Basic identity BSS, WSS. Their positions in ANOVA table. df. The
other columns. Examplain F test.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Fake data, one way.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[n] <i>Duration=6</i></legend>
[Convert to BC with intro.]
Start with 1-way set up, cast it as model. Explain a 2-way set
up. Mention additive. Refer to non-additive to come later.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 6.3</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6.3</i></legend>
2-way ANOVA table. Brief explanation.
[aov2.png]
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=5</i></legend>
2-way ANOVA with LibreOffice.
</fieldset>

</div>

<div class="header">
<h3><a
name="Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Real data
</fieldset>

</div>


<fieldset>
<legend>M3L2V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Real data: one way estimation.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real data: two way estimation.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real data: ANCOVA estimation.<img src="../image/box.png"></p>

</pre>
</fieldset>


<fieldset>
<legend>M3L3V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Show table. Ask to accept/reject.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show incomplete table. Ask to complete MS column based on
given SS and d.f. columns.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Describe set up. Ask to provide d.f.<img src="../image/box.png"></p>

</pre>
</fieldset>



<h2><a
name="Module 4: Lesson 4: Interaction">Module 4: Lesson 4: Interaction</a></h2>

<b>Total lesson duration
= 23.3</b>
<p></p>

<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.90</i></legend>
We had started our discussion of ANOVA with a story, the story of
a room with a lamp and two switches. The actual circuitry that
connected the inputs to the output, ie, the switches to the lamp
was unknown,...
---
 a blackbox, and we had to try out various values for
the inputs and observe the output values for them in order to get
an idea of the inner working of the blackbox. 
---
Well, we are back to a similar room<b><font color="red">
[[shown]] </font></b>, again two switches and a
lamp. But this time everything is turned off. Our aim as before
is to figure out the input output relation.
---
 Let's start by
flicking this switch<b><font color="red">
[[shown]] </font></b>. Well, nothing happens. Looks like this is
not out switch. OK, let's return
it to its original state.<b><font color="red">
[[shown]] </font></b> Now we flick the other
one.<b><font color="red">
[[shown]] </font></b>
---
 Oops, this is also seems to be ineffective. Or may be the lamp is fused? In
dismay we play with the first switch again, turn it on<b><font color="red">
[[shown]] </font></b>. Wow, the
lamp turns on. But didn't we turn this switch on just now also,
but did not get any effect?
---
 How come it has suddenly come back to life?
May be this switch is alive too now. Let's turn it off<b><font color="red">
[[shown]] </font></b>. Wow, this
also seems to control the lamp. Earlier neither seemed to have
any control, and now mysteriously both are controlling the
lamp.
---
 So I can control the lamp by the top switch <b><font color="red">
[[flick],] </font></b> 
oops it has gone to sleep once again. Let's try the
other one. <b><font color="red">
[[flick]] </font></b> Boy, this is dead again too! This room sure is
spooky. Or is it?
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">The mystery</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.70</i></legend>
<b><font color="red">
[[shown]] </font></b>Well, there is nothing really spooky about the switches. They are
basically connected in series like this<b><font color="red">
[[shown]] </font></b>.
---
 So when any one of them is
off, the circuit is already broken, and  the other switch has no
control. But if one switch is turned on, then it fate of the lamp
is controlled by the other switch. 
---
Now we are not here to discuss electric circuits really. My point
here is that it is possible to have situations more complex than
what we had started with in our first example, where each switch
was either controlling the lamp or not.
---
 Each behaved independently of the other. So if we asked "Does
this switch control the lamp?" the answer was eiter a clear yes,
or a clear no. 
---
But here the effect of one switch
in influenced by the other switch. So if now ask "Does this
switch control the lamp?", then we do not have a simple yes
or no anwer to the question.
---
  The answer depends on the curent state of the other switch. If that switch is
off, then the answer is no, if that switch is on, then the answer
is yes. 

In such a situation where two inputs are kind of entangled
together, we say that there is interaction between the two
inputs.
---
Let's look at it in the context of statistics.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Interaction</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.40</i></legend>
In general think of our familiar blackbox diagram. We say
two inputs have <b>interaction</b> if the effect of one depends on the
the value of the other.
---
It does not always have to be just two inputs. It is quite possible for even three or
more inputs to be involved in an interaction. Their combined effect
is called the <b>interaction effect</b>
---
. Had there been no interaction,
then their individual effects are called <b>main effects</b>.
Remember: main effect makes sense only when interaction is absent.
---
In any ANOVA problem with at least two inputs (other than the
random error input), we should worry about their
interaction.
---
 Before we can meaningfully talk about the effect of
any single input, we need to ascertain that it is not involved in
an interaction with the some other input. It is possible to test
this using a statistical test of hypotheses. 
---
However, the math is a little involved. Also Libreoffice does not
allow us to compute the interaction. So we shall restrict
ourselves to a pictorial method of assessing presence or absence
of interaction.

</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.70</i></legend>
In the last video we saw some expository examples of
interaction. Here we shall see how interaction occurs in real
life.
---
 In any example of interaction we must have at least two
inputs (excluding random error). Our first example will be where
the two inputs linked by interaction are both categorical. 
---
Consider an agricultural experiment where we are interested in
assessing the effect on yield of <b>two varieties</b> of a crop and <b>three
different fertilisers</b>. Here is the ...
</fieldset>

<fieldset><legend
    class="bu">
[n] <i>Duration=5</i></legend>
 ... blackbox diagram<b><font color="red">
[[shown]] </font></b>. Two inputs
variety and fertiliser (plus of course the inevtiable random
error). The output is yield.
---
 Typically such an experiment will
need as 2x3 layout like this. In each of the 6 cells we shall
take at least two plots.<b><font color="red">
[[labelled grid shown]] </font></b>
---
 Let's take exactly 2 plots in each. By the way,
this is called a <b>balanced layout</b>, since we take the same number
of plots in each cell. We shall index the plots
as <b>$(i,j,k)$</b>,...
---
 where $(i,j)$ refers to the cell
ie, $i$-th variety and $j$-th fertiliser, and $k$
is serial number (which is either 1 or 2) within the cell. The yield
from the $(i,j,k)$-th plot will be called <b>$y_{ijk}.$ </b>
---
To appreciate interaction in this context let's compute the
average yield for each of the 6 cells:
<b>$$
\bar y_{ij\bullet} = \frac{y_{ij1}+y_{ij2}}{2}.
$$</b>
---
Now consider averages in the first row, ie for variety 1. We
plot them<b><font color="red">
[[shown]] </font></b> as three three points, and join them with
lines.
---
 Since we are working with categorical variables in the
horizontal axis, joining them with lines is not entirely
justified, but still it is a common practice  to aid visual
interpretation.
---
  The shape that we get like this will be called
the<b> profile</b> for variety 1. Next we shall draw the profile for
variety 2, on the same plot<b><font color="red">
[[shown]] </font></b>.
---
 The profiles turn out to be more or less parallel. This allows
us to meaningfully ask questions about 
the varieties and fertilisers separately.
---
 For instance, which fertiliser produces the highest yield? Answer is fertiliser
2. Which variety yields less? Variety 1. 
---
Now consider another example. Now the yields from the plots are such that the two
profiles are not parallel.<b><font color="red">
[[shown}{anim]] </font></b> Now the question "Which fertiliser
is the most productive?" does not have a clear answer.
---
 We need to know which variety we are talking about. Fertiliser 3 is the best
for variety 1 and fertiliser 3 for variety 2. 
---
Since the effect of variety now is influenced by the choice of
variety, we say that we have <b>interaction</b> between variety and fertiliser
here. In the earlier case there was no interaction.
---
 This chart is
called an <b>interaction chart</b>. It is a valuable graphical device in
ANOVA whenever there are multiple categorical inputs.

In the next video we shall see how to make such charts in LibreOffice.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=5</i></legend>
Interaction chart.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.40</i></legend>
We have seen examples of interaction between two categorical
inputs, or factors, as they are called.
 In this video we shall see how a factor input may be
locked in interaction with a continuous input, ie a
covariate.<b><font color="red">
[[fac-cov-inter shown]] </font></b> 
---
We shall consider a toy data set where we have three variables height,
weight and gender of some adult persons. We take 20 people,
10 male and 10 female, and record their heights and weights.
---
Our aim is to see if
height has influence on weight, and if the height weight relation
is the same for both genders. 
---
Let's make a scatterplot using colour coding for gender.<b><font color="red">
[[shown]] </font></b>
Here blue is for male, and red for female. Notice that all the 20
points follow the same linear pattern. 
---
 We are deliberately using a toy data set here, so that we can
demonstrate another possibility. What you see now is the first
possibility, where the effect of height on weight is not
influenced by the person's gender.
---
 If I ask the question:
<b>"how many extra kilos for an extra inch?"</b> the answer is the slope
of the linear pattern. The same value works for either
gender. This is the "no interaction" case.
---
Now suppose that the female points are rotated
slightly.<b><font color="red">
[[shown]] </font></b> Now the male cluster and the female cluster
each has a linear pattern, but the slopes are different.
---
 So in
order to answer the same question, we now need to know the gender
in question, so that we know which linear pattern we should take
the slope of. Here we have interaction, interaction between the
categorical input gender and the continuous input height.
---
As we know main effect ceases to be meaningful in presence of
interaction. It is of course possible to average out the two
slopes and report that as the effect of height on weight
irrespectivel of gender. But that is actually meaningless. 
---
The next video will quickly remaind us how to create a colour
coded scatterplot like the ones used here.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=5</i></legend>
Color coded scatterplot.
</fieldset>

</div>

<fieldset>
<legend>M3L4V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Show interaction chart and ask if significant. Intersecting<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show interaction chart and ask if significant. Parallel<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show interaction chart and ask if significant. Yes, no
intersection, but not parallel.<img src="../image/box.png"></p>



<p>
<b>EXERCISE:</b>&nbsp;Significant interaction. Ask about main effect. Meaningful? No.<img src="../image/box.png"></p>

</pre>
</fieldset>



<fieldset>
<legend>M3L4V2</legend>
<pre>

<p>
<b>EXERCISE:</b>&nbsp;Make from data and interpret. Parallel.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Make from data and interpret. Nonparallel.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 4: Lesson 5: Review">Module 4: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 4, Lesson 5 , Video 1: Review">Module 4, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 4, Lesson 5 , Video 2: Review">Module 4, Lesson 5 , Video 2: Review</a></h3>
</div>



<h1><a
name="Module 5: Regression">Module 5: Regression</a></h1>


<h2><a
name="Module 5: Lesson 1: Regression concept">Module 5: Lesson 1: Regression concept</a></h2>

<b>Total lesson duration
= 23.3</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 1, Video 1: The concept">Module 5, Lesson 1, Video 1: The concept</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.4</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=2.30</i></legend>
We had mentioned scatterplots in the Basic Statitics course. They
are an important visual tool to assess the relation between two
continuous variables. For instance, consider this toy data set of
height and weight. The scatterplot looks like this. Here you can
see a linear pattern. I said a linear pattern, not a
line. Because not all the points are lying exactly on a line. But
still we can feel the presence of a line. Our aim is to capture
that line mathemarically. Let's frst try by eye estimation. Let's
consider this line. Is this a good fit? Since not all the points are exactly on a
line, whatever line we draw is bound to miss some points. But
this line goes far from the cloud of points. We need something
right through this cloud, like this. This is much better. Now
what about this? Well, this looks fine too. If I ask you choose
one of these two good fits, which one would you choose? Hard to
say by visual inspection alone. So you see the two problems that
lie in the way to mathematically capturing the best line.

* First, how to do mathematically what the eye does easily:
avoiding such lines that are wide off the mark.

* Second, how to do what eye cannot do: choosing objectively the
best among such best contenders.

Before we go into further details, let me tell you that the
problem is not specific to linear patterns alone. Consider this
scatterplot that shows a curved pattern like a parabola. Surely
this parabola is a bad fit[shift], as is this one[scale]. We
would like to have a good fit like this. And somehow we also want
an objective satisfaction of having chosen the best fit.

This is the problem of regression. It has two aspects:

* Choosing the right form (like a straight line or parabola or
something else)

* and then picking the best line of that form.
</fieldset>

<fieldset>
<legend>
(s2)[n] <i>Duration=1.10</i></legend>
Most of the time we choose the form either by visual inspection
or by domain knowledge, eg, we know that projectiles near the
ground move along parabolic paths. [More examples: Boyle, Lens] 
Any such mathematical form is
characterised by some parameters eg, a straight line by the
form $y = a + bx,$ where $a$ and $b$ are the
parameters. A parabola has the form $y = a + bx + cx^2,$
where $a,b,c$ are the parameters. Choosing the form is the
first step.

Once we have finished choosing the form the next step is to
estimate the values of the parameters. 

Regression that proceeds in these two steps is called parametric
regression.

If, however, we want to automate even the first step, namely
choosing the form, then we need what is called nonparametric
regression. This course will only discuss parametric regression.

</fieldset>

</div>

<fieldset>
<legend>M5L1V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Show plot. Classify as increasing/decreasing/nonlinear.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Strong relation, weak relation.<img src="../image/box.png"></p>

</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 5, Lesson 1, Video 2: Mathematical formulation">Module 5, Lesson 1, Video 2: Mathematical formulation</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=0.20</i></legend>
In this video and the next video we shall introduce the mathematical model
behind linear regression. It is best understood through an example.
</fieldset>

<fieldset><legend
    class="cu">
[n] <i>Duration=5</i></legend>
[Use shiny]
Here is a scatterplot. We discern a linear pattern. So we try to
fit a straight line to the data cloud. That means looking for a
line like $y=a+bx.$ Here $a$ and $b$ are the two
parameters. The first parameter is called the intercept, the
second the slope. We are looking for optimal values for them. To
appreciate the problem imagine that there are two sliders one
for $a$ the other for $b.$ If I move the $a$
slider then the line shifts up and down remaining parallel to
itself. If I move the $b$ slider then the line rotates
around the point where it hits vertical axis. Our aim is to
find $a$ and $b$ such that the line passes as close as
possible to all the data points. 
</fieldset>


</div>



<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Computing with LibreOffice for the same data set.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 4</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=4</i></legend>
Fit polynomial using linest.
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 4.7</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=0.70</i></legend>
Now the way I described in the last video, is more like looking at data. But as I have
been saying again and again, statistics is all about looking through
data at the underlying truth. So let's understand the underlying
truth here. It will help to visualise the set up as a measurng
the length of a spring from which we are hanging different
weights. The weight we hang is $x$ and the measured length
is $y.$ Every time we choose... a weight to hang,
</fieldset> 

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
ie we choose
a value for $x,$ nature decides about the length in two
steps. First, she uses two fixed numbers $\alpha $
and $\beta$ (depending on the spring used), and
computes $\alpha + \beta x.$ Then in the second step she
adds a little random error $\epsilon$ to it to get $y =
\alpha + \beta x + \epsilon,$ which she discloses to the
statistician. This is what happens in each measurement,
the $\alpha $ and $\beta $ remain the same, but the
random error keeps on changing. So we get this  model:
$$
y_i = \alpha + \beta x_i + \epsilon_i
$$
for $i=1,...,n.$ There is also some assumption on the random
errors. They are assumed to be independent with normal
distribution having mean 0 and some unknown variance $\sigma^2.$ 

This is called the regression model.

Now the problem is to estimate $\alpha$ and $\beta.$ A secondary problem is
to estimate $\sigma^2,$ which measures the accuracy of the
line. 
</fieldset>

</div>


<fieldset>
<legend>M5L1V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Show plots of errors as lines (vertical, horizontal,
perp). Ask to identify least squares.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 5: Lesson 2: Least squares">Module 5: Lesson 2: Least squares</a></h2>

<b>Total lesson duration
= 27.6</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 2, Video 1: Least squares">Module 5, Lesson 2, Video 1: Least squares</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.3</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=0.50</i></legend>
Here we shall describe the most popular technique for
regression. We shall explain the idea with fitting a straight
line, but the idea is equally valid for other types of lines, like
parabolas. 

We have already mentioned the idea of thinking of fitting a
line $y = a + bx$ by imagining that we have sliders
for $a$ and $b.$
</fieldset>

<fieldset><legend
    class="cu">
[n] <i>Duration=4</i></legend>
Show the vertical error lines.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Mathematically</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[n] <i>Duration=0.70</i></legend>
For any given values of $a$ and $b$ we have the total
error 
$$
S(a,b) = \sum (y_i - (a+b x_i))^2.
$$
We want to minimise this w.r.t. $a$ and $b.$ A little
mathematics shows that the minimum occurs when 
$$
b = ... \text{ and } a = \bar y - b \bar x.
$$
So we say $\hat \beta  = ...$ and $\hat \alpha   =
...$. 

This $\hat \beta $ is called the estimated regression
coefficient.

An estimator for $\sigma^2 $ is ...

</fieldset>

</div>


<div class="header">
<h3><a
name="Module 5, Lesson 2, Video 1: Least squares">Module 5, Lesson 2, Video 1: Least squares</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.3</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=0.30</i></legend>
We have learned one way to estimate $\alpha $
and $\beta.$ The estimators have some interesting properties
that we want to discuss here. First,...

</fieldset>


<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>

 the regression coefficient
is of the form $\frac{\cos(x,y)}{\var(x)}.$ And second the
estimated line always passes through the centre of the data cloud $(\bar x, bar y).$

As we have already mentioned in an earlier lesson, statisticians
always worry about the sampling distributions of the
estimators, in particular their bias and standard error. It may
be shown that both the estimators are unbiased.  Also their
standard errors are given by ... [Show that if x's are close
together then variance increases]

Don't be scared by the formulae. When you compute them for a
given data set, they are just two numbers. If the numbers are
large, then that is a cause for worry, because large standard
errors mean inaccurate estimators. 
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Hand computation of regression line.
<font color="red">
<pre>
set.seed(135319)
(x = 1:5)
y = as.numeric(format(2+x+rnorm(5),dig=2))
cbind(x,y)
(sx = sum(x)); (sy = sum(y))
(sx2 = sum(x*x)); (sy2 = sum(y*y))
(sxy = sum(x*y))
(mnx = mean(x)); (mny = mean(y))
(vx = var(x)); (vy = var(y)); (cxy = cov(x,y))
(b = lm(y~x)$coef)
as.numeric(format(lm(y~x)$fit))
</pre>
</font>

</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5.4</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=0.30</i></legend>
Notice that there is some assymmetry in the set up, predicting y
based on x. It could be the other way round. We shall discuss
that now.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">x on y</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[n] <i>Duration=5</i></legend>
[x-on-y, y-on-x, beta*beta = r^2, sign]
</fieldset>

</div>


<div class="scrpt">
<b>Total video duration
= 5.6</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=0.5</i></legend>
Ramble intro
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Orthogonal regression</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[n] <i>Duration=5</i></legend>
Orthogonal regression.
</fieldset>

</div>


<fieldset>
<legend>M5L2V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;
Show plots of errors as lines (vertical, horizontal, perp). Ask
to identify least squares.
<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show bad fit. Ask to improve by lift, lower, swing
clockwise, swing counter-clockwise.<img src="../image/box.png"></p>

</pre>
</fieldset>



<fieldset>
<legend>M5L2V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Fit least squares to toy data with intercept.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Fit least squares to toy data without intercept.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Anscombe.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 5: Lesson 3: Real life data">Module 5: Lesson 3: Real life data</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 3, Video 1: Real life data">Module 5, Lesson 3, Video 1: Real life data</a></h3>
</div>


<fieldset>
<legend>M5L3V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Real life data. Show plot. Ask for estimate by inspection.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Ask to guess sign of coeff.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data. Ask to juudge if intercept should be present.<img src="../image/box.png"></p>

</pre>
</fieldset>



<fieldset>
<legend>M5L3V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Real life data.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real life data.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 5: Lesson 4: Residuals, outliers, leverage">Module 5: Lesson 4: Residuals, outliers, leverage</a></h2>

<b>Total lesson duration
= 27.4</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 1: Residuals (theory) ">Module 5, Lesson 4, Video 1: Residuals (theory) </a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[n] <i>Duration=6.00</i></legend>
We have mentioned that statisticians look through data rather
than loot at data. They look through data at an underlying truth,
that is at least partly unknown. Unless some part of this truth
is assumed to be known it becomes impossible to make any
progress. But how can part of the truth be already known
beforehand? Sometimes from past experience or expert opinion. But
more often it is just an untested  gut feeling of the statistician that is
accepted at face value. Clearly any method built on such
unverified premise has a possibility of going wrong. So a
statistician must carefully assess the success of any proposed
method to detect any mistake in his original assumptions. Indeed,
this is the most important point where a professional statistcian
differs from an amateur one. It is not diffcult to become an
amateur statitician by learning a few
standard statistical softwares and applying their canned routines to a data
set. But that often leads to erroneous results, unless the
assumptions underlying the methods are carefully checked, and
appropriate measures taken in case the assumptions fail. It is
like driving a car. Pressing on the gas pedal and turning the
steering wheel are easy when everything goes according to your plan. The difficulty lies in
swerving your vehicle in time when a car comes at you unexpectedly from the other
direction. 

This idea of cross checking the basic assumptions after applying
a statistical method is a general principle applicable everywhere
in statistics. But its need is seldom felt more strongly than in
regression, because there the original assumptions often fail,
and a plethora of tools are available to rectify a failed
assumption. All these together are called Regression Diagnostics.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 2: Regression diagonsotics">Module 5, Lesson 4, Video 2: Regression diagonsotics</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[n] <i>Duration=6</i></legend>
Remember we had the assumption $y_i = \alpha + \beta x_i +
\epsilon_i,$ where the $\epsilon_i$'s were supposed to be
iid with normal distribution having mean 0. As we cannot observe
the $\epsilon_i$'s, how can we hope to check this
assumption? Here is a way. After estimating $\alpha$
and $\beta $ as $\hat \alpha $ and $\hat \beta $
we can compute $\hat \epsilon_i = y_i-\hat \alpha - \hat \beta
x_i.$ These are not same as $\epsilon_i$'s, just
as $\hat \alpha $ is not the same as $\alpha.$ These
are called the residuals. and give an idea about the
actual $\epsilon_i$'s which are unobserved. Plotting these
residuals against the $x_i$'s produces a residual plot. This
plot should ideally show no pattern. Like this. But if it does, then that is
to be construed as a shortcoming of the originally assumed
model. 

For example if the residual plot looks like this, where there a
slightly curved pattern, then possibly we should have included a
curvature in the model to start with. 

The next video will show such an example.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 3: Residuals (lab) ">Module 5, Lesson 4, Video 3: Residuals (lab) </a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Computing residuals. Plotting them. y=1/x model.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 4: Outiers, leverage (theory) ">Module 5, Lesson 4, Video 4: Outiers, leverage (theory) </a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.4</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=1.70</i></legend>
We have talked about outliers in our Basic Statistics course
already. Roughl speaking these are points that lie far away from
the main bulk of the data. For instance these two points in this
scatterplot are outliers, while these points are not. There are
two main types of outliers. Those with leverage and those without
it. In this and the next video we shall learn about them. 

Outliers, whether with or without leverage, must be studies
carefully. They are born out of unexpected reasons, which may be
just a typo, or a glimpse of an unknown behaviour. Whether an
outlier has leverage or not is determined by how much effect it
has to turn the fitted model towards itself. Consider this
scatterplot. The least squares line is this. Now consider an
outlier here. Here the least squares line moves only slightly
away due to the presence of the outlier. We say that this outlier
has low leverage. Such outliers are less harmful. 

Next consider an outlier here. Here the least squares line swings
a lot towards that point. Somehow this point exercises so much
pull on the line that this single point can force the line to
move away from bulk of the points. We say that this point has a lot
of leverage. Clearly such outliers are more dangerous. It is
important to remove them if we want to fit a line showing the
true pattern of the points.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Why leverage</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[n] <i>Duration=1.60</i></legend>

As may be guessed from the two examples, leverage of a point
determines on how far it is from the centre of the data along
the $x$-direction. This point, though away from the bulk of
the points, was close to the centre along
the $x$-direction. However, this point was far away from the
bulk of the points along the $x$-direction.

In fact, leverage is a property of the all the points, whether an
outlier or not. Every point has some leverage. Points near the
extreme along the $x$-direction have greater leverage. In
general, points with high leverage are not bad. In fact, if you
have a point here, it is good for the least squares fit, because
it strengthens our faith in the line when we see that even this
far away point still follows the same pattern. However, outlier
with high leverage must be removed before we accept the fitted
line. 

As outlier with high leverage tend to swing the fitted lies
towards themselves, these points may not have high residuals. So
a residual plot need not show these points clearly. There are
quite a few sophisticated techniques to measure leverage of
point. We shall not go into them. For our course simple visual
inspection of the scatterplot is enough.
</fieldset>

</div>

<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 5: Outiers, leverage (lab) ">Module 5, Lesson 4, Video 5: Outiers, leverage (lab) </a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Outliers, leverage plots.
</fieldset>

</div>

<fieldset>
<legend>M5L4V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Show scatter plot. Ask to identify outlier.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Show scatter plot. Ask to identify leverage point.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Toy data. Fit, drop and refit. Ask for change.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show residual plots. Ask to identify good one.<img src="../image/box.png"></p>

</pre>
</fieldset>


<fieldset>
<legend>M5L4V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Real data with outlier. Identify.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Make resudual plot.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 5: Lesson 5: Review">Module 5: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 5 , Video 1: Review">Module 5, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 5, Lesson 5 , Video 2: Review">Module 5, Lesson 5 , Video 2: Review</a></h3>
</div>



<h1><a
name="Module 6: Time series analysis">Module 6: Time series analysis</a></h1>


<h2><a
name="Module 6: Lesson 1: Time series concept">Module 6: Lesson 1: Time series concept</a></h2>

<b>Total lesson duration
= 13.4</b>
<p></p>

<div class="header">
<h3><a
name="Module 6, Lesson 1, Video 1: Concept: What it is">Module 6, Lesson 1, Video 1: Concept: What it is</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 5.9</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=5.90</i></legend>
To be able to predict the future has always been the dream of
man. From science to science fiction, from astronomy to
astrology, from stock market to politics, from mythology to meteorology we see ample evidence of
man's fascinaion with predicting the future. Ancient mythology of
every culture is rife with people endowed with the  super human
power of seeing the future. Weather forecast is part of our every
day life. Politicians analyse popularity trend and try to predict
the outcome of polls, share holders try to predict market trends.

Except in purely fiction, all attempts to predict the future,
whether successful or not, depend on the same basic technique:
analysing past data, and looking for patterns in how the values have been
evolving over time, and applying the same pattern for the future
as well. 

And that is what time series is all about. A time series 
consists of values of one or more  variables
over time, to understand its temporal behaviour. In such a data
set time is always one of the variables, and there is at least
another variable whose values are evolving with time.

A time series is completely different from the other types of
data we have handled so far. And this diffeence must be borne in
mind while working with time series. We had the mental picture of
some underlying distribution from which the data are being
generated. The idea was something like a coin toss, where the
same coin is being again and again. So the more tosses you have
the more likely you are to see statistical regularity to set
in. However, for the situation is different for a time
series. Here the underlying distribution may itself evolve with
time. Thus, the second value in a time series need not come from
the same distribution as the first one. This makes time series
analysis much more challenging than the type of data analysis we
have encountered so far. You might think of this analogy. I have
many coins laid out in a row. I toss each coin once, and try to
infer about the probability of head for all the coins. This is an
impossible task to achieve, because the coins are different, and
I have outcome of only a single toss for each coin. As a result
all time series analysis methods start by putting lots of
assumptions that prevent the underlying process from evolving too
much. Most of the time these assumptions, or time series models
as they are called are themselves quite complicated. Later in
this module we shall take a look at a simple time series model. 

Whatever we do takes time. And data collection is no
exception. In that sense most data sets should be called time
series data, unless all the cases where recorded in parallel. For
instance, if I am interviewing  100 households regarding some
demographic survey I am doing so sequentally over time. Should I
call that a time series data? Not really, because I do not expect
the underlying situation to have evolved much  during that
period. On the other hand if we are measuring the amount of
suspended particulate matter in the air at some crossing in a
city for a year, then it is definitely a time series, because the
amount of dust in the air is likely to vary in a systematic way
over the weekdays and weekends. The main aim behind any
statistical analysis is to account for variation in the
values. If you suspect that time possibly accounts for a
significant portion of the variation, then and only then you
designate that data as a time series.


Most time series analysis methods expect the data to be regularly
spaced in time. This temporal frequency is often mentioned in the
title of the data set, like daily rainfall, or monthly sales or
annual production. Thus, each value in a time series need not be
associated with a time point, but rather a time interval. Usually
each value asociated with a time interval is some kind of a
summary value of multiple actual measurements made during the
interval. For example it could be the total sales for a month, or
average pollution level over a day. Indeed, associating a value
with an interval rather than a single time point allows the data
collectors to avoid a serious problem, the problem of missing
data. Since we cannot go back in time, so if we failed to collect
data in time, then there is no way we can get it back later. By
the time we come back, the process has possibly evolved into
something new. But when we areport a single average value for a week, we
can still make up for the missing value by making other
measurements during the same week. 


In the modern days, when much of the data are collected by
sensors connected to microcontrollers, there is a boom of time
series data. These data sets often have very time resolution, ie,
we may have temperature sensed every minute. Such time series are
used very much by doctors. We are all familiar with ECG. From the
statistical viewpont they are nothing but time series data. 
</fieldset>

</div>

<fieldset>
<legend>M6L1V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Describe different data scenarios. Ask to identify time
series.<img src="../image/box.png"></p>

</pre>
</fieldset>



<div class="header">
<h3><a
name="Module 6, Lesson 1, Video 2: Concept: Estimation, prediction, smoothing">Module 6, Lesson 1, Video 2: Concept: Estimation, prediction, smoothing</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.3</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=4.30</i></legend>
Why do we care about time series data? 
The primary aim of time series analysis is, as I have already
mentioned, prediction. Trying to extrapolate patterns from the
past to guess what is going to happen in future. I shall mention
here three surprisingly different variations of situations where
time series are analysed with the chief goal of predicting the
future:

* The first variant is what we already know about. Predicting the
stock market. To be forewarned is to be fore armed. Is the motto
in economics. That is why we try to predict GDP, cost of houses,
or agricultural yield.

* A drastically different application of prediction is active
noise cancellation that is used to cancel noise by an
artificially generate counter-noise!
Imagine a noisy engine. The sound that it produces is a basically
a time series of the vibration of the air molecules. Now the
vibration is caused by applying force on the molecules. So if we
can apply just the opposite force on the air molecules at the
right time, then the air milecules can be made to remain
stationary. Such a counterbalancing force may be applied by
generating another noise. But this noise needs to mimick the
original noise very closely in order to be able to cancel it. So
the game is like this. A sensor picks up the noise,
mathematically inverts it, produces the counter noise and
releases in the air. All this takes a fraction of a second, but
even within that fraction the force applied by the original sound
has changed (it is a vibration after all, so forces are changing
to and fro all the time). So the counter noise we are producing
now should actually mimick the actual noise to be produced a
fraction of a second in the future. So that's where prediction
comes into play. Predicting just a fraction of a second ahead,
but still that's a prediction nonetheless. Any error in that
prediction will mean the counter noise will not cancel the true
noise, and may actually lead to even more noise.

* Another quite different type of prediction is needed in
tracking an air craft. Indeed, this application has given time
series analysis a frequntly used tool called Kalman
Filtering. The situation is like this. We are tracking a flying
aircraft. Our camera has located it as a tiny dot in the vast sky
and steadily moving always keeping the crosshair on the
aircraft. Suddenly a piece of cloud comes along, and the camera
loes the aircraft in it. Of course, the air craft is sure to
emerge out of the cloud and the camera should wit for that. But
where exactly should it point in order to pick up its
target. Starting to look al over sky is quite time consuming, and
the air craft may very well pass out of sight during the time the
camera is scouring the sky. A little time series analysis helps
here. During the time the camera was locked onto the air craft,
we knew how its position was changing over time, a time
series. So we have an idea of its speed, acceleration turning etc
just the moment before it disappeared. Well, it won't be very
unreasonable to assume that the aircraft continued the motion
more or less in the same way even when it was hidden behind the
cloud. So we can predict where the aircraft will be at a given
time point in the future. Remember we are talking of distant
future here, may be just a few seconds or at most a minute
ahead. So we continue to move the camera acclording the predicted
path, also keeping an eye on the sky in its vicinity. This
markedly improves the chance of picking up the lost aircraft
again after it emerges. 

So we learned about three types of
prediction. The next video will take up a different application
of time series. 
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 6, Lesson 1, Video 2: Concept: Estimation, prediction, smoothing">Module 6, Lesson 1, Video 2: Concept: Estimation, prediction, smoothing</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.2</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=3.20</i></legend>
Prediction is definitely the most important aim behind analysing
a time series. 
However, that is not the only 
application. Sometimes time series data are analysed for the
purpose of understanding the process generating the time
series. We shall mention three examples chosen from as diverse
fields as possible to provide an idea about the gamut of
application.

* Let's do a though experiment. Imagine that you are driving down an uneven road. Naturally
everything inside a car is shaking. An accelerometer
embedded in the car is keeping track of the bumpiness of the ride
and produces this  time series. You can see  ups
and downs in it. They are more or less regular except here, where
they seem much more rapid. Clearly, the car was shaking a lot
here, possibly over a particularly rough patch in the road. Now
imagine the same plot, but not in the context of a car ride. This
time consider this as a plot of the market price of some
stock. Then this rough portion denotes a period of greater
volatility. These periods are of great importance of market
analysts. They often try to explain the volatility of the market
in terms of what happened before it. 

* I am sure you use a mobiel phone. You may not be aware that
rather sophisticated time series analysis is needed to keep in
running. Indeed, so important is this application area, that it
has a special name Signal processing, which is often considered a
subject of its own, extending well beyond staistical time series
analysis. Some of the most important tools used in staistical
time series analysis come from this application area. Roughly
speaking, it tries to look for waves or periodic behaviours in a time
series corresponding to the different wavelengths. Information is
encoded in terms of those waves. The idea is used even to detect
cycles in the economic markets.

* One recent example is searching exoplanets. It is not
directly related to this course, but still I mention it because
it is such a unique application. NASA tries to find places in the
universe where life may be supported. Naturally, the focus of
attention is on earth-like planets around far away stars. Now the
stars themselves appear to be tiny dots, so a tiny planet like
earth circling around it is just not visible to even the most
powerful telescope. Quite surprisingly, time series analysis
helps to detect such planets in many cases. When the planet
passes in front of the star, its brightness diminishes a
little. Astronomers observe the brighnesses of the stars and
detect if there is any temporary dip in that series. If so, they
can estimate the size of the planet as well as its orbiting
period by the lengths of  and gaps between the dips. Personally,
I find it an amazing application.


</fieldset>

</div>


<fieldset>
<legend>M6L1V2</legend>
<pre>

</pre>
</fieldset>



<h2><a
name="Module 6: Lesson 2: Plotting">Module 6: Lesson 2: Plotting</a></h2>


<div class="header">
<h3><a
name="Module 6, Lesson 2, Video 1: Plotting">Module 6, Lesson 2, Video 1: Plotting</a></h3>
</div>

<b>Total lesson duration
= 0</b>
<p></p>

<fieldset>
<legend>M6L2V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Show time series plot. Ask for trend.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show plot. Ask for periodicity.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show periodic time series. Ask for period.<img src="../image/box.png"></p>

</pre>
</fieldset>




<fieldset>
<legend>M6L2V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Plot and identify trend.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Plot and identify periodic pattern.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show liner, quadratic and exponential curves. Ask to plot
RBI data. Identify the pattern.<img src="../image/box.png"></p>

</pre>
</fieldset>


<h2><a
name="Module 6: Lesson 3: Analysis">Module 6: Lesson 3: Analysis</a></h2>

<b>Total lesson duration
= 13.1</b>
<p></p>

<div class="header">
<h3><a
name="Module 6, Lesson 3, Video 1: Trend+Seasonal+Error">Module 6, Lesson 3, Video 1: Trend+Seasonal+Error</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 3</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=3.00</i></legend>
We had earlier talked about looking through data as opppsoed to
looking at data. We look through data at the underlying
process.  Writing down a clear
statement about what we assume known/unknown about this
underlying process is called the statistical model. 
So far in our course, this process has been a just a distribution, which we
visualised as the shape of a histogram. This sufficed because
the variables that we were working with varied only with chance. 
Like a coin toss, the same coin was tossed in each case. The
value could be related to other random variables, but before
collecting the value we had no a priori reason to suspect a
ifferent behaviour. This, however, is no more true for a time
series. Here I know the time even before measuring the value. So
the process must somehow incorporate the time. Thus a time series
model should be a mixture of a function of time and a
distribution. 

There are many different time series models proposed in the
literature ranging from simple to extremely complex. In this
course we shall satisfy ourselves with a simple model only. Let's
start with an example.

Here is a time series plot. How would you describe the pattern
that you see? It looks much like a straight line with some
undulations. The variable is influenced by both the time and
chance. It is reasonable to say that time influences it as a
straight line, while chance contributes the ups and downs. So a
model could be $y_t = a + bt + \epsilon_t,$
where $\epsilon_t$'s are random noise. This is a special
case of a more general form of time series model, where time and
chance enter into the variable as two additive
components. $a+bt$ is the time component
and $\epsilon_t$ is the chance component. And they are
added. Notice that there is no contriution of chance in the time
component, and controbution of time in the chance part. Though we
write $t$ as subscript in $\epsilon_t,$ we consider
them just like coin tosses, all independent and identically
distributed, order does not matter. 

Another model of the same type is where time enters as a
quadratic: $y_t = a + bt + ct^2+ \epsilon_t,$ resulting in a
plot like this. 

Now we may not always be able to come up with a simple
mathemaical formula for the time component. The next video will
discuss a more flexible alternative that suits diverse real life scenarios.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 6, Lesson 3, Video 1: Trend+Seasonal+Error">Module 6, Lesson 3, Video 1: Trend+Seasonal+Error</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.1</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=2.20</i></legend>
In the last video we learned about a class of time series moels
where the time component and chance components are added
together.  Such models, while not always applicable, are the
easiest to interpret. The time component is a function of time
that has no randomness in it. If we assume a precise mathematical
form for it, like linear or quadratic as we did in the last
video, then the resultin model becomes too inflexible to use in
most rela life cases. So instead statisticians prefer to use a
time component that may be further decomposed into subcomponents
accounting for various interpretable behaviours. Well, that's
quite a mouthful. Let's digest it with an example. 

Here is a time series. How would you describe it in words. It has overall
slowly rising trend. There are lots ups and downs. On closer look
we notice that even the ups and downs are of two types. There is
seems to be a wave that repeats every 12 months, <i>i.e.</i>, an annual
cycle. Other than  that the remaining ups and downs appear to
be just random. 

Accordingly we propose the following model:
trend + cyclic.

Here trend refers to the long term slow overall of the series. It
may go steadily up or down, or may remain at the same level. If
our time series is really long, then the trend pattern may also
change somewhere midway. But whatever the case, trend always
reflects long term behaviour. 

The cyclic component represents any known periodic pattern. For
instance, a daily time series is likely to show a weekly or
biweekly cycle, a monthly time series typically contains annual
cycles, and so on. In fact, some time series may contain cycles
of multiple periods. Then people sometimes refer to them as
seasonal and cyclical patterns.

Now let's see how to isolate these various components from a
given time series.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Extracting components</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[n] <i>Duration=1.80</i></legend>
Here is a time series data set. The first step to analyse it is
to make a plot. The plot will give a visual check that our model
is a reasonable one and also give us a rough idea about the
period of cycle given. In many cases, however, the period is more easily
obtained from the data domain, <i>e.g.</i> a weekly cycle for a daily
data, or annual cycle for monthly data. Then we perform a moving
average. This means taking average of one period worth of the
series at a time. Here we have a weekly cycle. So we average the
first 7, and plot it at the centre. It is as if we are looking at
the series through a window of length 7 and averaging the part of
the seris through the window. Next we move the window one step to
the right and take average. And we go on like this, move the
window and average the visible part. This is what is called
moving average. The result is a shorter series that has less ups
and downs. This is our estimated trend. Now we subtract the trend
from the original series (ignoring the two end). This part should
contain only the cyclical and error part. We stack the weeks one
above the other and average. Thus we get an average value for the
Mondays, another for the Tuesdays, and so on. Finally we subtract
this from the series, and consider the remainder as the error.

The next video will show this in action.
</fieldset>

</div>


<div class="header">
<h3><a
name="Module 6, Lesson 3, Video 1: Trend+Seasonal+Error">Module 6, Lesson 3, Video 1: Trend+Seasonal+Error</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
Create a daily time series from scratch. The trend values along a
line and some day-of-the-week values adding up to 0. Finally
errors as rand()-1. Recover and compare.
</fieldset>

</div>

<fieldset>
<legend>M6L3V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Toy data. Hand computation.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Toy data. Hand prediction.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Ask for conceptual trend, period from data scenario.<img src="../image/box.png"></p>

</pre>
</fieldset>



<fieldset>
<legend>M6L3V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Real data.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real data.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real data.<img src="../image/box.png"></p>

</pre>
</fieldset>




<h2><a
name="Module 6: Lesson 4: Real life examples">Module 6: Lesson 4: Real life examples</a></h2>

<b>Total lesson duration
= 0.5</b>
<p></p>

<div class="header">
<h3><a
name="Module 6, Lesson 4, Video 1: Real life example">Module 6, Lesson 4, Video 1: Real life example</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 0.5</b>
<p></p>

<fieldset>
<legend>
(s1)[n] <i>Duration=0.50</i></legend>
So far we have given some idea about times series, what they are,
why they are important, and also given a very simple introduction
to modelling them in terms of trend and cyclical components. 

In this lesson we are about to see real life examples of time
series. These data sets are all freely available, and are
retrieved from the internet.  With our limited exposure to time
series analysis, we shall be barely able to do anything beyond
downloading and plotting the data, and noticing important points
about them. Sometimes we shall try to apply moving average to
smooth the time series.
</fieldset>

</div>

<fieldset>
<legend>M6L4V1</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Ask for source.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show plot. Ask for anomaly.<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;Show plot. Ask for anomaly.<img src="../image/box.png"></p>

</pre>
</fieldset>



<fieldset>
<legend>M6L4V2</legend>
<pre>


<p>
<b>EXERCISE:</b>&nbsp;Real data. Estimate and predict.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real data. Estimate and predict.<img src="../image/box.png"></p>

<p>
<b>EXERCISE:</b>&nbsp;Real data. Estimate and predict.<img src="../image/box.png"></p>

</pre>
</fieldset>




<h2><a
name="Module 6: Lesson 5: Review">Module 6: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 6, Lesson 5 , Video 1: Review">Module 6, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 6, Lesson 5 , Video 2: Review">Module 6, Lesson 5 , Video 2: Review</a></h3>
</div>



<hr/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/>
</body>
</html>
