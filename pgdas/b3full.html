<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE0592.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script>
<style>
body {
  margin: 0;
}


.sticky {
  position: fixed;
  top: 0;
  width: 100%;
  background: #555;
  color: #f1f1f1;
}

.cu {
  background: #ffcccc;
}

.bu {
  background: #ccccff;
}

.scrpt {
  border-left: 5px solid black;
}
</style>
<script>
window.onscroll = function() {myFunction()};
window.onload = function() {myInit()};

var header, tphldr;
function myInit() {
  header = document.getElementsByClassName("header");
  tphldr = document.getElementById("topholder");
}

function myFunction() {
  var index = -1
  for(i=0;i<header.length;i++) {
    if (window.pageYOffset > header[i].offsetTop) {
       index = i
    }
    else {
       break
    }
  }

  if(index < 0) 
    tphldr.innerHTML = "";
  else
    tphldr.innerHTML = header[index].innerHTML
}
</script><script type="text/javascript" src="https://www.isical.ac.in/~arnabc/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">

Welcome to the PGDAS course on Statistical Methods. I am your
instructor Arnab Chakraborty.
---
Our journey of learning statistics had started with the Basic
Statistics course. There we had learned some terminology and saw
some simple number crunching tools.
---
 Now that you have also learned
probability theory in a parallel course, we are ready to go
deeper. And that's what we shall do in this course. 
---
This course consists of 6 weeks of lectures. One module per
week.[shown] Each module has 4 lessons.[shown] And
each lesson some prerecorded 
videos and practice problems, as well as a weekly test, that will
be graded.
---
Each test will have 5 problems carrying 2 points each. So we have
10 points per module[shown]. Since we have 6 modules, this accounts for
60 points in all[shown]. The remaining 40 points will come from a final
exam[shown] that will take place within 2 weeks from end of lectures.
---
All the unit test problems will be of the peer review type, where
each student's solutions will be graded by three other students,
and the median score will be taken.
---
We shall use this book 
Mathematical Statistics with Applications by Ramachandran and
Tsokos as our main text book. 
Ocassionally we shall also refer to other sources as and when
needed.
---
And we shall  use LibreOffice Calc as our software.

OK, time to get rolling!

</script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head>
<body>
<div class="sticky" id="topholder"> </div>
<a href="http://www.isical.ac.in/~arnabc/">[Home]</a>
<h3>Table of contents</h3>
<ul>
<li>
<a href="#Module 1: Estimation">Module 1: Estimation</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 1: The concept">Module 1: Lesson 1: The concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 1: The concept (population,
sample)">Module 1, Lesson 1, Video 1: The concept (population,
sample)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 2: The concept (sampling distribution)">Module 1, Lesson 1, Video 2: The concept (sampling distribution)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 3: The concept (sampling distribution)">Module 1, Lesson 1, Video 3: The concept (sampling distribution)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 4: The concept (sampling distribution)">Module 1, Lesson 1, Video 4: The concept (sampling distribution)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 1, Video 5: The concept (sampling distribution)">Module 1, Lesson 1, Video 5: The concept (sampling distribution)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 2: Estimating mean">Module 1: Lesson 2: Estimating mean</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 2, Video 1: Estimating mean">Module 1, Lesson 2, Video 1: Estimating mean</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 2, Video 2: Lab">Module 1, Lesson 2, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 2, Video 3: Sampling distributions of mean">Module 1, Lesson 2, Video 3: Sampling distributions of mean</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 3: Estimating proportions">Module 1: Lesson 3: Estimating proportions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 3, Video 1: Estimating proportions">Module 1, Lesson 3, Video 1: Estimating proportions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 3, Video 3: Lab">Module 1, Lesson 3, Video 3: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 4: Estimating dispersion">Module 1: Lesson 4: Estimating dispersion</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 4, Video 1: Estimating dispersion">Module 1, Lesson 4, Video 1: Estimating dispersion</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 4, Video 2: Lab">Module 1, Lesson 4, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 4, Video 3: Sampling distributions">Module 1, Lesson 4, Video 3: Sampling distributions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 4, Video 4: Alternatives">Module 1, Lesson 4, Video 4: Alternatives</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 1: Lesson 5: Review">Module 1: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 5 , Video 1: Review">Module 1, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 1, Lesson 5 , Video 2: Review">Module 1, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 2: Test of hypotheses">Module 2: Test of hypotheses</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 1: Concept of test">Module 2: Lesson 1: Concept of test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 1, Video 1: The concept of test (sound of water)">Module 2, Lesson 1, Video 1: The concept of test (sound of water)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 1, Video 2: The concept of test (sound of water)">Module 2, Lesson 1, Video 2: The concept of test (sound of water)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 1, Video 3: Lab">Module 2, Lesson 1, Video 3: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 1, Video 4: Critical value method">Module 2, Lesson 1, Video 4: Critical value method</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 2: One sample t-test">Module 2: Lesson 2: One sample t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 2, Video 1: one-sample t-test">Module 2, Lesson 2, Video 1: one-sample t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 2, Video 2: one-sample t-test">Module 2, Lesson 2, Video 2: one-sample t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 2, Video 2: Lab">Module 2, Lesson 2, Video 2: Lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 3: Paired t-test">Module 2: Lesson 3: Paired t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 4: Two-sample t-test">Module 2: Lesson 4: Two-sample t-test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 2: Lesson 5: Review">Module 2: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 5 , Video 1: Review">Module 2, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 2, Lesson 5 , Video 2: Review">Module 2, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 3: Goodness of fit and independence">Module 3: Goodness of fit and independence</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 1: Contingency tables">Module 3: Lesson 1: Contingency tables</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1, Video 1: Contingency tables">Module 3, Lesson 1, Video 1: Contingency tables</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1,  Video 2: Contingency table">Module 3, Lesson 1,  Video 2: Contingency table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1,  Video 4: Contingency table">Module 3, Lesson 1,  Video 4: Contingency table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1,  Video 5: Contingency table">Module 3, Lesson 1,  Video 5: Contingency table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 1,  Video 6: Contingency table">Module 3, Lesson 1,  Video 6: Contingency table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 2: Independence test">Module 3: Lesson 2: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 3: Permutation test">Module 3: Lesson 3: Permutation test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 3, Video 1: Permutation test">Module 3, Lesson 3, Video 1: Permutation test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 3, Video 2: Permutation test lab">Module 3, Lesson 3, Video 2: Permutation test lab</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 4: Goodness of fit test">Module 3: Lesson 4: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 3: Lesson 5: Review">Module 3: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 5 , Video 1: Review">Module 3, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 3, Lesson 5 , Video 2: Review">Module 3, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 4: ANOVA">Module 4: ANOVA</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 1: ANOVA concept">Module 4: Lesson 1: ANOVA concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 2: ANOVA terms and data layout">Module 4: Lesson 2: ANOVA terms and data layout</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 2, Video 1: ANOVA terms">Module 4, Lesson 2, Video 1: ANOVA terms</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 2, Video 1: ANOVA terms">Module 4, Lesson 2, Video 1: ANOVA terms</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 3: ANOVA table">Module 4: Lesson 3: ANOVA table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 4: Interaction">Module 4: Lesson 4: Interaction</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 4: Lesson 5: Review">Module 4: Lesson 5: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 5 , Video 1: Review">Module 4, Lesson 5 , Video 1: Review</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 4, Lesson 5 , Video 2: Review">Module 4, Lesson 5 , Video 2: Review</a>
</li>
<li>
<a href="#Module 5: Regression">Module 5: Regression</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 1: Regression concept">Module 5: Lesson 1: Regression concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 1, Video 1: The concept">Module 5, Lesson 1, Video 1: The concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 1, Video 2: Mathematical formulation">Module 5, Lesson 1, Video 2: Mathematical formulation</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 2: Least squares">Module 5: Lesson 2: Least squares</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 2, Video 1: Least squares">Module 5, Lesson 2, Video 1: Least squares</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 3: Real life data">Module 5: Lesson 3: Real life data</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 3, Video 1: Real life data">Module 5, Lesson 3, Video 1: Real life data</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 5: Lesson 4: Residuals, outliers, leverage">Module 5: Lesson 4: Residuals, outliers, leverage</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 1: Residuals (theory) ">Module 5, Lesson 4, Video 1: Residuals (theory) </a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 2: Regression diagonsotics">Module 5, Lesson 4, Video 2: Regression diagonsotics</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 3: Residuals (lab) ">Module 5, Lesson 4, Video 3: Residuals (lab) </a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 4: Outiers, leverage (theory) ">Module 5, Lesson 4, Video 4: Outiers, leverage (theory) </a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 5, Lesson 4, Video 5: Outiers, leverage (lab) ">Module 5, Lesson 4, Video 5: Outiers, leverage (lab) </a>
</li>
<li>
<a href="#Module 6: Time series analysis">Module 6: Time series analysis</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 1: Time series concept">Module 6: Lesson 1: Time series concept</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 1, Video 1: Concept: What it is">Module 6, Lesson 1, Video 1: Concept: What it is</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 1, Video 2: Concept: Diverse applications">Module 6, Lesson 1, Video 2: Concept: Diverse applications</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 2: Plotting and real life examples">Module 6: Lesson 2: Plotting and real life examples</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 2, Video 1: Plotting">Module 6, Lesson 2, Video 1: Plotting</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 3: Analysis">Module 6: Lesson 3: Analysis</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Module 6, Lesson 3, Video 1: Trend+Seasonal+Error">Module 6, Lesson 3, Video 1: Trend+Seasonal+Error</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Module 6: Lesson 4: Prediction techniques">Module 6: Lesson 4: Prediction techniques</a>
</li>
</ul>
<hr/> -*- eval: (folding-mode t) -*-
<title>Course: Basic 3: Statistical methods</title>


<h1><a
name="Module 1: Estimation">Module 1: Estimation</a></h1>

<div class="scrpt">
<b>Total video duration
= 0</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0</i></legend>
Welcome to the PGDAS course on <b>Statistical Methods</b>. I am your
instructor <b>Arnab Chakraborty</b>.
---
Our journey of learning statistics had started with the Basic
Statistics course. There we had learned some terminology and saw
some simple number crunching tools.
---
 Now that you have also learned
probability theory in a parallel course, we are ready to go
deeper. And that's what we shall do in this course. 
---
This course consists of 6 weeks of lectures. One module per
week.<b><font color="red">
[[shown]] </font></b> Each module has 4 lessons.<b><font color="red">
[[shown]] </font></b> And
each lesson some prerecorded 
videos and practice problems, as well as a weekly test, that will
be graded.
---
Each test will have 5 problems carrying 2 points each. So we have
10 points per module<b><font color="red">
[[shown]] </font></b>. Since we have 6 modules, this accounts for
60 points in all<b><font color="red">
[[shown]] </font></b>. The remaining 40 points will come from a final
exam<b><font color="red">
[[shown]] </font></b> that will take place within 2 weeks from end of lectures.
---
All the unit test problems will be of the peer review type, where
each student's solutions will be graded by three other students,
and the median score will be taken.
---
We shall use this book 
<b>Mathematical Statistics with Applications by Ramachandran and
Tsokos</b> as our main text book. 
Ocassionally we shall also refer to other sources as and when
needed.
---
And we shall  use <b>LibreOffice Calc</b> as our software.

OK, time to get rolling!
</fieldset>

</div>

<h2><a
name="Module 1: Lesson 1: The concept">Module 1: Lesson 1: The concept</a></h2>

<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 1: The concept (population,
sample)">Module 1, Lesson 1, Video 1: The concept (population,
sample)</a></h3>
</div>

<b>Total lesson duration
= 29.5</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 6.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.30</i></legend>
The world around us is full of unpredictable variations. Unpredictable, yet
the unpredictability has a pattern in it. Man has been studying this
pattern ever since his earliest existence.
---
 There are different ways to deal
with such patterned unpredictability, and statistics is one of them. In
this lesson we shall learn what sets statistics apart from the other
approaches.
---
 This difference is the key to appreciating statistical methods,
their strong points as well as their weak points. 
---
This key concept in a nutshell is this: Whatever data we collect is like a
<b>cup of water from a vast ocean</b>. The cup of water is all that we have to
base our inference on, but it is not the water in the cup that we want to
draw inference about.
---
 The  target of our inference is the entire ocean. The
statistical term for the cup of water is a <b>sample</b>, the ocean being
called the <b>population</b>. 
---
Any serious statistical exercise starts with a precise and clear picture of
the population and its relation with the sample.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Population and sample</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.70</i></legend>
We shall start with a simple example. The very term <b>population</b>
conjures up the vision of the totality of all the people living in a
country. While statistics uses the term in a broader sense, this
nevertheless remains a good picture to keep in mind. 
---
Here is our toy example. Imagine a population consisting of all the people living in a
large country. <b><font color="red">
[[men shown]] </font></b> We want to know the height of the
tallest person in our population.<b><font color="red">
[[line shown]] </font></b>  
---
Of course, if we had a list of the heights of all the
members of the entire population, then it would have been just a matter of
looking up the maximum, something that a computer could have done easily
for us.
---
 But the point is that we do not have such a list to start with.
 All that we can do  is to draw a random
sample of individuals and measure their heights
only.<b><font color="red">
[[highlighted]] </font></b>
---
 There is no guarantee
of course that this sample will contain the tallest person in the
population,...
---
 and so there cannot exist any sure  way to find the maximum
height in the population by looking at just the heights in the sample. The
best that we can hope for is only a reasonable guess.
---
 The precise statistical
term for obtaining  such a reasonable guess
is <b>estimation</b>. Let's try to come up with a good estimation procedure.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Estimation</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=3.50</i></legend>
 Suppose the heights
of the people in our sample are 5'2'', 5'5'', 5'0'', 5'9'' and
6'3''.<b><font color="red">
[[list shown]] </font></b> 
----
Based on this somone estimates the maximum population to be 6'.<b><font color="red">
[[6'?]] </font></b> Is this a
good estimate? No! Because even in our sample we have a person taller than
this!<b><font color="red">
[[finger]] </font></b>
---
 OK, let's revise our estimate to <b>6'3''</b> then, the maximum in our
sample. That is not as absurd as the last estimate, but still this assumes
that just by randomly selecting 5 persons out of an enormous population we
have managed to get one of the tallest guys in the entire
country!
---
 Not impossible, but not very probable either! So we should better allow some
margin above the sample maximum. How to choose the margin? Here are two ad
hoc suggestions: 
---
* One method could be to use the gap between the top two tallest persons in
the sample as the margin. So we shall compute maximum+gap between
the tallest two.<b><font color="red">
[[shown]] </font></b>, we have called this gap the top gap.
 In our case it is
<b><font color="red">
[[finger]] </font></b> <b>6'3''-5'9''=6''.</b>
---
 So we estimate the population maximum to be 6'3''+6'' = 6'9''.<b><font color="red">
[[shown]] </font></b>

* Another method could be to scale up the sample maximum by a factor 5/4,
which is sample size/(sample size-1),<b><font color="red">
[[shown]] </font></b>.
---
These are, as I said, ad hoc suggesions, and one can come up with many such
suggestions. This scenario is indeed common to all
statistical methods. We can think of many ad hoc procedures, and
need to choose the best, or at least a good one among them.
---
 Instead of just blurting out a single number as our
estimate, we are instead trying to come up with a  rule or formula to
produce the estimate from the sample. Like<b><font color="red">
[[finger]] </font></b> "sample max +
top gap" or "size/(size-1) * max".
---
Such formulae are called <b>estimators</b>, as opposed to
an <b>estimate</b> which refers to the numerical value that the formula takes for a
given sample. The target quantity that we are trying to estimate
is called a <b>parameter</b>.
---
 Indeed, any unknown quantity regarding the
underlying population is called a parameter, whether or not we
are trying to estimate it. 
---
Our parameter of interest was the population maximum. Had we possesed a complete list
of all the heights in the population, then this would have been a dumb
clerical exercise. No scope of creativty there.
---
 But since we do not have
such a master list available, we can propose and compare between different
estimators. Lots of scope for creativity here! 
---
So here is the take away message that you should never forget. The ultimate
aim of statistics is not to <b>look at data</b>, but <b>through data at
the underlying reality</b>. Sample corresponds to the data, population to
the underlying reality.
</fieldset>

</div>

<fieldset>
<legend>m1l1_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If we have a sample consisting of the values 2, 5, 1, and
  the estimator for the population mean is the sample mean, then
  what is the estimate? 
  defaultFeedback: &gt;
      The estimate is just the value of the sample mean for the
  given sample: (2+5+1)/2 = 4.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 2: The concept (sampling distribution)">Module 1, Lesson 1, Video 2: The concept (sampling distribution)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.20</i></legend>
In the last video we saw the simplest scenario of <b>sample</b> and
<b>population</b>. Indeed, what we discussed there is what most people would
anyway associate with these concepts. But statisticians use the terms
population and sample in a much more general sense.
---
In most branches of science, we need to make measurements. Now if you measure
the same quantity repeatedly under as identical conditions as you can manage,
still the measurements fluctuate to some extent.
---
 It is hard to pin point the cause behind the variation. But
something imponderable beyond our control changes somewhere.
---
 One cannot avoid noticing the uncanny resemblance of
this with a coin toss. The outcome is random.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Statisticians' perspective</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.80</i></legend>
Statisticians like to invoke the same population and sample analogy even in
this case. It may require some effort to visualise this at first, but it is
well worth getting used to.
---
 Think of each measurement as picking a random
sample from a population of all possible measurements. As if nature has in
her secret store all these slightly differing values.
---
That is the population of measurements. When you are
making 5 measurements, it is as if you are picking 5 of the stored
measurements and making them public.
---
 To distinguish this abstract usuage of
the term population, people sometimes call it an <b>infinite
population</b>. 
</fieldset>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.90</i></legend>
Admittedly this might look like a rather contrived way of looking at
things.  However, there is a theoetical justification behind this
approach. And it comes from the concept of <b> statistical
regularity</b>. 
---
We had discussed this in our Basic Statistics
course. But it boots repetition. 

In certain situations we see that lots of randomness piled together somehow
cancel each other out and a more or less regular behaviour emerges.
---
  Well, this concept makes a somewhat unexpected
appearance in the context of measurements. Suppose that you measure the
same quantity repeatedly keeping the set up as unchanged as
possible. Thus you get lots of numbers.
---
 If you create a histogram<b><font color="red">
[[start shown]] </font></b> of these numbers, then you'll see that
a particular shape emerges as you make more and more
measurements. <b><font color="red">
[[anim]] </font></b> 
---
The shape will depend on the quantity being measured
and the measurement procedure being followed. This fixed shape corroborates
the idea that there is something fixed sitting unseen beyond our immediate perception
and all our measurements are coming from that.
---
 The shape of the histogram as captured by this curve is
basically a rough picture of that unseen master process. 
---
The more data we collect the better we get an idea of that process. We could
have known it completely if only we could collect an infinite amount
of data. Hence the notion of an <b>infinite population</b>.
---
 We often use the term <b>distribution</b> to mean the same thing. Thus we talk about a
random sample from an infinite population, and sometimes refer to the same
thing as a random sample from a distribution. 
</fieldset>

<fieldset>
<legend>
(s4)[a] <i>Duration=1.40</i></legend>
You often hear statements like <b>Let's consider a random sample
from such-n-such distribution.</b> It is important to understand what this
means. 
---
In this context, a distribution, or rather <b>probability
distribution</b> to be precise, refers to a particular shape of the
underlying population histogram.
---
 The shape is often depicted as a curve<b><font color="red">
[[shown]] </font></b> for a continuous
variable, and a bar chart<b><font color="red">
[[shown]] </font></b> for a discrete one. We usually
capture the shape using math formula, called...
---
a <b>probability
density function (pdf)</b> for the continuous case,
and a <b>probability mass function (pmf)</b> for the discrete case.
---
 Suppose that we have some
variable in a data set.  If we
make a histogram of all its values, then the histogram will have that
particular shape.
---
 The shape need not be apparent if we have too few
cases. But as the number of cases grows the histogram will go closer and
closer to that shape. 
</fieldset>

</div>

<fieldset>
<legend>m1l1_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     In a clinical trial we measure blood pressures of 10
  patients twice, once before and once after a medication. 
  Is this like drawing a sample of size 20 from a population?  
  defaultFeedback: &gt;
      No. It is a like drawing a sample of size 10.
      The population consists of all the patients represented by
  these patients (eg, similar to these 10 in terms of gender, age
  group, etc). Each measurement produces a pair (before, after).

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 3: The concept (sampling distribution)">Module 1, Lesson 1, Video 3: The concept (sampling distribution)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.9</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.70</i></legend>
In the last video we have introduced the all important concept
that sets statistics apart from other approaches to analysing
data.
---
 Statistics, as I have already said, looks through data
instead of looking at data. We shall see an important consequence
of this concept now. 
---
In the Basic Statistics course as well as in our  school days we
have learned about computing the mean of a bunch of numbers. You
do not need to be a statistician to do that.
---
 But the way a statistician looks at  mean is not exactly how a layman considers
it. You are given a bunch of numbers. You crunch them with a
formula, and out pops the mean, which is
just another number situated more or less near the centre.
---
 From a layman's perspective the story ends there. But 
for a statistician it is a path leading
towards the mean of the unknown distribution from which the
sample has come.
---
  The observed sample  is just one possible sample that has turned up
randomly. Any conclusion based on that also suffers from the same
randomness. It is the underlying distribution that is the
unwavering truth worth seeking.
---
 When you hear this for the first
time it might not make much sense, besides sounding a bit
theatrical. But the germ of this idea is already there in our
everyday lives, as a little thought experiment will show.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">A little thought experiment</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=2.10</i></legend>
Suppose that you are comparing between two fertilisers for a
certain variety of crop. The aim is to see which variety produces
the greater yield.
---
 For this purpose two plots<b><font color="red">
[[shown]] </font></b> are chosen as
identical as possible. The same variety of crop is sown in
both. Fertiliser 1 is used in the first, fertiliser 2 in the second.
The yields turn out to be
234 bushels and 235 bushels<b><font color="red">
[[shown]] </font></b>.
---
 Here 235 &gt; 234. There can't be any
doubt about that. But is this evidence enough to clearly conclude
that the fertiliser used in the second plot is the better one?
 No, because the margin is so low here.
---
 Just a single bushel, a single bushel out of over 230 bushels. One might say that such
a small difference could very well have resulted from pure chance
variations. That's commonsense.
---
The idea is that if we repeat the
entire experiment all over again but using the same fertiliser
for both the plots,...
---
 even then some minor difference between the
two yields is quite likely, say a bushel or two this way or
that. That is like an inevitable error margin.
---
 One fertiliser can be called better than the other in terms of yield only when
it produces a difference significantly larger than this
margin. Thus you see our common sense has this idea built into
it:
---
 If we repeated the same experiment, some fluctuation is inevitable, and our conclusion
 must not get caught in this fluctuation.
---
Statistics formalises this notion into what is called the
<b>sampling distribution</b>. That's what we shall discuss in the next video.
</fieldset>

</div>


<fieldset>
<legend>m1l1_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
    The average horizontal distance between adult human eyes is
  63 mm. and average vertical distance between them is 0 mm. (ie,
  the eyes are situated at the same height when one stands
  keeping the head erect). Now let's say you meet a man with
  horizontal eye distance 70 mm. and another man with vertical eye
  distance 7 mm. Which man will strike you as more odd?
  defaultFeedback: &gt;
      The second man of course. In fact, a man with horizontal
  eye distance 70 mm. is quite common. But a man with vertical
  eye distance 7 mm. is rather rare. So the distance from the
  average is not the only point, we have to take the variability
  in the population into account.
</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 4: The concept (sampling distribution)">Module 1, Lesson 1, Video 4: The concept (sampling distribution)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.20</i></legend>
The last video introduced the very important concept of <b>sampling
distribution</b>. Having a clear idea about sampling distributions is the key
to understanding the working of most statistical procedures.
---
We shall explain with a familiar example:
estimating the maximum height of people in a vast population.<b><font color="red">
[[shown]] </font></b> We
shall compare between two contending procedures.
---
 In both cases we shall start by drawing a sample of size 5
randomly from our population.<b><font color="red">
[[highlighted]] </font></b> 
---
 The first procedure will use just the <b>sample
maximum</b>, while the second procedure will multiply the sample max
by  5/4, which is the ratio sample size/(sample
size-1).<b><font color="red">
[[formula shown]] </font></b>
---
Once a particular sample of size 5 is drawn, these two estimators
will yield two numbers or estimates. Of course, just by staring at
those two estimates, we can never decide which estimator is
better. That's where sampling distribution comes to our
help.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=4.7</i></legend>
Show population of size 10000. Show true max. Draw sample. Find
estimates. Mention repeating this 1000 times. Show the results
already in a different sheet. Show histogram already made. Compare.
</fieldset>

</div>


<fieldset>
<legend>m1l1_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     What is a average length of human kidneys? Somebody
  just blurted out an estimate 12 cm. A statistician, however,
  carefully took a random sample of size 5 and found the sample mean, which
  also turned out to be exactly 12 cm. What are the estimators
  here? Name one difference between theor sampling distributions?
  defaultFeedback: &gt;
      The ad hoc estimator is just the constant 12 cm. The second
  estimator is (x1+...+x5)/5, where xi is the length of the i-th
  sampled kidney length. The sampling distribution of the ad hoc
  estimator has zero dispersion (it is just a constant, no
  variation), while the dispersion of the sampling dispersion of
  the second estimator is surely something positive.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 1, Lesson 1, Video 5: The concept (sampling distribution)">Module 1, Lesson 1, Video 5: The concept (sampling distribution)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.5</b>
<p></p>


<fieldset>
<legend>
(s1)[a] <i>Duration=2.50</i></legend>
We have learned quite a few terms. A <b>parameter</b> is any unknown
quantity related to the underlying distribution. It is a fixed
quantity, part of the ultimate truth.
---
 To estimate it we need an
<b>estimator</b>, which is a formula. It is like a machine that takes a
sample in and produces a number out<b><font color="red">
[[shown]] </font></b>. In general any such formula
is called a <b>statistic</b>.
---
 It is a singular noun, no 's' at the
end. An estimator is a just a statistic that is being
used for the purpose of estimation.
---
 Every statistic has its own <b>sampling distribution</b>, its
behaviour when different random samples from the same population
are fed into it.  
---
This is basically what we showed in the last lab session. We
played god. We generated many random samples, evaluated the
statistic for them and made a histogram to provide a visual
representation of the sampling distribution. 
---
While a visual representation is a good thing to have, we can use
techniques like <b>mean</b> and <b>standard deviation</b> to describe the
sampling distribution quantatively. 
---
The mean of the sampling distribution is the value around which the values of the
statistic are expected to hover. This is often called the 
expected value or <b>expectation</b>.
---
 So if the statistic is an
estimator, then the diffrence between this expectation and the true
value of the parameter is a useful thing to have. This is called
the <b>bias: mean value of estimator - parameter</b>.
---
 Of course, you may not be able to compute this since the parameter value is
unknown. But sometimes you can even without knowing the true
parameter value! We shall take this issue up in
the next lesson. 
---
Then you can also consider the standard deviation of the sampling
distribution. That has a special name here: the<b> standard error</b> of
the statistic. Unlike bias, which makes sense only for
estimators, standard error is meaningful for any statistic. 
---
The next video will give us an idea about bias and standard
error using LibreOffice.
</fieldset>

</div>


<fieldset>
<legend>m1l1_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     We have learned about bias and standard error. Which of
  these involve the unknown parameter? 
  defaultFeedback: &gt;
      Bias. As a result it may not be computable based on data alone.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Start with a population. Compute min. Draw sample, compute sample
min. Show already computed values. Compute their mean, subtract
true min. Also compute standard error. 
</fieldset>

</div>


<fieldset>
<legend>m1l1_f.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Suppose that we want to estimate the maximum income of a
population. We plan to use the sample maximum as our
estimator. Can this estimator exceed the value of the parameer
being estimated? 
  defaultFeedback: &gt;
     No. The sample maximum cannot be more than the population
     maximum!
</pre>
</fieldset>




<h2><a
name="Module 1: Lesson 2: Estimating mean">Module 1: Lesson 2: Estimating mean</a></h2>

<b>Total lesson duration
= 27.9</b>
<p></p>

<div class="header">
<h3><a
name="Module 1, Lesson 2, Video 1: Estimating mean">Module 1, Lesson 2, Video 1: Estimating mean</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 1.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.80</i></legend>
 In each of this and the next two lessons we shall take up a different parameter and
learn about estimating it. 

In the current lesson the parameter to be estimated is the
<b>mean</b>.
---
 Remember that here we are talking about the <b><font color="red">
[[shown]population
mean] </font></b>, and not
the sample mean, which we anyway know for sure for the sample at
hand, and hence we do not need to estimate.
---
There is an underlying
distribution. It's best to think
of it as the shape of a histogram<b><font color="red">
[[shown]] </font></b>.
---
 This distrbution has a mean<b><font color="red">
[[arrow shown]] </font></b>, a fixed number, which is also
unknown. It is this unknown number that we are 
trying to estimate. That is our parameter of interest.
---
 All that we have is a random sample from this distribution,  a bunch of
numbers whose histogram roughly resembles this shape.
---
 Our job is to
obtain a good estimator in terms of this random sample.
---
Here is a simple solution. If the sample histogram closely
resembles the population histogram, and our aim is to estimate
the centre of the population, then why not use the centre of the
sample histogram for that purpose?
---
 In other words, just use the sample mean to estimate the
population mean. 
---
we shall now see the sample mean in action using LibreOffice.
So in the next video we shall go to
the lab to explore
the use of sample mean for estiating the population mean.
</fieldset>

</div>


<fieldset>
<legend>m1l2_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Suppose that we want to estimate the mean income of a
population. We plan to use the sample mean as our
estimator. Can this estimator exceed the value of the parameter
being estimated? 
  defaultFeedback: &gt;
    Quite possible.
</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 1, Lesson 2, Video 2: Lab">Module 1, Lesson 2, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Normal data already present. Draw a Sample. Find mean. Show lots
of means and histogram already in different sheets. Mention how
adding a constant to population shifts the sample mean.
</fieldset>

</div>


<fieldset>
<legend>m1l2_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If the population distribution is symmetric, then would you
  expect the sampling distribution of the sample mean to be
  symmetric as well?  
  defaultFeedback: &gt;
    Yes. Assuming the sample to be a random one, all values have
  a fair chance to be included, and the formula for the sample
  mean gives equal importance to all the sampled values.
</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 1, Lesson 2, Video 3: Sampling distributions of mean">Module 1, Lesson 2, Video 3: Sampling distributions of mean</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.80</i></legend>
The lab session in the last video gave us some idea about the
sampling distribution of the sample mean. A more quantitative idea may
be had using its bias and standard error.
---
 Recall that the bias of
an estimator is the <b>E(estimator) - parameter</b>. In
general, this may not be computable, since the parameter value is
unknown.
---
 Here, however, we can compute the bias, and it just turns out to
be zero! So we call sample mean an <b>unbiased</b> estimator of the
population mean.
---
Let's see why sample mean is unbiased for poplation mean.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Proof</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=4.5</i></legend>
Proof of unbiasedness.
</fieldset>

</div>

<fieldset>
<legend>m1l2_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Suppose I consider the sample weighted mean (X1+2X2+3X3)/6. 
     Assuming a random sample, will it be unbiased for the
     poplation mean?
  defaultFeedback: &gt;
     Yes. The same argument as that shown in the video works here
  as well. 

</pre>
</fieldset>


<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.10</i></legend>
In the last video we looked at the bias of sample mean as an estimator
of the poplation mean. 
Next let's turn to the <b>standard error</b> of sample mean.
---
 This is the standard
deviation as computed from the sampling distribution of the sample
mean. 
---
 As may be expected this is closely connected with the standard deviation
of the underlying distribution. If the population standard
deviation is less, so should be the standard error of sample
mean.
---
 Also another thing should be intuitively obvious: the
larger the sample size, the better we can guess the underlying
distribution. So the standard error should decrease with
increasing sample size.
---
 In fact, we can show that the standard
error is 
<b>$$
\frac{\sigma}{\sqrt n},
$$</b>
where $\sigma $ is the population standard deviation,
and $n$ is the sample size. Let's look at the proof.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Proof</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
Proof.
</fieldset>

</div>


<fieldset>
<legend>m1l2_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     We have a random sample of size 10 from a population. 
     We want to double the precision of the sample mean as an
  estimator of the population mean. How many more samples should
  we draw?
  defaultFeedback: &gt;
      Precision of an unbiased estimator is measured using its standard
  error. We plan to double the precision, so we need to halve the
  standard error. So square root of n needs to be doubled, ie, n
  needs to be four times its present value. So we need a sample
  size 10*4 = 40. Thus we need to increase the sample size by 40-10 = 30.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 4.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.30</i></legend>
There is a very important theorem in statistics called the <b>Central
Limit Theorem</b> about the sampling distribution of the mean.
---
 It says that even if you do not know the  population distribution,
but just know its mean and standard deviation, ... then you
basically know the distribution of the sample mean!

</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
CLT statement.
</fieldset>

</div>

<fieldset>
<legend>m1l2_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     A population has mean 15 and standard deviation 3. A random
  sample  of size 100 is drawn from it. Assuming this sample size
  to be large enough, what is the (approximate) sampling
  distribution of the sample mean?
  defaultFeedback: &gt;
      CLT says that the distribution is approximately normal. The
  mean is same as the population mean, 15. The standard deviation
  is population standard deviation divided by square root of
  sample size, ie, 3/10.
</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.20</i></legend>
Let's look at a practical example. <b>Confidence interval</b>. Let us see ... the
mathematical formulation.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
CI example.
</fieldset>

</div>


<fieldset>
<legend>m1l2_f.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     A machine produces pipes of a certain length that may be
  specified by a setting in its dashboard. We know from prior
  runs of the machine that standard deviation of the lengths
  produced by the machine is 0.5 cm. We have collected a random
  sample of size 25 from the production line. The sample mean
  length is 100.2 cm. Obtain a 95% CI for the population mean (ie
  the dashboard setting, which we assume unknown to us).

  defaultFeedback: &gt;
        \frac ./exraux3/cisol.png 
   Notice the formula bar (the input line).

</pre>
</fieldset>



<h2><a
name="Module 1: Lesson 3: Estimating proportions">Module 1: Lesson 3: Estimating proportions</a></h2>

<b>Total lesson duration
= 25</b>
<p></p>

<div class="header">
<h3><a
name="Module 1, Lesson 3, Video 1: Estimating proportions">Module 1, Lesson 3, Video 1: Estimating proportions</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.00</i></legend>
In the first lesson of this week we had learned about the concept
of estimation. We are now learning how to apply this concept to
various parameters of interest.
---
 In the last lesson we learned how to estimate
the population mean. In this lesson we shall estimate <b>population proportion</b>. 
---
First let's understand the importance of the problem with a real
life example. Suppose that an <b>election</b> is imminent in a
country. There are two major contending parties. Each striving
its best to win the election.
---
 What will they not give to know the result of the election
beforehand!  If only they knew where their
support base is weaker, they would be able to focus more effort
in those regions.
---
 In abstract terms this is the problem of estimating a proportion, 
the proportion of the people who would vote for that
party. <b><font color="red">
[[proportion of supporters]] </font></b>
---
As yet another application, consider  a <b>casino</b>. They use
various gambling devices there, slot machines, roullette wheels
etc. They need to be very carefully tuned in order to rake money
for the house.
---
 For instance, if a roullette wheel is slightly
tilted resulting in certain numbers showing up more often and the
gamblers get a hint of the that, then that means ruin for the
house.
---
 So the casino owner has to keep an eye on the unbiasedness of
his devices. How can he do that?  Let's consider the simplest
gambling device, a coin<b><font color="red">
[[actually show]] </font></b>.
---
 Given a coin how do you find its <b>probability of
showing head</b>? That is an example of a population proportion. Unlike the radius, thickness or mass of the coin,
its probability of head is not a directly observable feature. 

That's where estimation of a population proportion becomes important.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">A simple solution</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=2.20</i></legend>
How would you estimate the <b>probability of head</b> of a coin? What is
the commonsense method? Just toss the coin a large number of
times and find the proportion of times you get head.
---
 Say I toss the coin <b>1000</b> times, and find <b>465</b> heads. Then I estimate the
probability of head as <b>$\frac{465}{1000]=0.465.$</b> Of course,
what you get in this way is the <b>sample proportion</b>.
---
 But if the sample is large enough, then it should be close to the population
proportion, thanks to statistical regularity.
---
Similarly for the election problem we can draw a sample of voters
and try to determine their political leanings. Then find the
proportion based on the sampled voters.  
---
There is another equivalent way of looking at estimating
population proportion that may be understood using this voter
example.
---
 Associate a number 0 or 1 with each voter in the
population according as the voter is against or for the party in
question.<b><font color="red">
[[shown]] </font></b> This creates a new variable.
---
 Notice that the population
mean of this variable is precisely the proportion we are trying
to estimate. Because when you sum, you are basically counting the
1's. 
---
Thus estimation of population proportion is just a
special case of estimation of population mean. So we may use
sample mean of this variable as before. It is easy to see that
this sample mean is just the sample proportion. 
---
The advantage of this line of thought, considering proprtion as a
special case of mean, may not be readily apparent. The main
advantage is that  sample proportion inherits all the
properties of the sample mean.
---
 This will make our life easier when we shall try to work with the sampling distribution of
sample proportion.

But first let's look at a little hands on computation.
</fieldset>

</div>

<fieldset>
<legend>m1l3_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If a variable takes only the values 0 and 1, then  what is
  the sum of squares of the values minus the sum of all the values?
  defaultFeedback: &gt;
      Just zero. Because 0^2=0 and 1^1=1. 
</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Voting data already present. Draw a Sample. Find mean after ifelse. Show lots
of proportions and histogram already in different sheets. 
</fieldset>

</div>

<fieldset>
<legend>m1l3_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Sample proportion based on a random sample is a categorical
  variable. Is this statement correct? 
  defaultFeedback: &gt;
      No. In principle, it can take all values between 0 and 1.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 1, Lesson 3, Video 3: Lab">Module 1, Lesson 3, Video 3: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.80</i></legend>
The lab session in the last video gave us some idea about the
sampling distribution of the sample proportion. As in the case
sample mean, a more quantitative idea may
be had using its <b>bias</b> and <b>standard error</b>.
---
 We know  that the bias of an estimator is the <b>expectation of
the estimator - parameter</b>. In 
general, this may not be computable, since the parameter is
unknown.
---
 But in certain special cases the bias may be computed
indirecly via mathematical reasoning even without knowing the
value of the parameter. This was the case for sample mean, and
this is also going to be the case with sample proprtion.
---
 And that is hardly unexpected because as we have already
mentioned  sample proportion
is a special case of sample mean, it is the mean of a 0-1 veriable. 
---
Sample mean was <b>unbiased</b> for population mean. So here sample
proportion is unbiased for population proportion. Thus, while the
sample proportion is not guaranteed to equal the population proportion
but it is likely to hover centred around the true value.
---
Next let's turn to the <b>standard error</b>. This is the standard
deviation of the sampling distribution of the sample
proportion. Again we shall invoke the corresponding result for sample
mean.
---
 There the standard error was 
<b>$$
\frac{\sigma}{\sqrt n},
$$</b>
where $\sigma $ is the population standard deviation,
and $n$ is the sample size.
---
 Now our variable takes only the values 0 and 1. 
So $\sigma$ may be... simplified further. 
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=3</i></legend>
Let
the population proportin be $\theta.$ Then $\sigma $ is 
$$
\frac1N\sum x_i^2-(\bar x)^2 = \theta - \theta^2 = \theta(1-\theta).
$$
Becuase $x_i^2 = x_i$ since $x_i$ is either 0 or
1. Thus the standard error is 
$$
\frac{\theta(1-\theta)}{\sqrt n}.
$$
</fieldset>

</div>

<fieldset>
<legend>m1l3_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     A random sample of 100 items has  exactly 23 defective
  items. Estimate the population proortion od defectives as well
  as the standard error of the estimator.
  defaultFeedback: &gt;
      Estimate for population proortion od defectives is just the
  sample proportion 23/100.
      Its estimated standard error is 0.23*(1-0.23)/10.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 5.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.80</i></legend>
We have a large store full of items,<b> $\theta$ proportion
of which are defective</b>. This $\theta$ is unknown. We want to
estimate it.
---
 For this we have drawn a <b>random sample of 100</b> items,
and found <b>73 defective</b> items. This gives us a point
estimate $\frac{73}{100}.$<b><font color="red">
[[anim]] </font></b>
---
 But we want a $95\%$ CI, ie,
two numbers <b>$L$ and $U$</b> based on our sample such that 
<b>$P(L\leq \theta \leq U ) = 0.95.$</b> Let us see how the central
limit theorem ... helps us here.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>

</fieldset>

</div>

<fieldset>
<legend>m1l3_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Repeat the estimation process shown in the video if the
  sample size is 200, and the number of defectives in it is 34. 

  defaultFeedback: &gt;
      
</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 1, Lesson 3, Video 2: Lab">Module 1, Lesson 3, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.60</i></legend>
In case of estimating population  mean we had used
sample mean. Now sample mean is not robust against outliers. So
if we suspect presence of outliers, a robust altenative like
trimmed mean may be preferable.
---
Now sample proportion is a special case of sample mean.
But we do not have to worry about outliers here, because here our
variable takes only the values 0 and 1 by design. So no question of
extreme values creeping in.
---
 However, there is a problem from a
different direction, as we are about to discuss. 

We had been pretty close to this problem when we talked about the
election scenario.
---
 There our suggestion was to sample some voters
and find out their political leanings. Now a voter is entitled to
privacy. He/she may not agree to reveal his/her political
leaning.
---
 Even worse, a voter may simply lie. This poses a
different problem, the problem of respondents trying to hide
sensitive information. 

The  problem also occurs in a rather different context. 
---
Suppose we have a <b>multiple choice question with 4 options</b>
exactly one of which is known to be correct. We want to estimate
the proportion of students who know the correct answer.
---
 If we
simply take a sample of students, ask the question, and find the
proprtion of correct answers, then that won't be a good estimator
[qn: biased up or down?] as a student may have answered correctly
by chance.
---
So here we need to model the students' thought process. Either a student
knows the answer or not.<b><font color="red">
[[tree shown]] </font></b> In the first case, she answers
correctly<b><font color="red">
[[shown]] </font></b>, but in the second case she makes a
guess<b><font color="red">
[[shown]] </font></b>, 
say a random
guess<b><font color="red">
[[tree grows]] </font></b>.
---
Let the true probability of her knowing the
correct answer be $\theta.$<b><font color="red">
[[shown]] </font></b> So this is $1-\theta.$
When she guesses,  she has $\frac 14$ chance of getting the
correct answer<b><font color="red">
[[shown]] </font></b>, and $\frac 34$ of getting it wrong.
---
 Then the probability of her
giving the correct answer is <b>$\theta + (1-\theta)/4.$</b>
<b><font color="red">
[[finger expl]] </font></b> It is
this thing that is estimated well by the sample
proportion, $p$ say, of correct answers. 
---
So we may say 
<b>$$
\theta + \frac 14(1-\theta) \approx p,
$$</b>
hence <b>$\theta\approx \frac{4p-1}{3}.$</b>
---
This is not entirely intuitive. Also, if this turns out to be
negative, we should of course take the estimated $\theta $ to be 0. 
---
 A similar
application of this idea is used to estimate population
proportion of sensitive issues like political leanings. The
method is called randomised response.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Randomised response</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.30</i></legend>
<b>Randomised response</b> refers to situations where the respondents
add a layer of extra randomness to hide the truth. The student
guessing in an MCQ was one example where the randomisation was
unplanned. Here is a planned version of the same.
---
 Again consider the election scenario with two contending parties. We pick a
random sample of voters and ask them: <b>"Will you vote for party
A?"</b> But in order to protect their privacy you also hand them a
fair die<b><font color="red">
[[shown]] </font></b>...,
---
 ask them to roll it in private, and <b><font color="red">
[[scheme shown]] </font></b> answer truthfully if
the die shows 6, and lie if the die shows something else.
---
 Thus, let's say I am a voter favouring party A.<b><font color="red">
[[finger]] </font></b> I
roll my die, get a <b>5</b>, 
so I answer "No". My friend is against party A, she gets a 6, and
she also answers "<b>No</b>". 
---
Let's see how we may estimate the proportion of A supporters from
the responses we thus obtain.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Estimation</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.00</i></legend>
 A voter is either for or against party A, with probabilities $\theta$
and $(1-\theta),$<b><font color="red">
[[tree top shown]] </font></b>
respectively.
---
 Suppose he is for A. <b><font color="red">
[[finger]] </font></b> Then the die may show a 6 or not,
probabilities $\frac 16$ and $\frac 56.$<b><font color="red">
[[tree bot left
shown]] </font></b> Similarly if he is against A.<b><font color="red">
[[bot right shown]] </font></b>
---
Accordingly there
are two ways one may answer "Yes".<b><font color="red">
[[finger]] </font></b>
---
 The total probability is 
<b>$$
\theta \times \frac 16 + (1-\theta)\times \frac 56.
$$</b>
This is estimated by the sample proportion of "yes".
---
 If this
proportion is $p,$ then 
<b>$$
\theta \times \frac 16 + (1-\theta)\times \frac 56\approx p.
$$</b>
So <b>$\theta \approx \frac{5-6 p}{4}.$</b>
---
This is called <b>Warner's method</b>. There are different variations of
the same idea in use.
</fieldset>

</div>


<fieldset>
<legend>m1l3_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider an MCQ with 4 options with exactly one correct.
  Unknown proportion p of students know the correct answer, the
  others guess one of the options randomly. If in a random sample
  of 100 students 86 have got the correct answer, then what will
  be an estimate of p?
  defaultFeedback: &gt;
      p + (1-p)/4 is approximately 86/100. So p is approximately 0.81.

</pre>
</fieldset>




<h2><a
name="Module 1: Lesson 4: Estimating dispersion">Module 1: Lesson 4: Estimating dispersion</a></h2>

<b>Total lesson duration
= 27.1</b>
<p></p>


<div class="header">
<h3><a
name="Module 1, Lesson 4, Video 1: Estimating dispersion">Module 1, Lesson 4, Video 1: Estimating dispersion</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.60</i></legend>
We are continuing with our plan of estimating various parameters
following the idea presented in the very first lesson of this
module. We have already learned how to estimate population mean,
and population proportion.
---
 In this lesson we shall attack
<b>population dispersion</b>. Now dispersion is just an intuitive
concept, it cannot be a parameter. We need some conrete measure
of dispersion to act as our parameter.
--- 
<b>Population variance</b> and <b>population standard deviation</b> are the two
most popular choices.
---
The need for this estimation could be felt even when we learned
to estimate population mean using sample mean. There the standard
error was <b>$\frac{\sigma}{\sqrt n}.$</b>
---
Here $\sigma $ is the population standard deviation,
and $n$ is the sample size.

 This quantitiy gave us an idea about the precision of our estimator. But how
can it be useful, because it involves $\sigma,$ which is
unknown? 
---
So we need to estimate $\sigma$ or
equivalently $\sigma^2.$ 
---
Throughout this lesson keep this picture at the back of your mind.
There is an underlying
distrbution (the unknown truth). It's best to think of it
as the shape of a histogram. The spread of this histogram is what
we have in mind.
---
 A population with low  scatter is often easier to
work with, as all the members are more or less similar. A
population with wide scatter requires much more effort to
explore. 
---
So prior to any detailed exploration of a
population, we need to estimate its variance to get an idea of
how much work lies ahead.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">A simple solution</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.30</i></legend>
We already know a simple solution from our experience so far with
estimating population mean and proportion. Just use the sample
analog.
---
 By the way, this idea of estimating a population
parameter by using the corresponding sample quantity has a name. It
is called the <b>plugin principle</b>. 
---
It is justified on the ground
that the sample histogram closely resembles the population
histogram. Here this principle suggests
using sample variance as an estimator of population
variance, and...
---
 sample standard deviation as an estimator of
population standard deviation. 
---
By the way, we had noticed earlier in the Basic Statistics course
that there are two alternative definitions of sample variance
(and hence of sample standard deviation). One is<b><font color="red">
[[move]] </font></b> 
<b>$$
\frac 1n\sum (x_i-\bar x)^2
$$</b>
and the other is 
<b>$$
\frac{1}{n-1}\sum (x_i-\bar x)^2.
$$</b>
---
LibreOffice, as well as most other standard statistical softwares,
use the second formula by default. Earlier we had glossed over this point
in our basic Statistics course. But here we shall give a more detailed
exposition. 
---
But first it is time to go to the lab in the next video.
</fieldset>

</div>


<fieldset>
<legend>m1l4_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     A botling plant is supposed to produce bottles of  a fixed
  weight. But due to inherent randomness in its mechanism there
  is some variation. Let the population variance be s. Estimate s
  from the random sample:
    10.3, 10.2, 9.8, 9.9, 9.9, 10.0.
  Use the formula with n-1 in the denominator.
  defaultFeedback: &gt;
      Sample variance is 0.0037, and this is a reasonable
  estimate of the population variance.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 1, Lesson 4, Video 2: Lab">Module 1, Lesson 4, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Normal data already present. Draw a Sample. Find variance. Show lots
of variances and histogram already in different sheets. Mention how
adding a constant to population shifts leaves the result unaffected.
</fieldset>

</div>

<fieldset>
<legend>m1l4_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     The video  above showed the sampling distribution of sample
  variance as a histogram. What can you say about its skewness?
  Positive or negative or zero?
  defaultFeedback: &gt;
      Positive.
</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 1, Lesson 4, Video 3: Sampling distributions">Module 1, Lesson 4, Video 3: Sampling distributions</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.10</i></legend>
The lab session in the last video gave us some idea about the
sampling distribution of the sample variance. A more quantitative idea may
be had using its bias and standard error.
---
 The bias of
an estimator is the <b>mean of the estimator - parameter</b>. In
general, this may not be computable as a number, since the parameter is
unknown.
---
 But in case of sample mean as well as sample proportion it
turned out to be zero. So there the estimators were unbiased. But
the situation somewhat different for sample variance.
---
 For one thing the estimator 
<b>$$
\frac 1n\sum (x_i-\bar x)^2
$$</b>
is not unbiased.
---
 We really cannot compute the bias as a
number, only express it as a formula involving the unknown parameter.
Let's do so.
---
The expected value of this
estimator is 
<b>$$
\frac{n-1}{n}\times \sigma^2.
$$</b>
---
So the bias is <b>$-\frac 1n \sigma^2.$</b>

Let's prove this fact.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Proof</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Proof
</fieldset>

</div>


<fieldset>
<legend>m1l4_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the sample variance with n in the denominator. We
  have learned that it is a biased  estimator of population
  variance. Is it an underestimate or an overestimate?
  defaultFeedback: &gt;
      Underestimate, because the bias is negative. See the video
  for the exact expression of bias.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 1.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.70</i></legend>
The proof of this fact <b><font color="red">
[[shown]] </font></b>
that we saw in the last video may be too technical
for your taste. 
---
It has one implication, however, that may 
be appreciated without going too deep into mathematics.
---
Notice that the expected value<b><font color="red">
[[finger]] </font></b> is a known multiple
of $\sigma^2.$ This $\frac{n-1}{n}$ is known because the
sample $size$ $n$ is known.
---
 Also, it is not random, though the sample itself is random,
because the sample size is known even
before the sample has been drawn.
---
  So we may just divide our estimator by this known constant
to make the expected value exactly equal to $\sigma^2.$ When
we do this we arrive at
<b>$$
\frac{1}{n-1}\sum (x_i-\bar x)^2.
$$</b>
---
So now you know where that alternative estimator came from.

Next we should turn to the standard error. Unfortunately, the
formula is rather complicated. So we shall not torture ourselves
with that formula here.
---
 However, as we may guess, the standard
error will go down as the sample size increases.

Incidentally, it might interest you to know the shape of the
sampling distribution.
---
 Even though we assume a nice symmetric
bell shape for the population, the sampling distribution is
asymmetric, or skewed to the right, to use a precise mathematical
term. We have already seen the shape in the last lab session.
</fieldset>

</div>

<fieldset>
<legend>m1l4_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     We have a random sample from a population: 
       2.3, 4.2, 2.1, 3.2.
  Estimate the variance of the sample mean.
  defaultFeedback: &gt;
      The variance of sample mean is sigma^2/n = sigma^2/4, where
  sigma^2 is the population variance. Now an
  unbiased estimator of sigma^2 is sample variance with n-1 in
  the denominator: 0.92. So the answer is 0.92/4 = 0.23.

</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 1, Lesson 4, Video 4: Alternatives">Module 1, Lesson 4, Video 4: Alternatives</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.40</i></legend>
Sample variance (whether with an $n$ or a $n-1$ in the
denominator) is a natural estimator of population variance.
But it suffers from lack of robustness.
---
Suppose that there is a little spike in our underlying distribution.<b><font color="red">
[[shown]] </font></b> 
A distribution, as we have mentioned earlier, is basically the
shape of a histogram.
---
The spike<b><font color="red">
[[finger]] </font></b> indicates a little contaminating
value, an extreme value that just does not fit with the rest of
the values.
---
Now because it is just one bad point as opposed to many many good
points, the population variance is not influenced much by it. But imagine
what may happen when you draw a random sample of much smaller
size.
---
 If that spike somehow gets into your sample it might wreak
havoc. Because there are only a handful of good values to counter
it. As a result the sample variance will move far from the
population value. 
---
So we need something more robust. Something that does not care
much about extreme points. <b>Median Absolute Deviation (MAD)</b>
is one good option. Let's quickly recall the definition
and computation of MAD.
</fieldset>

<fieldset><legend
    class="cu">
[r] <i>Duration=3.5</i></legend>
[Computation of MAD]
</fieldset>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.90</i></legend>
 However, it is rather difficult to compute its bias, because its formula
involves abolute value, and that makes mathematical computation of its
expected value difficult. 
---
Now there is one point that must be bourne in mind. Our aim is to
estimate population dispersion, and not necessarily population
variance.
---
 Do not consider MAD as an estimator of population
variance or population standard deviation. Think of it as an
estimator of population MAD. 
</fieldset>

</div>


<fieldset>
<legend>m1l4_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider this data set \frac ./exraux3/compare.csv.
  Compute both MAD as well as sample standard deviation (using
  either n or n-1 in denominator). Compare them. What do you
  conclude about the data?
  defaultFeedback: &gt;
      SD = 98 and MAD = 1. Surely there is some outlier somewhere!

</pre>
</fieldset>

<font color="red">
<pre>
cw('exraux3')
set.seed(35343)
value = trim(rnorm(100,mean=20))
value[43] = 1000
csv(value,'compare.csv')
MAD = median(abs(value-median(value)))
SD = sqrt(var(value))
</pre>
</font>

<div class="scrpt">
<b>Total video duration
= 4.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.30</i></legend>
In the last video we talked about using sample MAD to estimate
population dispersion, as captured by the population MAD. 
Similar is the case with
<b>interquartile range</b>. Let us quickly recall its computation. 
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=4</i></legend>
Show IQR computation.
</fieldset>

</div>



<fieldset>
<legend>m1l4_f.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Compute IQR from this data set \frac ./exraux3/compare.csv.  
  defaultFeedback: &gt;
      21 - 19 = 2.

</pre>
</fieldset>



<fieldset>
<legend>m1_add.yml</legend>
<pre>
- typeName: peerReview
  prompt: &gt;
    Consider the data set \frac ./exraux3/length.csv. These are
  lengths of a random sample of bolts taken from a production
  line. Estimate the population mean length and also find an
  estimate of the standard error of its estimate. Show the
  formulae you are using.

  Rubric:
   1) Mean = 5.209 cm. (0.5 point)
   2) Standard error = sigma/sqrt(100) = sigma/10 (1 point)
   3) Estimated sigma = 0.04 (0.5 point)

- typeName: peerReview
  prompt: &gt;
    Consider a huge population where every person is either left
  handed or right handed. We draw a random sample of
  size $n$ from this population and report a variable
  "handedness" that takes the values 0 and 1, respectively, for
  "right" and "left". We want to estimate the population variance
  of this quantity. Suggest (with proof) an unbiased estimator of
  this. Simplify as much as as possible.

  Rubric:
   1) Sample variance (1 point)
   2) Simplification: (1 point)
        (p - p^2)*n/(n-1), where p is sample proportion

- typeName: peerReview
  prompt: &gt;
    A random sample of size 100 drawn from a production line
  contains 30 defective items. Use CLT to find an approximately 95%
  confidence interval for population proportion of defectives.

  Rubric:
    1) [0.3 - a, 0.3 + a] form (1 point)
    1) a = 0.3*(1-0.3)/10 * 1.96 (1 point)

- typeName: peerReview
  prompt: &gt;
    If the standard error of the mean of a sample of size 100 is
  0.4, then what minimum sample size would you need to sample in order
  to achieve standard error less than or equal to 0.1?

  Rubric:
    1) Formula for standard error (1 point)
    1) 0.4/0.1 = 4. So need to make sample size 4^2 = 16
    times. So required sample size is 1600 (1 point)

- typeName: peerReview
  prompt: &gt; 
    Suppose that we estimate the minimum value in a population by
  the minimum of a random sample drawn from it. Then what can you
  say about the sign of the bias? Justify your answer.

  Rubric:
    1) Formula for bias  (1 point)
    2) Sample min can never be less than population min. So bias
    must be nonnegative (1 point)
</pre>
</fieldset>


<h2><a
name="Module 1: Lesson 5: Review">Module 1: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 1, Lesson 5 , Video 1: Review">Module 1, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 1, Lesson 5 , Video 2: Review">Module 1, Lesson 5 , Video 2: Review</a></h3>
</div>



<h1><a
name="Module 2: Test of hypotheses">Module 2: Test of hypotheses</a></h1>

<div class="scrpt">
<b>Total video duration
= 0</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0</i></legend>
In the first week we learned about estimation. Guessing the value
of unknown parameters. We learned about two types of estimation:
point estimation where our guess for a parameter value took the
form of a single number, and confidence intervals...
---
 where we provided an interval as our guess, like the parameter will lie
between this and that value with 95% probability.
---
In this module we shall tackle something less ambitious. We shall
not be directly concerned with finding the value of a
parameter.
---
 We shall remain content with answering just yes-or-no
type questions about a parameter. Like I do not know your age, nor
do I want to know it.
---
 I am just curious if you are are below 30
or not. That's all. A simple yes or no. The technique we shall
employ for guessing answers to such questions based on data 
is called <b>test of statistical hypotheses</b>.
---
And that's what we shall learn this week!

The week's structure will remain as before. 4 lessons, practice
problems, and finally a unit test.
OK, let's start with our first lesson!
</fieldset>

</div>

<h2><a
name="Module 2: Lesson 1: Concept of test">Module 2: Lesson 1: Concept of test</a></h2>

<div class="header">
<h3><a
name="Module 2, Lesson 1, Video 1: The concept of test (sound of water)">Module 2, Lesson 1, Video 1: The concept of test (sound of water)</a></h3>
</div>

<b>Total lesson duration
= 27.3</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 5.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.50</i></legend>
In this module we shall learn about <b>test of statistical
hypotheses</b>. What is that? You may ask. Well, it's a long story
with many details that might scare beginners away.
---
 We shall go into
those details later. For now let's start with a simple
non-mathematical example. 
---
Here I have a sealed container, and I want to know if it is half
filled with water. How do I ascertain that? Remember: my aim is
not to find the actual content of the container,...
---
 but just to
provide a yes/no answer to the question: Is it partly filled with
water?

Why not just open it and see?
---
 Well, it  is sealed, and I
cannot open it. So guess work is my only option. 
---
Let's start by feeling  the weight. It's not empty. But it could
be water or wood or something else.
---
 Let's shake it, and listen to the sound. It does not sound like water at all. It actually
sounds like...any way who cares! That it does not sound like
water is all I need to know. That's enough info for me to
conclude that it cannot be half filled with water.
---
Test of statistical hypotheses is just a formal way of carrying
out this same common sense procedure.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">More formally</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.40</i></legend>
The set up started with two possibilities: either the container
is <b>partly full of water</b> or <b>not that</b>. In statistical parlance we call
these  two <b>hypotheses</b>. 
---
One of them is called the <b>null hypothesis</b>
the other is called the <b>alternative hypothesis</b>. We shall learn
later how to choose which one to call the null and which one to
call alternative.
---
 But to keep going let's say "partly filled with
water" is our null hypothesis. We denote the null hypothesis by
the symbol <b>$H_0$</b> and the alternative one
by <b>$H_1$</b>. 
---
The two hypotheses must cover all possibilities, and must not
overlap. Thus, it should not be possible to have a situation
where neither hypothesis holds or both the hypotheses
hold. 
---
 Deciding in favour of one of these hypotheses is called a
<b>test of hypotheses</b>. Opening the container and peeping inside
would have given the answer for sure, but that was impossible by
the rule of the game.
---
 So guessing based on imperfect indirect
knowledge is the only option. Hence we have <b>statistical</b>
hypotheses testing.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Test statistic</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=2.00</i></legend>
When we picked up the container, felt its weight and gave it a shake,
and listened to the sound,
we were collecting <b>data</b>, some useful and some not. The weight did
not help us at all, but the sound did.
---
 The relevant aspect of the data is called the <b>test
statistic</b>. 
Here we meet the term
statistic again, a quantity computed based on the sample.
---
 Since
it is being used for testing, so the term test statistic. Its
choice crucially depends on the two hypotheses we are trying to
distinguish between.
---
 For instance, had we tried to distinguish
between water and milk, sound would have been of little value,
because either would have sounded about the same.
---
  If the two
hypotheses were water versus empty, then weight itself would have
provided enough clue.
---
The test statistic should be some aspect of the data that behaves
very differently under the two hypotheses. In order to decide if
the null hypothesis is true we need to know the behaviour of the
test statistic under the null hypothesis,...
---
 ie. the sound of water
in our toy example. This is called the <b>null distribution</b> of the
test statistic.
---
We compare the observed value of the test statistic with its
null distrbution. If the observed value is too extreme compared
to what is expected for the null distrbution, then we naturally
cast our vote in favour of the alternative hypothesis.
---
 This decision is customarily called <b>Rejecting $H_0$</b>.
If, on the other hand, the observed value
of the test statistic is nothing incongruous with the null
distrbution, then we <b>Accept $H_0.$</b>

</fieldset>

</div>

<fieldset>
<legend>m2l1_a.yml</legend>
<pre>
- typeName: multipleChoice
  prompt: &gt;
We have a random sample of patients. We have found the
proportion of COVID infections among them. Which of the
following is a valid hypothesis to be tested?      
  defaultFeedback: &gt;
      Remember the hypothesis is about the unknown population
  quantities, the parameters, and not the known sample quantities.
  shuffleOptions: true
  options:
  - answer: The population proportion is 0.5
    isCorrect: true
    feedback: &gt;
       Yes, a hypothesis must be completely in terms of
  parameters, and not any sample quantity.
  - answer: The sample proportion is 0.5
    feedback: &gt;
      
  - answer: The  sample proportion is less than the population proportion
    feedback: &gt;
</pre>
</fieldset>      


<div class="header">
<h3><a
name="Module 2, Lesson 1, Video 2: The concept of test (sound of water)">Module 2, Lesson 1, Video 2: The concept of test (sound of water)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.60</i></legend>
In the last video we talked about comparing the observed value of
the test statistic with its null distribution, ie, how the test
statistic is expected to behave if the null hypothesis were true.
---
This comparison may be done in a couple of ways:

* using <b>p-value</b>
* using <b>critical value</b>
---
The p-value technique is what is used by most standard
statistical softwares and is the easier to explain. So we shall
start with that.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">p-value</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.40</i></legend>
In the Basic Statistics course we had already discussed <b>p-value</b> as a means of checking if a
given value is too extreme compared to a bunch of numbers. This
is precisely what we need here.
---
 We know the <b>null distribution</b>,
ie, the type of values that the test statistic is expected to
take if the null hypothesis were true.
---
 And we are to compare the
<b>observed value of the test statistic</b> with these. If the value is
too extreme, then we shall reject $H_0$, else we shall
accept $H_0.$ 
---
As we have already learned in the Basic Statistics course, the
p-value is basically the chance that the test
statistic may be more extreme  than the observed value assuming
the null hypothesis is true.
---
 If this is small, then the observed
value is already too extreme. and we reject $H_0.$ Clearly
smaller p-values favour rejection of $H_0.$
---
 How small is
small enough? A commonly used threshold is <b>5%</b>. If you want to be
more liberal use 1%. Whatever threshold you use is called
the <b>level of significance</b> of the test. 
---
It is possible to use a mathematical curve
to find p-values. Let's understand this...using pictures.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
Show points along a numberine explain p-value. Draw
histogram. Approx by curve. Explain by area.
</fieldset>

<fieldset>
<legend>
(s3)[a] <i>Duration=0.50</i></legend>
Thus the pvalue technique for test of statistical hypotheses is
to <b>compute the test statistic</b>, <b>compute the p-value</b> and
reject/accept $H_0$...
---
 according as the p-value is below or
above 0.05<b><font color="red">
[[shown]] </font></b>. Statistical softwares generally just give you 
the p-value, leaving the choice of the cut off to you.
</fieldset>

</div>


<fieldset>
<legend>m2l1_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     What do you do if the p-value is above the level of significance? 
  defaultFeedback: &gt;
      You accpet the null hypothesis.
</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 2, Lesson 1, Video 3: Lab">Module 2, Lesson 1, Video 3: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.20</i></legend>
In this video we shall take a closer look at the concept of
p-value in the context of test of hypotheses.
</fieldset>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Toy example: start with data and some arbit test
statistic.  Give lots
of typical values in a different sheet. Find p-value based on
them. Accept or reject. Mention that this is an exploded view,
not recommended in practice.
</fieldset>

</div>


<fieldset>
<legend>m2l1_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If the cut-off for p-value is raised from 5% to 10%, am I
  being more liberal towards accepting the null hypotheses? 
  defaultFeedback: &gt;
      No, just the opposite. A p-value of 0.07 would now lead to
  rejection of the null hypotheses, while earlier it would have
  led to acceptance.

</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 2, Lesson 1, Video 4: Critical value method">Module 2, Lesson 1, Video 4: Critical value method</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.80</i></legend>
The <b>p-value method</b> is simple, but leaves one important question
unanswered. How do we choose the test statistic? This
question is not terribly important for us in this course,...
---
 because we shall be using standard softwares where the test statistic is
already built in. But still it is good to have an idea. And that
is where the <b>critical value method</b> will help us. 
---
Incidentally, it will also expose us to some more statistical
jargon. 

In a statistical hypothesis testing scenario our conclusion is
based only on indirect evidence, and hence liable to
errors. There are two types of errors. Let's discuss them.
---
 There are two hypotheses that cover all possibilities without any overlap
<b><font color="red">
[[H0 and H1 shown in column headed truth]] </font></b>. Accordingly we have two possible verdicts:
<b>accept $H_0$ and reject $H_0$</b><b><font color="red">
[[shown as row
with head verdict]] </font></b>.
---
So we have a tabular structure with 4 cells.<b><font color="red">
[[shown]] </font></b> These two cases
correspond to correct decisions.<b><font color="red">
[[finger]] </font></b> $H_0$ correct, and we have
rightly accepted it. Here $H_0$ does not hold, and we have
indeed rejected it.
---
 These two cases correspond to errors. Here we
have rejected $H_0$ even though it is actually correct, and here
we have accepted a wrong $H_0.$ These two are called type
I and type II errors.<b><font color="red">
[[shown]] </font></b>
---
 Notice that if we swap the
labels $H_0$ and $H_1$, then the  types are also
swapped. The convention is to call the <b>more serious</b> of the two
errors Type I, an accordingly label the hypotheses as $H-0$
and $H_1.$
---
Let's understand this with an example.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">An example</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.20</i></legend>
Suppose that I have a tumour in my hand. I am worried about its
being cancerous. So there are two possibilities: it is cancerous
or it is not. These are my two hypotheses.
---
 I visit a doctor, who
makes diagnostic measurements (ie collects data) and either says
"Yes, it is cancer" or "No, it isn't".<b><font color="red">
[[table shown]] </font></b> Now let's look at the two
errors.
---
 Here<b><font color="red">
[[finger]] </font></b> I do not have cancer, but the doc says cancer. So I
pass a few sleepless nights, curse my fate, and spend some money
to get my harmless tumour treated.
---
 Now let's see the consequence
of the other error. I do have cancer, but the doc says, "Don't
worry, you're just fine!" I feel very happy, go home, cancer
spreads, and RIP.
---
I hope you'll agree that the latter error is the more serious of
the two. So we shall call that our Type I error. Now type I error
is the error of rejecting a correct $H_0.$ So we label the
two hypotheses accordingly. 
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Level of significance</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.20</i></legend>
Since Type I error is the more serious, we naturally want to
guard against that first. We set an upper bound on the
probability of committing that error. This upper bound is called
the <b>level of significance</b> or the <b>size</b> of the test.
---
 Common choices are 5% and 1%. Subject to this we choose a test that minimises
the probability of the other error. This minimisation dictates
the choice of the test statistic, as well as of the cut off
value, or <b>critical value</b>, as it is called.
---
In this approach we have to  compute the test statistic from the data and
compare it against the critical value. Depending on the side of
the critical value our test statistic lands, we either accept for
reject $H_0.$
---
However, in our course we do not need to worry about all
these. The softwares already know the best test statistic to use
in each case, and so the simpler p-value approach will give the same
result as the more ambitious critical value approach.
</fieldset>

</div>


<fieldset>
<legend>m2l1_d.yml</legend>
<pre>
- typeName: multipleChoice
  prompt: &gt;
      If the level of significance of a test is 0.05, this means:
  defaultFeedback: &gt;
    Remember that level of significance is the maximum
  permissible probability of rejecting a correct null hypotheses.      
  shuffleOptions: true
  options: 
  - answer: &gt;
  If the null hypothesis is true then the chance of
  rejecing it is at most 5%.
    isCorrect: true
    feedback: &gt;
      
  - answer: The chance of making a wrong decision is at most 5%.
    feedback: &gt;
      
  - answer: &gt;
    If the null hypothesis is false, then the chance of accepting it is at
    most 5%.
    feedback: &gt;
      
</pre>
</fieldset>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
A huge lot of items. Proportion of defective items is \theta
unknown. 
The lot is is good if H_0: \theta \leq 0.2  vs H_1: \theta &gt; 0.2. Accept a
lot based on a sample 
of 10 iff all ok. Compute type I and type 2 prob.
</fieldset>

</div>


<fieldset>
<legend>m2l1_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the scenario shown in the video. But now we have the
  null hypothesis theta = 0.5. A sample of size 2 is drawn. We
  accept it if only if at least one item is ok. Compute type I error probability.
  defaultFeedback: &gt;
      We reject if and only both the items are defective. Under
  the null hypothesis the chance of this happening is 0.5*0.5 =
  0.25, which is the required answer.


</pre>
</fieldset>



<h2><a
name="Module 2: Lesson 2: One sample t-test">Module 2: Lesson 2: One sample t-test</a></h2>


<div class="header">
<h3><a
name="Module 2, Lesson 2, Video 1: one-sample t-test">Module 2, Lesson 2, Video 1: one-sample t-test</a></h3>
</div>

<b>Total lesson duration
= 22.3</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 6.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.00</i></legend>
There are many different types of tests of statistical hypotheses
suited for different purposes. We shall talk about quite a few of
them this week.
---
 Let's start with the one that is possibly
the easiest to understand and has wide application. We shall
start with an example. 
---
We all use bottled liquids, milk, shampoo, oil, medicines. The
container bears a label telling us the amount of the content. How
does the manufacturer ensure that the amount is as it should
be?
---
 Of course, if we fill each bottle manually we may
painstakingly measure out the right amount and pour it in. But
these bottles are not packaged manually. The amounts are poured
out by some automated device, and churned out by the hundreds.
---
 If some setting somewhere goes off a bit, it will have a disastrous
effect. So the manufacturer has to keep a constant eye on the
process. And test of statistical hypotheses plays the role of
that eye.
---
First, we must understand that even when the bottling plant is working
satisfactorily, there is bound to be some inevitable random variation among the
amounts of the contents in the bottles. 
---
 The extent of this variation is typically a property of the machine
itself,  its precision, and may be expected to hold through out. 
But even with the precision in place, the setting may get shifted 
over time. And that's what we need to guard against. 
---
This calls for a <b>test of mean</b>. Again,
be careful here: this mean is the population mean. 

The procedure is this: We take some bottles from the production line, open
them and actually measure their contents.
---
 That's our data. We want toy know if the population
mean is shifted from its advertised value or not. Let's
understanding this: using some concrete numbers.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=3.5</i></legend>
Start with a population. Draw a sample. Draw another
sample. Shift population. Draw
another sample.  Compute means of all three samples. 
Point out that the difference between the first two is ignorable,
but not so for the third.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Back to theory</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.00</i></legend>
So we take our null hypothesis <b>$H_0: \mu = 50$</b> and the alternative
hypothesis <b>$H_1: \mu \neq 50.$</b>
 Be careful here, this $\mu $ is the population mean,
 the unknown quantity depending on the current possibly faulty condition of the
machine. 
---
Had it been just the sample mean, then we could just
have computed it and checked if it is equal to 50. 

Even here, we do need to compare the sample mean with 50, but since the sample mean is
just an approximation of the population mean, we should allow some
margin.
--- 
A sample mean just a little off from 50 should not be an
excuse to stop the production process and call for the
repairman. 
---
The question is how to choose the margin. And that's what we
shall discuss in the next video.
</fieldset>

</div>

<fieldset>
<legend>m2l2_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     In order to test the null hypothesis that the population of
  a random sample of a continuous variable is 50, a student
  proposes to reject the null hypothesis if and only if the
  sample mean equals 50. Is it a good test?
  defaultFeedback: &gt;
      Not at all. Even if the population mean is indeed 50, the sample
  mean will almost deviate slightly from 50 due to random
  error. So you'll never accept the null hypothesis.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 2, Lesson 2, Video 2: one-sample t-test">Module 2, Lesson 2, Video 2: one-sample t-test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.40</i></legend>
We are learning test of population mean. In the last video we
discussed a real life scenario where such a test is called
for. Let's quickly recall the set up, but this time in an
abstract way. 
---
Our set up consists of a population, an underlying distribution
with an unknown <b>mean $\mu.$</b> In our bottling plant example
this is the population of all bottles that the machine can churn
out in its current possibly faulty setting.
---
 We have an <b>advertised value $\mu_0$</b> for $\mu.$ We want to
test <b>$H_0: \mu =\mu_0$ versus $H_1: \mu \neq \mu_0.$</b>
---
 In certain other situations it may be known a priori that $\mu$ cannot fall
below $\mu_0.$ Then we can just test againt <b>$H_2: \mu &gt;
\mu_0.$</b>
---
 Similarly, other situations may call for testing against <b>$H_3: \mu &lt;
\mu_0.$</b>
 These are called respectively
<b>two-tailed, upper-tailed and lower-tailed</b> alternatives.
---
We have a random sample from the population <b>$X_1,...,X_n.$</b>
We compute <b>$\bar X$</b> and are about to compare this
with $\mu_0.$
---
 We intuitively feel that we should
reject $H_0$ in favour of $H_1$ not just if $\bar X$ is slightly
different from $\mu_0,$ but only if $\bar
X$ is too far away from $\mu_0.$.
---
 Similarly we should reject $H_0$ in favour of $H_2$
(or $H_3$) if $\bar X$ is much larger (or smaller)
than $\mu_0$. 
---
The question now is "how large is too large?" The answer, as you
might have guessed already, depends on the dispersion present in
the population.
---
 When you are measuring an agricultural plot a
shift of a few centimetres during measurement is nothing serious,
but during an eye operation using laser beams, a shift of even a
10th of a millimetre may be disastrous. 
---
Hmmm...dispersion present in the population. How can we know
that? The population is that underlying unknown truth. Well,
yes. But not everything has to be unknown about the
population.
---
 For instance, many machines come with a rating of its
inherent precision, and even though we suspect that the mean has
shifted, we may have reason to believe that the dispersion is
still at its advertised value.
---
 If that is the case, then we are in luck. Let's call this <b>known 
population standard deviation $\sigma.$</b> Then our test statistic is 
<b>$$
Z = \frac{(\bar X - \mu_0)}{\sigma/\sqrt{n}}.
$$</b>
---
Its null distribution is <b>$N(0,1)$</b> which involves no unknown quantity,
and may be used to compute critical values or p-values.
---
Don't bother too much about this formula or the null distribution. 
These are already built
into all standard statistical softwares. However, you should
quickly recognise the denominator as the standard error
of $\bar X.$
---
 Notice the letter $Z.$ Well, this test is
called the $Z$-test of mean.
</fieldset>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.20</i></legend>
So far we have discussed the case where we are in luck: the
population standard deviation is known. Our machine has possibly
slipped in the mean, but still holding its advertised
precision.
---
 In most situations, however, we are not this lucky. If
we suspect that our machine has drifted away from its advertised
mean, we are equally unsure about its current standard
deviation.
---
 Well, nothing to despair even then. If you do not
know <b>$\sigma,$</b> just estimate it!
 So our test-statistic now becomes 
<b>$$
t = \frac{(\bar X - \mu_0)}{\hat \sigma/\sqrt{n}},
$$</b>
---
Here <b><font color="red">
[[finger]] </font></b>
$\hat \sigma$ is the sample standard deviation
with $n-1$ in the denominator. The rest is basically same as
the test with known $\sigma.$ 
This is the <b>one sample $t$-test.</b>
---
Of course, for the sake of complteness, we should also mention the null
distribution. 
 For z-test it was <b>$N(0,1)$</b> and for
the $t$-test it is something called $t_{(n-1)}$<b><font color="red">
[[shown]] </font></b>.
---
Anyway, they are built into standard softwares. So
let's not torture ourselves with their not too apetising
mathematical forms. Instead it is time to see these things in action.
</fieldset>

</div>

<fieldset>
<legend>m2l2_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     t-test is used  the population standard deviation is
  unknown. True or false?
  defaultFeedback: &gt;
     True. If the population standard deviation is known, then we
  use z-test.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 2, Lesson 2, Video 2: Lab">Module 2, Lesson 2, Video 2: Lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Toy data. Full computation. Mention t-distribution. df.
<font color="red">
<pre>
set.seed(23261)
x = as.numeric(format(rnorm(6,mean=10),digit=2))
m=mean(x)
v=var(x)
n=length(x)
(m-11)*sqrt(n)/sqrt(v)
t.test(x,mu=11)
t.test(x,mu=11,alt="greater")
</pre>
</font>

</fieldset>

</div>

<fieldset>
<legend>m2l2_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     What is the degrees of freedom in a t-test if the sample
  size is 20? 
  defaultFeedback: &gt;
      20-1 = 19.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
z test and one sample t-test.
</fieldset>

</div>

<fieldset>
<legend>m2l2_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider this data set \frac ./exraux3/length.csv that stores
  the lengths of a random sample of bolts.  Test at 5% level of
  significance the null hypothesis that the population mean is 5 cm.
  defaultFeedback: &gt;
      The t-statistics is 10.14. The p-value is nearly 0, and so
  &lt; 0.05. Hence we reject the null hypothesis to conclude that at
  5% level of significance the population mean of the bolts
  differ from 5 cm.

</pre>
</fieldset>

<font color="red">
<pre>
cw('exraux3')
set.seed(3534)
length = trim(rnorm(100,mean=5.2,sd=0.2))
csv(data.frame(length),'length.csv')
</pre>
</font>



<h2><a
name="Module 2: Lesson 3: Paired t-test">Module 2: Lesson 3: Paired t-test</a></h2>

<b>Total lesson duration
= 22.9</b>
<p></p>

<div class="header">
<h3><a
name="Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=4.30</i></legend>
As we have already said, there are many different types of tests
of statistical hypotheses 
suited for different purposes. We have already talked about the
t-test, or one-sample t-test, to be more precise. 
---
It was used to compare mean of a population with some given
value. Like here is a sample, do you think that the population it
comes from has mean equal to 50? That was pretty useful as it
stood. But we can do more with the idea. 
---
In this lesson we shall talk about a related
test, which is basically an application of what we have learned in
the last lesson. As usual, we shall start with an example. 
---
Often measurements are done in pairs. Like you do something, and
want to see the effect. Then you make two measurements, one before
and one after that activity. 
---
You have a medication for <b>insomnia</b>, say. Then you randomly
select some <b>patients</b>, and measure their
<b>daily average amounts of  sleep</b>.
<b>before</b> the medication starts, and then
again for the same set of patients <b>after</b> the medication is over.
---
 Another scenario that leads to paired
observations is where you have two similar, but different aspects of
something that you want to compare.
---
 Like you want to compare
 <b>salaries</b> of husbands and wives. You randomly select some <b>households</b>
with earning couples.
and compare the 
<b>husband's salary with the  wife's salary</b> within the same household.
---
 Or may be you have two different ways to measure the amount of
<b>active ingredient</b> in a tablet. You want to compare them. So you
take a random asortment of <b>tablet</b>s, break each tablet into two
halves and apply the <b>two methods</b>, one to each half, and record
the measurements.
---
There are many other possible scenarios. But we shall continue to use
the insomnia medication scenario as our running example. 
Imagine that there is a random sample of such patients, and we have two
measurements for each, one before and one after the medication.<b><font color="red">
[[shown]] </font></b>
---
 We are interested in knowing if the medication had any effect. Our
null hypothesis is <b>$H_0: $ no good effect vs $H_1:$ some
good effect.</b>
---
 Clearly, we should look at the amount of increase in
the sleep. So we should subtract the before values from the after
values.
---
 We can express the hyptoeses mathematically like this. The
null hypthesis says:<b><font color="red">
[[move]] </font></b> on an
average the after values are the same as the before values.<b><font color="red">
[[shown]] </font></b> The
alternative says: the  after values are larger.<b><font color="red">
[[shown]] </font></b>
---
 Again, be careful here. The hypotheses are in terms of
the population means<b><font color="red">
[[finger]] </font></b>, these $\mu$'s are the
population means, and not the sample means. 

We have a population of patients.
---
 As each patient produces two measurements, one before
and one after, each patient is like a pair of numbers. So we have
a population of pairs.
---
 If in each pair we subtract the first from
the second, we get a population of differences. We want to to
know that if the mean of this population is zero or positive.
---
 In this case we are ruling out the possibility that the mean may be
negative, ie, the medication actually backfires, and reduces the
amount of sleep. 
---
We can now perform a t-test on the difference. This is called <b>paired
t-test</b>. 
---
Notice the adjective "paired". This is important. Here we have
two sets of measurements, the ones made before the medication and
the ones made afterwards.
---
 But these are not just like two
unordeeed sets. The first measurement in this set and the first
measurement of that set both correspond to the same patient. This
common aspect provides the pairing between them. 
---
Now it is time to take a look at paired t-test in practice.
</fieldset>

</div>

<fieldset>
<legend>m2l3_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     In order to test if male  programmers earn more than
  female programmers the monthly incomes of 50 randomly selected
  male programmers and 50 randomly selected female programmers
  have been collected. Is this data set appropriate for a paired t-test?
  defaultFeedback: &gt;
      No. There is no pairing here. For example, the first male
  value has nothing to with the first female value. Such a
  situation calls for different tye of t-test, that we shall
  learn soon.  
</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6.5</i></legend>
Medication data. Peform the test.
</fieldset>

</div>


<fieldset>
<legend>m2l3_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     We want to test if the class 10 scores differ significantly
  from the class 12 scores for students in a certain state. A random  sample
  of students are selected from those who have appeared for both
  the exams in that state, and their scores are recorded in
  \frac ./exraux3/marks.csv
  Perform a paired t-test at 5% level. 
  defaultFeedback: &gt;
      The test statistic is -2.91. The p-values is 0.005 &lt;
  0.05. So we reject the null hypothesis to conclude that at 5%
  level of significance the two means differ.

</pre>
</fieldset>

<font color="red">
<pre>
cw('exraux3')
set.seed(33332)
marks10 = sample(10:100,100,rep=T)
marks12 = sample(30:100,100,rep=T)
csv(data.frame(marks10,marks12),'marks.csv')
t.test(marks10,marks12,paired=T)
</pre>
</font>

<div class="header">
<h3><a
name="Module 2, Lesson 3, Video 1: Paired t-test">Module 2, Lesson 3, Video 1: Paired t-test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.10</i></legend>
In the last video we saw paired t-test in action. Here we shall
look at it from a more abstract viewpoint, and also consider the
assumptions on the underlying distribution that are needed. 
---
We start with a data matrix with 2 continuous variables whose
values are paired. Let's pause for a moment and understand this
concept.
---
 Suppose we have two sets of agricultural plots, each set
has 5 plots.<b><font color="red">
[[rectangles shown]] </font></b> In the first set we have sown paddy and in the
second we have sown wheat.<b><font color="red">
[[shown]] </font></b>
---
 We measure the yields of all the
plots. Thus we have 5 paddy yields and 5 wheat yields. We
create a 5x2 data matrix out of them.<b><font color="red">
[[shown]] </font></b>
---
 Is this suitable for
paired t-test? No, because the two columns are not paired. You
might as well shuffle the values within a column without
losing any information. 
---
Now compare this with  a similar situation. A new chemical
spray has been been proposed that is supposed to enhance the
yield of paddy.
---
 To test its efficacy 5 plots are taken<b><font color="red">
[[shown]] </font></b>, 
and paddy is sown in them.
---
 Then each plot is divided into
two equal halves<b><font color="red">
[[shown]] </font></b>, and the spray is applied to only one half.

---
 The yields from the two halves are measured separately, resulting in 5
with-spray yields and 5 without-spray yields. Again we get a
5x2 data matrix.<b><font color="red">
[[shown]] </font></b> Is this data set suitable for a paired t-test?
---
The answer is Yes. The fact that the same plot is split into two
halves provide the pairing. The two halves in the same pair share
common soil type or irrigation.
---
Now let's come back to ... the mathematical set up.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
Coming back to the mathematical set up we have two continuous variables of
the same type (like both are measurements of the same quantity),
and their values are paired. We shall denote them as 
<pre>
x1 y1
...
xn yn
</pre>
There is a populaton mean for the xi's and a populaton mean for
the yi's. Call them $\mu_x$ and $\mu_y.$ We are trying
to test null hypothees like $\mu_y = \mu_x$ or $\mu_y \leq
\mu_x$ against alternatives like $\mu_y \neq \mu_x$
or $\mu_y &gt; \mu_x.$ 

The assumption that we make here is the $w_i=x_i-y_i$ values
have a normal distribution. This of course has
mean $\mu_x-\mu_y=\mu,$ say, and some unknown standard
deviation $\sigma.$ Then the null hypothees become sometime
like $H_0: \mu = 0$ or $H_0: \mu \leq 0$ with
alternative hypothees like $H_1: \mu \neq 0$ or $H_1: \mu&gt;
0.$ 

We use the test statistic
$$
t = \frac{\sqrt n \bar w}{sd(w)}.
$$
Its null distribution is $t$ with degrees of freedom $n-1.$
</fieldset>

</div>

<fieldset>
<legend>m2l3_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     In the pair t-test statistic as well as the 1-sample t-test
  static we are dividing by the sample standard deviation. Why?
  Can you see the need for this intuitively?
  defaultFeedback: |-
      We are basically trying to see if the sample mean (in case
  of 1-sample t-test) or the sample mean of the differences (in
  case of the paired t-test) is close to zero or not. Now
  "close to zero" is to be interpreted with respect to the
  variability of the quantity. When we measure distance of the
  stars even an error of a few light years is "good enough"
  because the variability of the measurement is very high. But
  for eye surgery an error of even 0.1 cm is serious, because
  there the aparatus is much more precise. The sample standard
  deviation gives us an idea of this variability.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Toy data. Full computation. Mention t-distribution. df.
<font color="red">
<pre>
set.seed(23263)
x = as.numeric(format(rnorm(6,mean=3),digit=2))
y = as.numeric(format(rnorm(6,mean=3),digit=2))
z = y - x
m=mean(z)
v=var(z)
n=length(x)
(m-0)*sqrt(n)/sqrt(v)
t.test(z,alt="greater")
</pre>
</font>


</fieldset>

</div>

<fieldset>
<legend>m2l3_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     Consider this toy data set and compute the paired t-test
  statistic for equality of means:
  | X | Y |
  |---+---|
  | 2 | 3 |
  | 4 | 4 |
  | 3 | 4 |
  | 2 | 2 | 
  What are the degrees of freedom?
  defaultFeedback: &gt;
      Test statistic value is -1.73, degrees of freedom are 3.

</pre>
</fieldset>

<font color="red">
<pre>
t.test(c(2,4,3,2),c(3,4,4,2),paired=T)
</pre>
</font>




<h2><a
name="Module 2: Lesson 4: Two-sample t-test">Module 2: Lesson 4: Two-sample t-test</a></h2>

<b>Total lesson duration
= 21.2</b>
<p></p>


<div class="header">
<h3><a
name="Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.9</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.80</i></legend>
In the last lesson we learned about the paired sample
t-test. What we are going to learn in this lesson is deceptively
similar to that. It is called <b>two sample t-test</b>, or sometimes
called the <b>independent samples t-test</b>.
---
 This latter terminology is more suggestive, and clearly expresses its difference from a
paired sample t-test. Here also we have two sets of numbers, but
instead of being paired they are indepenedent. 
---
Let's spend some time appreciating the difference. While
discussing paired t-test we had mentioned an example where we
were assessing the effectiveness of a spray in increasing
agricultural yields. The set up there was like this.<b><font color="red">
[[shown]] </font></b>
---
 We had halved a number of plots, and applied the spray to only one half
and measured the yields of the halves separately. Now this is pretty cumbersome
to carry out in practice. So instead here is a different more
easily implementable set up.
---
 We start with a bunch of more or
less identical plots, say 9.<b><font color="red">
[[shown]] </font></b> Then we pick 4 plots randomly out
of them and apply the spray only to those.<b><font color="red">
[[shown]] </font></b> So we have 4
yields from these, and 5 from the others.<b><font color="red">
[[finger]] </font></b>
---
The resuting data look like this.<b><font color="red">
[[shown]] </font></b> 
So now we have to compare 4 numbers with 5 numbers. Clearly there is no pairing between
elements of these two sets. They are indepenedent. Even the sizes of these two sets are
different, one is 4, the other 5.
---
 Here we need a different type
of t-test, the <b>two-sample t-test or the independent samples
t-test.</b> 
---
The first step  is to compute the
average yield from both the groups, call them $\bar x$
and $\bar y$ and subtract one from the other.<b><font color="red">
[[shown]] </font></b>
---
 It is not enough to  check that it is more than or less than
zero.  We must remain
aware that our conclusion is to hold for the population means,
for which the sample means are just approximations.
---
 So we need to
have an idea about how good the approximations are, ie, the
variability of the yields within the two populations must be taken
into account. 
---
Here we face an unexpected problem. It may be the case that the
variability within the two populations are the same. Or it could be
that there is no such guarantee. Accordingly we have two different tests:
<b>homoscedastic</b> and <b>heteroscedastic</b>. 
---
Let's consider the cases one by one.
</fieldset>


<center>
<img src="image/jingle.png"><font color="blue" size="+3">Homoscedastic and heteroscedastic</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=3</i></legend>
In the homoscedastic case,  the dispersion is the same in
both the groups. Here we use this test statistic.<b><font color="red">
[[shown]] </font></b>
---
 For the
heteroscedastic case the dispersion may be different
in the two  groups. There we use a this test statistic.
<b><font color="red">
[[shown]] </font></b>
---
 The formulae are somewhat complicated, but their null
distributions are simpler. They are 
always $t$-distributions with different degrees of
freedom.
---
 In the homoscedastic case it is $m+n-2,$<b><font color="red">
[[shown]] </font></b>
In the hetescedastic
case it is complicated. 
They are complicated,
but they have a pattern. In both the cases the numerator is the
difference of the two sample means, and the denominator is an
estimator of its standard error.

So let's see how to perform two-sample $t$-test using LibreOffice.
</fieldset>

</div>

<fieldset>
<legend>m2l4_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     In order to test if boys run faster than girls on an average
  20 boys are 20 girls are chosen at random and for each of them
  the time to run 100 m is recorded. Is this an appropriate set
  up for a 2-sample t-test?
  defaultFeedback: &gt;
      Yes. However, in practice we should be more precise about
  the scope of the study, like age group training level etc. 

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Homo and hetero 2-sample t-test
[data.ods]
</fieldset>

</div>

<fieldset>
<legend>m2l4_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Here is a data set of 20 boys' and 20 girls' running time
  for a 100 m sprint. \frac ./exraux3/runtime.csv

Perform both a homo
  defaultFeedback: &gt;
      For the homoscedastic test, the test statistic is -4.53
  with p-value 1. So we accept (say at 5% level of significance)
 the null hypothesis that boys do
  not run faster than girls.
      For the heteroscedastic case, the test statistic is again
  the same, and so is the p-value, and the conclusion.
</pre>
</fieldset>

<font color="red">
<pre>
cw('exraux3')
set.seed(6868)
boy = 16+sample(0:20,20,rep=T)/10
girl = 17+sample(0:20,20,rep=T)/10
csv(data.frame(boy,girl),'runtime.csv')
t.test(boy,girl,alt='greater',var.eq=T)
t.test(boy,girl,alt='greater',var.eq=F)
</pre>
</font>

<div class="header">
<h3><a
name="Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.30</i></legend>
We have seen how there are two versions of
2-sample $t$-tests based on <b>homo and heteroscedastic</b> set
ups. 
---
Both these tests seek to achieve the same basic aim,
comparing the means of two populations. Then why are there two
different tests? Well, because they work under two different set
ups. 
---
So the important question now is: given a real life problem, how
on earth are we to know which t-test we should use? The homoscedastic one
or the heteroscedastic one?
---
The answer depends on the <b>standard deviations of the two
populations</b>. If the two standard deviations are known to be equal, then we
are in the homoscedastic set up, otherwise (ie if the equality is not
known for sure) then we are in the heteroscedastic
set up.
---
 Now the population standard deviations are typically unknown. So there is no sure
way of knowing if we are in a homoscedastic set up or a
heteroscedastic one.
---
However, we may guess, or to use a more precise statistical term,
test the hypothesis of homoscedasticity against that of
heteroscedasticity. And there is indeed a standard test for that,
the <b>$F$-test</b>. We shall learn about this in this video.
---
First we need some new symbols. We are talking about equality of
two population variances (or equivalently equality of standard deviations).
---
Remember that here we have two populations, and one sample has
been drawn from each, independently of the other. So each
population has its own standard deviation.
---
 We call these <b>$\sigma_1$ and $\sigma_2.$</b> Then we are trying to
test <b>$H_0: \sigma_1 = \sigma_2$ vs $H_1: \sigma_1 \neq
\sigma_2.$</b> 
---
We have already mentioned in the first lesson of this module, how
the best test statistic may be obtained by restricting the
probability of type I error, and minimising the probability of
type 2 error subject to that.
---
 Well, if we follow that rather
mathematical process, we finally arrive at a test statistic, which
is quite intuitive. It is 
<b>$$
\frac{\sum_i (x_i-\bar x)^2/(m-1)}{\sum_j (y_j-\bar y)^2/(n-1)}.
$$</b>
---
Here $m$ is the size of the sample from the first
population, and $n$ is that for the second.

The numerator is just the sample variance for the first sample,
and the denominator is the sample variance  for the second
sample.
---
 If the two population variances are indeed the same, ie,
if $H_0$ holds, then this quantity should be close to 1. How
close? That answer is given by the null distribution.
---
 Again, that is a complicated beast, but fortunately people have already
computed it. It is called an <b>$F$-distribution with degrees
of freedom $m-1$ and $n-1.$</b>

Let's see it in practice in the next video.
</fieldset>

</div>

<fieldset>
<legend>m2l4_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Again consider the same set up as in the last exercise. But
  this time we have 10 boys and 12 girls taking part. If we want
  to carry out an F test for homoscedasticity then what will be
  the two degrees of freedom? 
  defaultFeedback: &gt;
      The first degrees of freedom is #boys-1 = 10-1=9. The
  second degrees of freedom is #girls-1 = 12-1=11.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 2, Lesson 4, Video 1: 2-sample">Module 2, Lesson 4, Video 1: 2-sample</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
F-test.
[ftest.ods]
</fieldset>

<font color="red">
<pre>
p(2,4,4)
set.seed(3346)
x1 = 20+trim(rnorm(100))
y1 = 10+trim(rnorm(100))

x2 = 10+trim(rnorm(100,mean=1))
y2 = 10+trim(rnorm(100,sd=0.5))
csv(data.frame(x1,y1,x2,y2),'ftest.csv')
</pre>
</font>

</div>

<fieldset>
<legend>m2l4_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Perform an F-test  for homoscedasticity at 5% level of
  significance using the boys' and girls' running time data in
  \frac ./exraux3/runtime2.xlsx. 
  defaultFeedback: &gt;
   \frac ./exraux3/runtime2sol.png
   The p-value shown in yellow is &gt; 0.05. So at 5% level of
  significance we accept the null hypothesis of homoscedasticity.
</pre>
</fieldset>

<font color="red">
<pre>
cw('exraux3')
set.seed(68681)
boy = 16+sample(0:20,12,rep=T)/10
girl = 17+sample(0:20,12,rep=T)/10
csv(data.frame(boy,girl),'runtime2.csv')
</pre>
</font>



<font color="red">
<pre>
cw('exraux3')
set.seed(2363476)
x = trim(rnorm(100,mean=11))
csv(data.frame(x),'meantest.csv')
t.test(x,mu=10)
</pre>
</font>

<font color="red">
<pre>
cw('exraux3')
dat = read.csv('tablet.csv')
t.test(dat[,2],dat[,3],pair=T)
</pre>
</font>

<fieldset>
<legend>m2_add.yml</legend>
<pre>
- typeName: peerReview
  prompt: &gt;
    I am trying to test the null hypothesis that a particular
  coin is fair against the alternative that it is not fair. I
  toss the coin twice and reject the null hypothesis if and only
  if both tosses show the same outcome. What is the probability of type I error
  here?

  Rubric:
    1) Type I error means rejecting H0 when it is true. H0 is
    true means the coin is fair. Rejection of H0 means both
    tosses show the same outcome (HH or TT). (1 point)
    2) Probability is 0.5*0.5 + 0.5*0.5 = 0.5. (1 point)
- typeName: peerReview
  prompt: &gt;
    I have performed a test to get a p-value 0.08. Should I
  accept the null hypothesis at 5% level of significance? 
  What about 10% level of significance?

  Rubric: 
    1) 5%: accept (1 point)
    2) 10%: reject (1 point)

- typeName: peerReview
  prompt: &gt;
    Here is a random sample \frac ./exraux3/meantest.csv from a population with unknown mean
    and variance. Perform a test of hypothesis of the null
    hypothesis that the population mean equals 10 against the
    two-tailed alternative. Clearly mention whether you
    accept/reject the null hypothesis at 5% level of significance.

  Rubric:
    1) test statistic: 11.1, df = 99, p-value = 0. (1 point)
    2) Conclusion: At 5% level of significance reject H0:
    mean=10. (1 point)

- typeName: peerReview
  prompt: &gt;
    There are two methods (NIR and HPLC) to find the amount of the active
    ingredient in a tablet. To compare them a random asortment of
    tablets are taken. Each tablet is halved and one method is
    applied to each half. The resulting data set is in \frac ./exraux3/tablet.csv. Perform
    a suitable test at 5% level of significance to test the null hypothesis that both the
    methods give basically the same result. Clearly mention
    whether you accept/reject the null hypothesis.

   Rubric:
     1) Paired t-statistic: -0.54, df = 9, p-value = 0.61 (1 point)
     2) Since p-value &gt;= 0.05, at 5% level of significance we accept H0: both methods give
     same result. 

- typeName: peerReview
  prompt: &gt;
    If we perform a homoscedastic two-sample t-test using two
    samples of sizes 5 and 8, then what will be the degrees of
    freedom? What will the degrees of freedom be for an F-test to
    test equality of variance using the same two samples?

  Rubric:
    1) For the t-test: 5+8-2 = 11.
    2) For the F-test: 5-1 = 4 and 8-1 = 7.


</pre>
</fieldset>

<h2><a
name="Module 2: Lesson 5: Review">Module 2: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 2, Lesson 5 , Video 1: Review">Module 2, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 2, Lesson 5 , Video 2: Review">Module 2, Lesson 5 , Video 2: Review</a></h3>
</div>




<h1><a
name="Module 3: Goodness of fit and independence">Module 3: Goodness of fit and independence</a></h1>

<div class="scrpt">
<b>Total video duration
= 1.9</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.90</i></legend>
We are about to start our 3rd week. Last week we learned about
tests of hypothesis. This week we shall discuss 
two important applications of that concept.
---
Tests of hypothesis allow us to guess answers to yes-or-no
questions. Here is some such questions: Is smoking related to lung
cancer? Or does one's chance of getting admitted to some reputed
institute depend on one's religion?
---
 These and other such questions
are of great social importance. These are all yes-or-no
questions relating two categorical variables. Are you a smoker
(yes/no) with do you have lung cancer (yes/no).
---
 Or the relation between  your
religion (which is a categorical variable) and whether you
got admitted (yes/no). How do we go about answering such
questions based on data? That's one thing we shall learn this
week.
---
The other thing is slightly less obvious. We have learned about
distribution of a variable. In many cases we want to know if the
distribution is of some ideal type.
---
 Like here is a coin. Is it fair? Here is a Roullette wheel, it
is supposed to be perfectly balanced. Is it really so? Or look at
the gender distribution among the employees, is it really free of
bias?
---
 So you have an ideal distribution (or ideal class of
distributions in mind) and  some  data from a real
distribution. You want to answer the question:
---
 Does the real distribution (which you do not know beyond 
having a sample from it) really belong to or fit into that ideal
class. Such tests are called <b>goodness-of-fit</b> tests. 
---
We shall learn about them this week.

We shall work exclusively with categorical variables and
contingency tables. Lesson 1 will remind us
about them. 
</fieldset>

</div>

<h2><a
name="Module 3: Lesson 1: Contingency tables">Module 3: Lesson 1: Contingency tables</a></h2>


<div class="header">
<h3><a
name="Module 3, Lesson 1, Video 1: Contingency tables">Module 3, Lesson 1, Video 1: Contingency tables</a></h3>
</div>

<b>Total lesson duration
= 18.2</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 5.4</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=1.30</i></legend>
In the Basic Statistics course we talked about pivot tables, and
had mentioned how one special 
 type of pivot table
 has much more important use than the rest.
---
 These are the <b>contingency table</b>s. We had only touched upon the subject
in that course. Now is the time for a fuller exposition.
---
Let's start with a brief recap.
A pivot table requires specification of two things:
 * One or more <b>categorical</b> variables in the data set.
 * Some <b>summary</b> measure.
---
To get a contingency table, we need to choose  two
categorical variables, and count as the summary measure. Let's
take an example.
---
Here is a table<b><font color="red">
[[shown]] </font></b> with two categorical variables gender
(Male/Female) and Handedness (Left/Right). The corresponding
contingency table is 2 by 2<b><font color="red">
[[shown]] </font></b>, where each cell has the
corresponding count. <b><font color="red">
[[Explain by pointing].] </font></b>
---
It should be quite easy to create such a contingency table in
LibreOffice, as we have already learned how to create a general
pivot table. Anyway, here is a little lab session, in case you
need one.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=4</i></legend>
[Screencast: Make contingency table.]
lr.ods
</fieldset>

</div>


<fieldset>
<legend>m3l1_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
           Create a contingency table from this data set: \frac ./exraux3/typedef.csv.
  defaultFeedback: &gt;
      \frac ./exraux3/typedefsol1.png
</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 3, Lesson 1,  Video 2: Contingency table">Module 3, Lesson 1,  Video 2: Contingency table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.10</i></legend>

Contingency tables are very important objects in statistics. How
do we use them?
---
 They are used to  explore relation
between two categorical 
variables. We had a taste of this in the Basic Statistics
course. An example would help to jog our memory.
---
We often want to answer questions like <b>"Is smoking related to
cancer?"</b>. Or what is the relation between <b>education level and
income group</b>? Does the chance of <b>admission to an educational
institute depend on one's gender</b>? 
---
All these questions are trying
to connect two categorical variables. In the first case smoking
habit (Y/N) with cancer (Y/N).
---
 In the second case, educational
level (preschool, school,  college and above)
and income level (low, middle, high). In the third case admission
status (admitted / rejected) and gender (male /
female). 
---
Contingency tables help us to understand such relations.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Joint distribution</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.00</i></legend>
We have learned about frequency distributions in our Basic
Statistics course, and seen how they lead to probability
distributions via statistical regularity. 
---
Well, contingency tables do the same thing too. They are also
frequency distribution tables of categorical variables, not of just one categorical
variable, but multiple ones simultaneously. 
---
Thus we do not merely
ask questions like "how many males are there in my sample?", or "how
many lefthanded persons?" We ask both the questions together,...
---
 like "how many lefthanded males are there in the sample?" We call
these  <b>joint frequency distributions</b>.
---
 Just as we could
construct bar charts based on frequency distributions, we can
create bar charts based on joint frequency distrbutions<b><font color="red">
[[shown]] </font></b>. Here
each bar is like a 3D pillar, one pillar per cell.
---
Suppose we make the height of each bar depict not the
 frequencies, but the <b>relative frequencies</b>, ie the cell frequncies
 divided by the total sample size.
---
 Then statistical
 regularity will kick in. As the sample size increases, the bar
 plot will converge to a fixed shape. This shape is called the
 <b>joint probability distributions</b> of categorical
variables.
</fieldset>

</div>


<fieldset>
<legend>m3l1_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     We have made a contingency table with education level
  (prescool, school, college, higher) and
  income group (low, middle, high), based on a random
  sample of 1000 persons. Assuming that this sample size is large
  enough, what probability does the releative frequency of the
  (1,1)-th cell approximate?
  defaultFeedback: &gt;
    The probability that a randomly selected person from the
  population will have preschool education level and be in the
  low income group.
</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 3, Lesson 1,  Video 4: Contingency table">Module 3, Lesson 1,  Video 4: Contingency table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.00</i></legend>
We have mentioned how we may construct  a contingency table from
raw data. When we collect raw data with the aim of creating a
contingency table, there are three major ways to go about it. Let's
understand this with an example. 
---
Suppose that our population consists of employees from a
particular job sector. We are interested in two variables <b>gender
and responsibility</b> levels. Let's say there are four
responsibility levels: <b>Manager,
Engineer, Clerk and  Support staff</b>.
---
 Our final aim is to get a
contingency table<b><font color="red">
[[emptytab]] </font></b> like this, that shows genders in the rows
and the responsibilities in the columns. 
---
How should we go about collecting data for this? One method could
be to select 100 employees randomly from the entire population,
and observe the gender and responsibility levels for
each.
---
 In this scheme we have no control on how many men and how
many women will be in our sample. Neither do we have any control
on the counts for the responsibility levels. So both the row
marginals and the column marginals are random.
---
An alternative approach<b><font color="red">
[[gentab]] </font></b> is to draw a random sample of 50 men and
50 women, and then observe the responsibility levels for each of
them. Here there are 50 men and 50 women by design. So the row
marginals are fixed, not random. However,  the col marginals
continue to be random as before.
---
Of course, we could have done it the other way around, fixing the
column marginals first<b><font color="red">
[[restab]] </font></b>. Then we should have chosen in advance the number of
persons in each responsibility level, say 25 each, and randomly
chosen that many employees from each level.
---
 Finally, we observe
the genders of these selected employees. 

Why are we bothering about these different sampling schemes? Because the
different  schemes will let us make different types of
inferences. 
---
Suppose that we want to
know the proportion of male employees versus female
employees. Then the second scheme is not suitable, because there
we deliberately chose an equal number of men and women.
---
Similarly,
if we are interested in comparing the responsibility levels for
the two genders, <i>i.e.</i>, trying to check if the responsibility
level distribution for men...
---
 differ significantly from that for
women, then the third scheme is not suitable, as there we are
starting by fixing the responsibility level counts.
---
  The first
scheme may also not be suitable if there happen to be too few men (or two
few women) in the sample. Here the second scheme is the best
suited for the purpose.
</fieldset>

</div>

<fieldset>
<legend>m3l1_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     A statistician suggests using the following sampling scheme
  to create a contingency table between gender and
  responsibility levels: Draw a random sample of size 100 such
  that al the marginals are fixed: 50 persons of
  each gender AND 25 persons of each responsibility level. What
  would you say to this plan?
  
  defaultFeedback: &gt;
    You really cannot carry this out in practice without first
    creating a contingency table in a non-random way! Such a
    contingency table will not be suitable for statistical analysis.
</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 3, Lesson 1,  Video 5: Contingency table">Module 3, Lesson 1,  Video 5: Contingency table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 1.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.60</i></legend>

Contingency tables tell us how multiple categorical variables
vary together. So far we have been considering the simplest
possible scenario, just two categorical variables.
---
 As a result our contingency tables are all 2-way or
2-dimensional. While these are indeed the most commonly used, it
is quite possible to have higher dimensional 
contingency tables, as well.
---
 A 3-way contingency table will not look like a flat rectangle
drawn on a piece of paper, but as a 3-dimensional block. Or like
a stack of rectangles, one rectangle per layer.
---
 Let's take an example. Suppose we want to study the relation
between 3 categorical variables, say, <b>gender</b>, <b>educational
level</b>
and <b>income group,</b>
---
 We consider  two levels
under gender <b><font color="red">
[[shown]] </font></b>, 4 under educational
level <b><font color="red">
[[shown]] </font></b>, and 3 under income group<b><font color="red">
[[shown]] </font></b>.
---
So we shall have a <b>3-way
contingency table</b>,which is <b>$2\times4\times 3$</b> in
size.
---
Here we commonly refer to the first variable as the
<b>layer</b>, the second as the <b>row</b>, and last as
 the <b>column</b>. It is as if a 3-way contingency table consists of
 layers, each layer holding a 2-way contingency table.
---
Thus, gender is the layer variable, educational level is the
row variable, and income group is the column variable. Such
 tables are generally printed  layer by layer, where each layer is
a 2-way table. 
---
Most of the concepts that we have learned about 2-dimensional contingency
tables readily generalise for higher dimensional contingency
tables, as well. For example, we may talk about association
between the variables.
---
However, a higher dimensional contingency table allows more
variety. For instance,  income group and educational level may
be associated in one way in the male layer, but  differently in
the female layer.  
---
We had some brush with such strange behaviours of
 multi-dimensional contingency tables in our Basic Statistics
 course already. 
---
In the next video we shall remind ourselves how to construct a 3-way contingency
table from raw data using LibreOffice.
</fieldset>

</div>

<fieldset>
<legend>m3l1_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     What will be size of a contingency table that shows gender
  (M/F) as layer, responsibility levels (Manager/Clerk/Support)
  as rows and income groups (High/Middle/Low) as columns?
  defaultFeedback: &gt;
      2x3x3
</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 3, Lesson 1,  Video 6: Contingency table">Module 3, Lesson 1,  Video 6: Contingency table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
[Screencast: 3-way simpson's paradox example. Just construction.]
simp.ods: victim, defendant, death penalty
</fieldset>

</div>

<fieldset>
<legend>m3l1_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If all the cell frequncies are doubled in the example shown
  in the video, will Simpson's paradox still be there? 
  defaultFeedback: |-
     Yes, because we are working with ratios. So doubling
  everything will have no effect.
</pre>
</fieldset>



<h2><a
name="Module 3: Lesson 2: Independence test">Module 3: Lesson 2: Independence test</a></h2>

<b>Total lesson duration
= 31.3</b>
<p></p>

<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 6.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=6.10</i></legend>
In this video we shall discuss the most important application for
contingency tables: <b>testing independence between two categorical
variables.</b> 
---
Let's work through an example to build up the idea. We had
already seen this in the Basic Statistics course. Here we shall
develop the idea further. 
---
Let's start with an example. We want to see if there is any
association between <b>educational level</b> and <b>income group</b>.
---
 We have four educational
levels (<b>Preschool, School, College and Univ</b>) and 
three income groups (<b>High, Mid and Low</b>) and  we have a
population of people in mind,...
---
we draw a random sample from it, and
ask the people in the sample about their income groups and educational
levels. This will fill up a 3x4 contingency table.<b><font color="red">
[[empty table trans]] </font></b>
---
Notice that  I have not told you the entries in the cells. All
that I have given are the marginals. So you know the proportions of the
three income groups in the sample,...
---
 and also the proportions of
the four educational levels. Assuming that the sample is
representative enough, these are good approximations for the
population proportions.
---
Now suppose I pick a random person from the population, then
what is the chance that he/she belongs to the high income group? It
is<b><font color="red">
[[finger]] </font></b> this 60 by this 300, which is 1/5.
---
 Now suppose, just for the sake of argument,  I tell you that income group
is independent of educational level. What does that mean? That
means even if I whisper in your ear that the randomly selected
person has never been to school,...
---
 you'll have no reason to change your answer to the first
question. You'll still say that the 
probability of that person's being in the high income group is 1/5.
---
 This is true about all the
persons. In particular about<b><font color="red">
[[finger]] </font></b> all the 41 persons in this preschool
group.
---
 So out of these 41 persons each has 1/5 chance  of being
in the high income group. So the expected number of persons in
this cell is 41/5 = 8.2.<b><font color="red">
[[shown]] </font></b>
---
 Am I making sense? It's like I toss a coin with
1/5 chance of head. If I toss it 41 times, then the expected
number of heads is 1/5 of 41, ie, 8.2.
---
 Don't worry about it's not being a whole number, it is just the result of the numeric
computation. We mean the frequency in that cell should be near
about that number. Now notice how we arrived at this number.
---
 It is <b>row total times column total by grand total</b>. This holds for all the
cells. So we can compute the expected frequencies for all the
cells. <b><font color="red">
[[shown]] </font></b>
---
All these are under the assumption that income group and
educational level are independent. Now we also have some observed
frequencies in the cells.<b><font color="red">
[[shown]] </font></b>
---
 If these observed frequencies are more
or less like the expected frequencies, then we do not have reason
to suspect any association between the two variables. Otherwise,
this table itself is an evidence against the assumed
independence. 
---
The next question therefore is about how to compute the
difference between the expected and the observed
frequencies. Computing the difference between two numbers is easy,
you just subtract on from the other.
---
 But here we have two sets
of numbers. Some of them may be close, while some may be far
apart. We need some kind of a pooling across all the cells. Here
is our first attempt. 
---
<b>$$
\sum_{ij} (e_{ij} - o_{ij}).
$$</b>
where $e_{ij}$'s are the expected frequencies
and $o_{ij}$'s are the observed ones.
---
Unfortnately this will just give a zero, because this is just
subtracting the grand total from itself. The problem is that
positive differences are cancelling off negative differences.
---
 But
for us a difference is a difference, we do not care about the
sign. So we get rid of the sign by squaring:
<b>$$
\sum_{ij} (e_{ij} - o_{ij})^2.
$$</b>
---
That's better, but should all the cells be considered with equal
importance? If for a cell we have expected frequency 20000 and
observed frequency is just 10 more than that, then the squared
difference is 100.
---
 If for another
cell the expected frequency is 5 and observed is 10 more than that, then also
the squared difference is 100. Shouldn't these 100's be treated
differently?
---
 In the first case the 100 is where the expected
frequency was large, 20000, while in the second case the same
amount came from a smaller expected frequency, just 5.
---
 So, relatively, the second case is a more serious departure from
independence. So we update the formula to 
<b>$$
\sum_{ij} \frac{(e_{ij} - o_{ij})^2}{e_{ij}}.
$$</b>
---
We have already met this quantity in our Basic Statistics
course. It is called the <b>$\chi^2$-statistic</b>. Large values of
this makes us go against the null hypotheses of independence.
---
How large is large? We shall get into those questions. But first
a little lab session is in order. That's what we shall do in the
next video.
</fieldset>

</div>

<fieldset>
<legend>m3l2_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     Fill in the expected frequencies in this contingency table
  assuming independence.
     |       | Female | Male | Total |
     |-------+--------+------+-------|
     | Blue  |        |      |    24 |
     | Brown |        |      |    38 |
     | Black |        |      |    54 |
     |-------+--------+------+-------|
     | Total |     71 |   45 |   116 |

  defaultFeedback: |-
     |       |   Female |      Male | Total |
     |-------+----------+-----------+-------|
     | Blue  | 14.68966 |  9.310345 |    24 |
     | Brown | 23.25862 | 14.741379 |    38 |
     | Black | 33.05172 | 20.948276 |    54 |
     |-------+----------+-----------+-------|
     | Total |       71 |        45 |   116 |
      

</pre>
</fieldset>

<font color="red">
<pre>
rw = c(24,38,54)
cl = c(71,45)
gt = 116
outer(rw,cl)/gt
sum(rw)

</pre>
</font>

<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Hand computation (whorl)
<font color="red">
<pre>
obs = matrix(c(35,129,10,33),2)
rtot = c(45,162)
ctot = c(164,43)
gtot = sum(obs)
expc = matrix(0,2,2)
expc[1,1] = rtot[1]*ctot[1]/gtot
expc[1,2] = rtot[1]*ctot[2]/gtot
expc[2,1] = rtot[2]*ctot[1]/gtot
expc[2,2] = rtot[2]*ctot[2]/gtot
expc
(obs-expc)^2/expc
sum((obs-expc)^2/expc)
</pre>
</font>

</fieldset>

</div>


<fieldset>
<legend>m3l2_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     Compute chisquare value from this contingency table:
       |    |  W | NW |
       |----+----+----|
       | W  | 34 | 25 |
       | NW | 98 | 34 |
 
  defaultFeedback: |-
     Here are the expected frequencies
     |    |         W |       NW |
     |----+-----------+----------|
     | W  | 1.1256652 | 2.518437 |
     | NW | 0.5031382 | 1.125665 |
     The chisquare value is 5.27.
</pre>
</fieldset>

<font color="red">
<pre>
obs = matrix(c(34,98,25,34),2)
rtot = apply(obs,1,sum)
ctot = apply(obs,2,sum)
gtot = sum(obs)
expc = matrix(0,2,2)
expc[1,1] = rtot[1]*ctot[1]/gtot
expc[1,2] = rtot[1]*ctot[2]/gtot
expc[2,1] = rtot[2]*ctot[1]/gtot
expc[2,2] = rtot[2]*ctot[2]/gtot
expc
(obs-expc)^2/expc
sum((obs-expc)^2/expc)
</pre>
</font>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Compute $\chi^2$ by raw computation and automatically.
</fieldset>

</div>


<fieldset>
<legend>m3l2_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     Compute chisquare value using LibreOffice using the same
  contingency table as in the last exercise:

       |    |  W | NW |
       |----+----+----|
       | W  | 34 | 25 |
       | NW | 98 | 34 |
 
  defaultFeedback: |-
     Here are the expected frequencies
     |    |         W |       NW |
     |----+-----------+----------|
     | W  | 1.1256652 | 2.518437 |
     | NW | 0.5031382 | 1.125665 |
     The chisquare value is 5.27.

</pre>
</fieldset>


<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Idea of null distribution of $\chi^2.$ Show a
population. Draw sample compute. Redraw and compute again. Show
difference. Show precomputed values in a different sheet. Make
hist.
chidemo.ods
</fieldset>

<font color="red">
<pre>
p(3,2,4)
set.seed(346346)
ser = 1:10000
gender1 = sample(c('male','female'),10000,rep=T)
type1 = sample(c('A','B','C'),10000,rep=T)
gender2 = sample(c('male','female'),10000,rep=T)
type2 = sample(c('A','B','C'),10000,rep=T)
val = numeric(1000)
for(i in 1:1000) {
   ind = sample(10000,500)
   val[i] = chisq.test(gender2[ind],type2[ind])$stat
}
csv(data.frame(ser,gender1,type1),'actual.csv')
csv(data.frame(ser,gender2,type2),'indep.csv')
csv(val,'samp.csv')
</pre>
</font>

</div>

<fieldset>
<legend>m3l2_d.yml</legend>
<pre>
- typeName: checkbox
  prompt: &gt;
      Which of the following statements is/are true?
  defaultFeedback: &gt;
      Sampling distribution means the behaviour of a statistic if
  fresh samples are drawn from the same population
  repeatedly. Null distribution is the special case where the
  population is as specified by the null hypothesis.
  shuffleOptions: true
  options:
  - answer: Null distribution is a special case of sampling distribution.
    isCorrect: true
    feedback: &gt;
      
  - answer: Sampling distribution is a special case of sampling distribution.
    feedback: &gt;
      
  - answer: Null distribution and sampling distribution are synonymous.
    feedback: &gt;
      
  - answer: &gt;
      Null distribution and sampling distribution are
      unrelated concepts.
    feedback: &gt;
      

</pre>
</fieldset>



<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.20</i></legend>
In the last lab session we have got a taste of the null
distribution of the $\chi^2 $ statistic. The process we used
there was cumbersome. Statisticians have found a smarter method,
albeit an approximate one.
---
 They have seen that if the sample size
is large, and so are all the expected frequencies (say <b>sample
size at least 30 and each expected frequency at least 5</b>, as a
rule of thumb),...
---
 then the shape of the histogram looks like this<b><font color="red">
[[shown]] </font></b>. 
Of course this is just the general shape, there could be
variations. 
---
To be precise the general shape gives a family of distributions,
called the <b>$\chi^2 $ distributions</b>. Each member of the
family is identified by a number called the <b>degrees of
freedom</b>.
---
 The larger the degrees of freedom, the more spread out
the shape. For example this<b><font color="red">
[[shown]] </font></b> has higher degrees of freedom than
this.
---
Given a contingency table there is a simple way to work out the
degrees of freedom of the null distribution. It is just 
<b>(nrows -1) times (ncols - 1)</b>.
---
 When you compute the numbers of
rows and columns, don't include the totals. We are counting only
the numbers of categories of the two variables.
---
 For instance, in the educational level vs income group example<b><font color="red">
[[shown]] </font></b>, the degrees of
freedom are (4-1)(3-1) = 6.
---
What happens if you have just one row or only one column? Is the
degree of freedom 0 then? Well, you do not need to worry about
that case.
---
 A caegorical variable should have at least two
categories, because otherwise it is just a constant! So you'll
always have at least 2 rows and 2 columns.
---
Of course, as a practicing statistician in the modern age, you
really do not have to remember all these. All standard
statistical softwares have these things built in. 

The next video will demonstrate this in action.
</fieldset>

</div>

<fieldset>
<legend>m3l2_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If the degrees of freedom of a chisquare distribution
  increases, then how will its variance change?  
  defaultFeedback: &gt;
     It will increase. Think about the shapes shown in the
  video. Higher the degrees of freedom, the more spread out the shape.
      
- typeName: textReflect
  prompt: &gt;
     What can you say about skewness of a chisquare distribution
  based on the shapes shown in the video? 
  defaultFeedback: &gt;
    Positively skewed.
      

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 3, Lesson 2, Video 1: Independence test">Module 3, Lesson 2, Video 1: Independence test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
$\chi^2 $ test.
[eduinc: use menu, then show]
</fieldset>

</div>

<fieldset>
<legend>m3l2_f.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Carry out chisqure test of independence using this data set 
     \frac ./exraux3/bodygender.csv about the gender and body type
  of a random sample of persons. 
  defaultFeedback: &gt;
      \frac ./exraux3/bodygendersol.png
      Notice how we decide about the accept/reject decision. 
      A computer will not do that for you. But a test is
  meaningless unless you clearly mention your final decision.

</pre>
</fieldset>



<h2><a
name="Module 3: Lesson 3: Permutation test">Module 3: Lesson 3: Permutation test</a></h2>

<b>Total lesson duration
= 23.3</b>
<p></p>

<div class="header">
<h3><a
name="Module 3, Lesson 3, Video 1: Permutation test">Module 3, Lesson 3, Video 1: Permutation test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.8</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=2.70</i></legend>
We have seen in the last video how the $\chi^2$ statistic
may be used for testing independence between two categorical
variables.
---
 We first compute the $\chi^2 $ statistic using a
special formula and then check if it is too large. If it is, then
we suspect that the two categorical variables are not
independent, else our verdict goes for independence.
---
 How do we
check for the value being too large? We employed
the $p$-value method. Now, $p$-value compares a given
number to a distribution, the null distribution in our case.
---
 That is, how the $\chi^2 $ statistic is expected to behave had
the two categorical variables really been independent. 

Thus there are two steps:
---
* First, <b>finding the $\chi^2 $ value</b>
* Second, <b>comparing it with the null distribution.</b>
---
How are we computing the null distribution? Well, we as
practising statisticians are not computing it ourselves, but
letting the computer do it for us using mathematics.
---
 Now most mathematical reasoning requires certain assumptions, and
<b>statistical regularity</b> is one of the major assumptions
here.
---
 Statistical regularity creates regular behaviour out
of randomness, when a large amount of randomness is piled
together carefully.
---
 So we need a large amount of randomness, ie,
a large sample. In particular for the $\chi^2$ distribution
to hold, each of
the expected frequencies must be large as well.
---
 How large? Well, the larger the better, but a commonly used  rule of thumb sets
the threshold as <b><font color="red">
[[ineqs shown]] </font></b>low as 5. If the total sample size is at least
30  and expected frequencies in all
the cells in the table are at least 5,...
---
then we may safely use the null distribution provided by the
computer, says the rule. But 
what happens if some expected frequency drops below 5? Well, the
first step is still meaningful. 
---
The $\chi^2 $ statistic still makes sense. 
It is sort of a distance between the observed
frequencies and the frequencies expected under independence.
---
 The only problem is that the null distribution prescribed for it by
the computer is not accurate any more.

In such a situation we use a different technique called
<b>permutation test</b>.
---
 The idea behind the test is quite intuitive,
though the final procedure is a bit too sophisticated for
LibreOffice. But still let me explain with an example. 
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">An example</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=3</i></legend>
[Go fast]
Suppose we have 20 mother child pairs and we have classified the
fingerprints of the 20 mothers and their 20 children as either a
whorl pattern or a no whorl pattern. This results in a 2x2
contingency table. We want to test if the mother having a whorl
pattern is independent of her child's having a whorl pattern. For
this we think of the 20 mothers standing in a row. Now we shall
play god and distribute the children among them. 
Let's say mothers with whorls are more likely to
produce babies with whorls. Then god is more likely to choose a
whorl child for a whorl mother, and a non-whorl child for a
non-whorl mother. On the other hand if there is no association
between mothers' whorl and children's whorl, then the god just
assigns the children at random to the mothers. We shall do
precisely this, artificially. Assign the 20 children randomly to
the 20 mothers without caring about whorls. This produces a new
data set which identical to the original data set in terms of the
numbers of whorl and non-whorl mothers and numbers of whorl and
non-whorl children. But unlike in the original data, here we know
that there is no association between the mothers' patterns and
the children's patterns. This new data set may again be
summarised as a 2x2 contingency table, which has the same
marginal totals, but possibly different cell entries. If we now
compute the $\chi^2 $ value from this table, then we get a
taste of how the $\chi^2 $ should look like in case of
independence, ie. an idea about the null distribution. Well, just
a single value  won't give us a clear picture about the null
distribution. But then we can repeat this procedure a large
number of times, say 1000 times. Each time assigning the same 20
children randomly to the same 20 mothers (in possibly different
orders), making a 2x2 table and computing $\chi^2 $
values. In this way we get 1000 different $\chi^2 $ values
all under the assumption of no association. Now we may compute
the $p$-value of the $\chi^2 $ value from the original
table w.r.t. these 1000 numbers. 

Of course, we are making heavy use of the computer, repeating the
entire process 1000 times, but conceptually it is not that
demanding. The next video will show this in action. 
</fieldset>

</div>


<fieldset>
<legend>m3l3_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     In the video we kept the mother fixed and randomly shuffled
     the children. Could we serve the same purpose  if we randomly
     shuffled the (mother, child) pairs?
  defaultFeedback: &gt;
      No, then the contingency table would not changed at all. We
  need to break the (mother, child) pairings randomly.
</pre>
</fieldset>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Randomise using random permutation.
[cards.ods: random number, show change. Fix, sort, then show that
fixing was not needed]
</fieldset>

</div>

<fieldset>
<legend>m3l3_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
    Create a column of numbers 1,2,...,100 in
  LibreOffice. Compute the mean of the first 50 in a cell using a
  formula like =average(a1:a100). Then randomly shuffle
  the column 10 times. See the average values change. Can you
  guess the Expectation of this random average?
  defaultFeedback: &gt;
      Since any of the 100 numbers have the same change of coming
  to the top 50 positions, the expected value is the just the
  average of 1,...,100, which is 50.5.
</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 5.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.5</i></legend>
We have seen one complicated method. Now we shall see a simpler but
crude method.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Merging.
<font color="red">
<pre>
set.seed(135317)
obs = matrix(c(46,34,5,2,47,78),2)
rtot = apply(obs,1,sum)
ctot = apply(obs,2,sum)
gtot = sum(obs)
(exp = outer(rtot,ctot)/gtot)
obs = matrix(c(51,36,47,78),2)
rtot = apply(obs,1,sum)
ctot = apply(obs,2,sum)
gtot = sum(obs)
(exp = outer(rtot,ctot)/gtot)
sum((exp-obs)^2/exp)

</pre>
</font>

</fieldset>

</div>

<fieldset>
<legend>m3l3_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If we use chisquare test of independence based on a 3x4
  table where the last two columns had to be merged, then what
  degrees of freedom would you have? 
  defaultFeedback: &gt;
      3 rows, (4-1)=3 columns. So degrees of freedom is (3-1)(3-1)=9.
</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 3, Lesson 3, Video 2: Permutation test lab">Module 3, Lesson 3, Video 2: Permutation test lab</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Permutation test: do one set of randomisation. Then show
many $\chi^2 $ values in a different sheet. 
[whorlperm.ods]
</fieldset>

</div>

<font color="red">
<pre>
p(3,3,4)
set.seed(25235)
dat = read.csv('whorlperm.csv')
n = nrow(dat)
chisq.test(dat[,1],dat[,2])
tmp = numeric(1000)
for(i in 1:1000) {
  ord = order(runif(n))
  tmp[i] =  chisq.test(dat[,1],dat[ord,2])$stat
}
csv(tmp,'1.csv')
lohist(tmp,'tmp')
</pre>
</font>



<h2><a
name="Module 3: Lesson 4: Goodness of fit test">Module 3: Lesson 4: Goodness of fit test</a></h2>

<b>Total lesson duration
= 26.9</b>
<p></p>

<div class="header">
<h3><a
name="Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 6.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.60</i></legend>
In statistics and probability we often talk about an <b>unbiased
coin</b>, a coin that when tossed is equally likely to show either a
head or a tail.
---
 We use such coins to make impartial decisions, like which
team should bat first in a cricket tournament. Now here is a
coin<b><font color="red">
[[real]] </font></b>. I want to know if it is unbiased. How do I go
about it?
---
Well, no amount of staring at it will take me anywhere. I can
start by tossing it a large numbr of times, say <b>1000</b>
times. Let's say I get <b>523 heads and 477 tails</b>. May I conclude
from it that the coin is unbiased?
---
 Well, ideally I should have
obtained <b>500</b> heads and as many tails. But a coin toss is random
after all, so we can never guarantee an exact 50% even for an
unbiased coin.
---
 So close enough to 500 is what we should look for. So the
all important question now is: Are 523 and 477 close enough to
500?  
---
Note the similarity of this situation with the independence test
that learned in the last lesson. We have<b><font color="red">
[[finger]] </font></b> observed frequncies 523
and 477 and expected frequncies 500 and 500.
---
 And we are trying to
test if the observed frequncies are too far away from the
expected ones. Let's try ... the $\chi^2 $ statistic here.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
 We compute the differences between the  the observed and the
expected frequncies, square them divide by the expected
frequencies and sum. 

Clearly, if this too large then we should think that the coin is
biased. Well, we are doing a statistical test here. So let's
spell out the two hypotheses cleary. The null hypothesis
is $H_0:$ unbiased and the altrnative is $H_1:$ biased.

Now we need the null distribution. As before we do not really
need to know the name or form of it, as the software should have
that built in. But here there is a slight twist coming up
soon. In order to prepare for that let me tell you the name of
the null distribution. It is called
the $\chi^2 $-distribution. It is not a single distribution,
but a family of distributions, where each member of the family is
indexed by a parameter called the degrees of freedom. Here are
some examples.

Our null distribution is a member of this family. Which member?
That will be given by the degrees of freedom. There is a formula
for this. If you are checking for a fixed distribution as in this
case (unbiased coin means both the probabilities are given to
be $\frac 12$) then the degrees of freedom is one less than
the number of categories. In our case we had just two categories,
head and tail. So the degree of freedom is 1.
</fieldset>

</div>


<fieldset>
<legend>m3l4_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Suppose that we are performing a test similar to that shown
  in the video to test the fairness of a die. What degrees of
  freedom should we use?
  defaultFeedback: |-
      A die has 6 faces. So degrees of freedom should be 6-1=5.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
[nsshugli.ods] Equal prob.
</fieldset>

</div>


<fieldset>
<legend>m3l4_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     Carry out goodness of fit test for the probabilities 0.2,
  0.3 0.5 using the following data. 
   | Type  | Frequency |
   |-------+-----------|
   | A     |        35 |
   | B     |        24 |
   | C     |        41 |
   |-------+-----------|
   | Total |       100 |

  defaultFeedback: &gt;
   \frac ./exraux3/gofsol.png

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.80</i></legend>
In the last two videos we learned to perform a goodness-of-fit test
for some completely specified distribution, ie, a distribution
where the probabilities for all the categories were specified as
numbers.
---
 Sometimes we need to fit a distribution with unspecified parameters in
it. Then the procedure is similar but not entirely the same. 

Let's start with an example. 
---
You should have learned about the <b>Poisson distribution</b> in your
probability course, and have possibly also learned there that it
is used for modelling the probabilities of <b>rare events</b>. In case this
sounds new to you, here is a quick refresher. 
---
Consider this event:  a soldier
getting killed by a horse kick. Fortunately for the soldiers,
such events  occur pretty rarely. They usually get to die more
glorious deaths.
---
 Even if you count all such
unfortunate deaths in a band of soldiers for a whole year, the
number should be quite low, often just zero. This is an example
of a rare event.
---
 Now there is a belief supported by some
mathematical arguments that the probability of observing exactly $k$
such events  is ...
---
<b>$$
e^{-\lambda} \frac{\lambda^k}{k!}
$$
for $k=0,1,2,...$</b> This is called the Poisson distribution
with parameter $\lambda$, which is (population) average number of events.
---
We want to test if this belief holds true for our data
set. <b><font color="red">
[[Prussian horse kick data raw]] </font></b>
[Describe data]
---
We first summarise the data in the form of a frequency
distribution table.<b><font color="red">
[[shown]] </font></b>
---
 We need to know the probabilities, which
depend on the unknown $\lambda $. Of course, that is not a
problem, as we can estimate it by the sample average from the raw data. 
<b>$$
\hat \lambda = 0.7
$$</b>
---
 Then we have the probabilities or, rather, the estimated
probabilities<b><font color="red">
[[shown]] </font></b>. From these the expected frequencies are found
easily, by multiplying these with this total.
---
Now we compute
the $\chi^2 $ statistic as usual.

So far so good. Next we need the null distribution. It is a chisq
distribution. The df is 
<b>#cat - 1 - #est param</b>.
---
 Here we have
just 4 categories. So the degrees of freedom should be 4-1=3. But
no, here we have estimated one parameter, and so the degrees of
freedom will be one less than 3, ie, 2. 

The rest is as before. We shall see the computational details in the next video.
</fieldset>

</div>

<fieldset>
<legend>m3l4_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If are testing goodness of fit of a categorical variable
  with 5 categories against a distribution  with two estimated
  parameters, then what will the degrees of freedome be? Assume
  that each category has expected frequency at least 5, and the
  sample size is large enough.
  defaultFeedback: &gt;
      #categories-1-#estimated parameters = 5-1-2=2.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 3, Lesson 4, Video 1: Goodness of fit test">Module 3, Lesson 4, Video 1: Goodness of fit test</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Prussian data lab.
[kick.ods]
</fieldset>

</div>


<fieldset>
<legend>m3l4_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Carry out the same analysis as shown in the video, but with
  this data set \frac ./exraux3/fitpoi.csv. Do you think that the
  fit with a Poisson distribution is good at 5% level of significance? 
  defaultFeedback: &gt;
      \frac ./exraux3/fitpoisol.png.
      The estimated lambda in the green cell is the sample mean.
      Notice the yellow cell. How we have taken the last category
  as "&gt;=5" to cover all possibilities. Similarly, the blue
  probability is computed to keep the total 1. 
</pre>
</fieldset>

<font color="red">
<pre>
cw('exraux3')
count = sample(0:5,200,rep=T)
csv(data.frame(count),'fitpoi.csv')
</pre>
</font>

<div class="scrpt">
<b>Total video duration
= 5.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.5</i></legend>
Did you notice one thing in the last video? The last category was
"3 or more". In the raw data we had 4 as the maximum number of
deaths. So why not have one category for 3 and another for 4?
---
Because then the expected frequncies become too small for the
chisq null distribution to hold. We shall
discuss this issue in this video with a different data.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Merging.
<font color="red">
<pre>
set.seed(135317)
x = rpois(50,lambda=1)
table(x) 
50*(1-sum(dpois(0:4,lam=1)))
# 0     1     2     3     4    5 
#19    22     4     2     2    1
#18.39 18.39  (9.20  3.07  0.77 0.18)=13.21
ob = c(19,22,9)
ex = c(18.39,18.39,13.21)
k=sum((ob-ex)^2/ex)
1-pchisq(k,df=2) #0.36
</pre>
</font>

</fieldset>

</div>

<fieldset>
<legend>m3l4_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Suppose that we have to merge some categories in a frequency
  distribution for performing a chisquare goodness of fit test
  against a completely specified distribution. 
  If the  original frequency distribution had 6 categories,and
  the merged frequency distribution has 4, then what degrees of
  freedom should we use?
  defaultFeedback: &gt;
      Since the test is done based on the merged frequency
  distribution, which has 4 categories, the degrees of freedom
  should be 4-1=3.

</pre>
</fieldset>



<fieldset>
<legend>m3_add.yml</legend>
<pre>
- typeName: peerReview
  prompt: &gt;
    Consider this data set \frac ./exraux3/typedef.csv where a
  each item in a random sample of items of three different types have been
  classified as defective or non-defective. Carry out a
  chi-square test to assess if there is any association between
  type and defectiveness. Use 5% level of significance.

  Rubric:
    1) The solution: \frac ./exraux3/typedefsol.png
    2) Compute the correct p-value (1 point)
    3) Write the conclusion (1 point)

- typeName: peerReview
  prompt: &gt;
    A certain plant can be of three types that we call types A,
  B, C  and D. Theory predicts their proportion of occurence in
  nature as 0.1, 0.4, 0.2 and 0.3. A random sample of 100 such plants
  have been been collected and classified in
  \frac ./exraux3/plant.csv. Test at 5% level of significance
  whether these satisfy the predicted proportions.

  Rubric:
    1) The solution: \frac ./exraux3/plantsol.png
    2) Compute the correct p-value (1 point)
    3) Write the conclusion (1 point)

- typeName: peerReview
  prompt: &gt;
    What are the assumptions that you must check before carrying
    out chi-square test of independence?

  Rubric
  ------
    1) Size of the sample must be large (say &gt;= 30)
    2) Each expected cell frequency should be large (say &gt;= 5)

- typeName: peerReview
  prompt: |-
    A statistician is carrying out a test of goodness of fit of
  categorical variable against a distribution involving one
  unknown parameter. If the categorical variable has 5 different
  values each with expected frequency &gt;= 5, then what degrees of
  the freedom should she use for the chi-square test? Explain.

  Rubric:
    1) 5-1-1 = 3 (1 points)
    2) Explanation: #category-1-#(estimated param) (1 point)

- typeName: peerReview
  prompt: |-
    Consider this incomplete 2x2 contingency table:
     |        | A   | B | Total |
     |--------+-----+---+-------|
     | Male   | *   |   |   230 |
     | Female |     |   |    y  |
     |--------+-----+---+-------|
     | Total  | 100 | x |       |
  If the expected frequncy under the assumption of no association
  is 46, then Find x-y.

   Rubric:
     1) The expected frequncy is 46 = 230*100/(grand total). So
        grand total is 500. (1 point)
     2) So x = 500 - 100 = 400
           y = 500 - 230 = 270
        Thus x-y = 130.  (1 point)
</pre>
</fieldset>

<font color="red">
<pre>
cw('exraux3')
type = sample(c('A','B','C'),500,rep=T)
defective = sample(c('Yes', 'No'),500,rep=T)
csv(data.frame(type,defective),'typedef.csv')
</pre>
</font>

<font color="red">
<pre>
cw('exraux3')
set.seed(434665)
type = sample(c('A','B','C','D'),100,rep=T,prob=c(0.1,0.2,0.3,0.4))
csv(type,'plant.csv')
</pre>
</font>

<h2><a
name="Module 3: Lesson 5: Review">Module 3: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 3, Lesson 5 , Video 1: Review">Module 3, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 3, Lesson 5 , Video 2: Review">Module 3, Lesson 5 , Video 2: Review</a></h3>
</div>



<h1><a
name="Module 4: ANOVA">Module 4: ANOVA</a></h1>

<div class="scrpt">
<b>Total video duration
= 2.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.50</i></legend>
We have learned about two important arms of statistical inference
in the first two weeks: estimation and testing. The third week
taught us two applications of testing.
---
 This week we shall learn about another important application
area. <b>Analysis of Variance or 
ANOVA</b> for short.
---
 What is it? Well, we shall need to spend some
time to wrap our brain around the basics. We shall do so in the
first lesson. 
---
But for now let me tell you
where it is applied. Imagine any set up where you have some inputs
and some output,...
---
 like your washing machine. The various settings as well as the
nature of the clothings you put 
into it are the inputs, and the quality of cleaning is the
output.
---
 In order to assess the performance of the machine (whether as a
designer or a user) you naturally want to explore the
input-output relation. ANOVA is the first step to that.
---
 It will let you make decisions like
"This knob is basically useless" or
"That setting has hardly any effect unless you are washing silkens." 
---
That's just one example. It doesn't have to be about a physical
machine like a washing machine. It could be like teaching a
student. That's also like a machine...conceptually.
---
 The inputs are the various pedagogic techniques, audio visual systems,
instructiin language, etc
and output is say the marks the student obtains in a final
exam.
---
 And we want to decide which inputs really matter, and which
don't. That's something ANOVA will let you answer.
---
As yet another example, consider a plot of agricultural land. You
sow some variety of crop, apply a certain fertiliser, use some
irrigation method. These are you inputs. The output is the yield
of your harvest.
---
 Again, ANOVA techniques help you to answer
questions like whether a fertiliser is really good or not.
---
We can go on and on like this. A doctor might consider a patient
as a machine, the medicines etc as input and some diagnostic
measurement as the output. An engineer may consider a production
process as the machine, and so on.
---
It is remarkable how the same reasoning may be applied to all
these diverse fields. And that's what we are about to explore
this week.
</fieldset>

</div>

<h2><a
name="Module 4: Lesson 1: ANOVA concept">Module 4: Lesson 1: ANOVA concept</a></h2>

<b>Total lesson duration
= 22.6</b>
<p></p>

<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.80</i></legend>
Suppose that you have entered a room, that you haven't been to
before<b><font color="red">
[[shown]] </font></b>. There  you find a lamp that is on. And two switches like
these, one turned on, the other off. 
---
Just by looking at these can
you conclude which switch controls the  lamp? Since only this
switch is on, and so is the lamp, it is natural to guess that it
is this switch that controls the lamp.
---
 OK, let's now try to turn
the lamp off. We flick the switch<b><font color="red">
[[shown]] </font></b>. Well, nothing happens. We now
try the other switch<b><font color="red">
[[shown]] </font></b>. Aha, now the lamp responds.
---
So what's the conclusion now? Which switch really controls the
lamp? This second switch of course!
---
This simple example contains an important maxim: when we want to
understand the relation between input and output it is more
reliable to link changes in the input with changes in the output
than value of the input to the value of the output. 
---
The switch whose state (on/off) matches the state of the lamp
need not be the one controlling it. The switch whose change of
state corresponds to the lamp's change of state is the actual
controller. 
---
There are plenty of real life examples demonstrating this maxim. I
am ill, I take a drug and get cured in a week<b><font color="red">
[[shown]] </font></b>. Does that prove
that the drug is effective?
---
 Not until I compare this with a
similar situation where the drug is not taken.<b><font color="red">
[[shown]] </font></b> May be then also I
would have been cured in a week<b><font color="red">
[[shown]] </font></b>. 
---
In that case, the input changed from drug to no drug, but the
output remained the same. So the drug is not effective.  That's why it is
important to see how the output changes when the input changes. 
---
This basic principle sits at the heart of what we shall discuss
in this module. In many branches of science as well as in
everyday life we often have to understand the input-output
relation of a system that we treat as a blackbox unit.<b><font color="red">
[[shown]] </font></b>
---
 If the unit is a blood pressure patient<b><font color="red">
[[move, shown]] </font></b>, then the
input could be a drug that may be administered in different doses
and output could be the  blood pressure measured after medication.
---
If the unit is a spring<b><font color="red">
[[shown]] </font></b>, then the input could be the load hung
from it, and the output would be the elongation of the spring.
---
 There could be multiple inputs as well. In a typical agricultural
experiment<b><font color="red">
[[shown]] </font></b>, the unit is a plot,  the inputs could
be the variety of crop, the fertiliser used,  and the output would be the yield.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">What are we trying to find?</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.60</i></legend>
The main aim here is not really to assess the input-output
relation but to check which inputs influence the output and which
do not. 
---
This is like a preliminary step before embarking upon a
formal analysis to estimate the relation.
---
 And it is here that our
maxim plays an important role. Change the inputs one at a time
and see if the output also changes accordingly.

The next video will illustrate this with a concrete example.
</fieldset>

</div>


<fieldset>
<legend>m4l1_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     The students who join a particular tutorial centre perform well in the
  final exam. Can I conclude from this that the tutorial really
  helps to well in the exam.
  defaultFeedback: &gt;
      No. Here is one objection: 
       We need to also compare with similar students who have
  not joined that centre. May be the exam is so easy that
  everybody does well. 
</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.30</i></legend>
The example that I am going to discuss here is painfully
real. Indeed so much so, that I have to hide behind symbolic
notation lest I hurt anybody's personal feeling. 
---
In a certain country the people are of two religions. Call them
<b>Religion 1 and Religion 2</b>. A commonly held belief in that country
is that Religion 2 is <b>against education</b>. 
---
And even a cursory glance at the populace seems to provide ample evidence in favour
of this belief. If you take a random sample of people following
Religion 1, and another random sample of people following
Religion 2,...
---
 then indeed the educational achievements of the
latter group would be far below that of the former. Since
education is a good thing, and people following Religion 2 have
less education,...
---
 so it is claimed that Religion 2 is bad, and
keeps its followers backward. But is this conclusion indeed justified?
---
Well, here we can visualise this as a blackbox system.<b><font color="red">
[[shown]] </font></b> Religion
is the input and education is the output.
---
 We see that a change in
Religion (ie, comparison between people of different religions)
leads to change in education. So apparently the conclusion seems
justified.
---
Now in that country Religion 1 used to be only religion, and there used to be
much oppression against the poor class. This class sought to
build a separate identity and fight back, and that's how Religion
2 gained currency.
---
 Even now  Religion 2 is embraced chiefly by
the poorer people of the country. 

Let's take a second look at our blackbox in light of this new information. 
---
So now we have  another input into our blackbox, <b>income</b>. 
This opens up a whole new interpretation. Is it really
religion that determines the educational level or is it the
income?
--- 
It is quite possible that poorer people get
less education, and since followers of Religion 2 are mostly
poor, we see a greater number of less educated persons among
them.
---
 A conclusion can be drawn by changing only a single factor
at a time, ie by comparing the educational levels of the rich and
poor within the same religion, and comparing religions among
people of similar financial status.
---
 Our earlier conclusion was that most the variation in the
education was due to diference in religions. Let's show variation
with a double arrow,<b><font color="red">
[[shown]] </font></b> the thicker the arrow the more
the variation. 
---
 The religion arrow was thicker meaning it accounted for  the
lion's share of the output variation, while this arrow was much thinner. 
---
In the new interpretation<b><font color="red">
[[shown]] </font></b> this arrow becomes big and this
becomes smaller. 
---
This idea of accounting for different parts of the variability in
the output by variation in the inputs is called <b>ANalysis Of
VAriance or ANOVA</b>. Because we are splitting or analysing the
variation and assigning the parts to different heads.
</fieldset>

</div>


<fieldset>
<legend>m4l1_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the last exercise once again. Come up with another
  objection in light of the video you just watched. 
  defaultFeedback: &gt;
         May be there is some hidden factor like financial
  status. Only rich students can afford to join that institute,
  and being rich they also afford better education elsewhere.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.4</b>
<p></p>


<fieldset>
<legend>
(s1)[a] <i>Duration=3.40</i></legend>
When we draw a blackbox like this<b><font color="red">
[[shown]] </font></b> it looks like a machine that
takes this input and produces that output. Now the output of a machine is
supposed to be completely determined by its inputs.
---
 However, consider this example, where the unit is a plot, input is crop
variety and  output is yield.<b><font color="red">
[[shown]] </font></b> Even if I take two
identical plots, sow the same variety,
is it guaranteed that the yield will be exactly the same?
---
 No, there are
bound to be many imponderable factors that will make the two
yields slightly different. In order to show this schematically in
our box diagram we add this input<b><font color="red">
[[shown]] </font></b> and call it the random
error.
---
 Now when we ascribe parts of the output variation to
different inputs, the part that we could not explain using the
other inputs, all those imponderable effects are put under this
head.
---
To appreciate let's consider comparing the yields of three
different varieties of the same crop. There are say 5 fields
under each variety, so 15 fields in 
all. We measure the yield of each plot, and that's our output.<b><font color="red">
[[shown]] </font></b> 
---
Here we have shown all the output values in a number line. So we have 15
points. We have shown the varieties using different
colours. There are 5 points of each colour.
---
 Here the points of
the same colour are all tightly together with ample gap between
the points of different colours. What can we conclude from this?
 That the varieties are really different in terms of yield.
---
 Why are the points of the same colour  not exactly together?
That is because of random error. Thus, random error has its
contribution no doubt, but the effect of the varieties is far
stronger.
---
 We denote this using arrows like this. 
A fat arrow for the variety input, and a thin one for the random
error.<b><font color="red">
[[shown]] </font></b>
---
Compare this with this situation.<b><font color="red">
[[shown]] </font></b> Now the points of
different colours are all mixed together. Shall we say here that
the varieties really have much effect on the yield?
---
 No, here chance plays the dominating role. So now the random error gets
the fatter arrow<b><font color="red">
[[shown]] </font></b>, and to maintain the same total, the variety
input gets the slender arrow.
---
This simple example introduces possibly the most important
concept in the whole of ANOVA: using the variation due to the random
error input as the yard stick.
---
 We say that an input has significant effect on the output if and
only if its effect is appreciably larger than what we anyway expect from mere
chance. 
---
Thus ANOVA is not only just about ascribing variablities
in the output to the different inputs (including random error),
but also expressing the variations contributed by all the inputs in terms
of that due to the random error. 
</fieldset>

</div>


<fieldset>
<legend>m4l1_c.yml</legend>
<pre>
- typeName: multipleChoice
  prompt: &gt;
           We take 2 girls and  2 boys from the same class in the same
  school. We consider their mathematics marks and physics
  marks. Consider this as a blackbox system with inputs gender and
  subject (and random error) and output marks. Then which of the
  following differences sheds light on the contribution of the
  random error input?

  defaultFeedback: &gt;
      You must compare outputs of two units for which gender and
      subject are the same.
  shuffleOptions: true
  options:
  - answer: between physics marks of the two girls
    isCorrect: true
    feedback: &gt;
      
  - answer: between math marks and physics marks of the same boy 
    feedback: &gt;
      
  - answer: between math marks of a boy and that of a girl
    feedback: &gt;
      
  - answer: between math marks of a girl and physics mark of boy.
    feedback: &gt;
      
      

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.30</i></legend>
We have already presented the two important pillars of
ANOVA. Let's start by reminding ourselves.<b><font color="red">
[[box shown]] </font></b>
---
* One, splitting the variation<b><font color="red">
[[shown]] </font></b> in the output into components
  ascribable to the various inputs including the random error.
---
* Two, using the variation due to the random error as the yard
  stick of how much variability we must endure, and measuring the
  variations due to the other inputs relative to that.
---
Traditionally this idea is presented in the form of a table
called the <b>ANOVA table</b>. These tables have many intricate details
that need not concern us here.
---
 But it is important to know the basic structure.<b><font color="red">
[[shown]] </font></b>
We have shown only the most 
important columns:<b><font color="red">
[[finger]] </font></b> source, this SS stands for Sum of Squares,
then F and finally p.
---
 The table has one row for each of the arrows (including
the random error and output). The total row stands for the
output. 
---
 The source column tells us which  row is
for which input<b><font color="red">
[[anim shown]] </font></b>.
---
 In our schematic box diagram, these double arrows denote
variabilities, fatter means higher variability, thinner means
lower variability.
---
 These are quantified<b><font color="red">
[[finger]] </font></b> as something called the
Sums of Squares, or SS  for short. These populate the second
column in our table<b><font color="red">
[[anim shown]] </font></b>.
---
 This total sum of squares is the
total variablity in the output, which is being decomposed into
sums of squares due to the different sources. 
---
After this come a few more
columns which are of technical nature and hence would be skipped
here. Finally we want to express the different sums of squares
relative to this error sum of squares.
---
 In other words, we divide
these by this. Again, there are some technicalities that I am
skipping here. But the F column does basically this. Now we check
if any of these numbers is too large.
---
And this too large is measured as usual by a p-value, that's why
we have the last column.
 Any of these p-values falling below 0.05 indicates that the
corresponding source has a significant effect on the output.  
---
As you can see, I am glossing over certain details, partly
because the math is a bit involved, and mostly because they play
no role in the interpretation of the result.
---
 And interpretation of the table is what we, as practising statisticians, are
primarily interested in. Still let me give you a taste of the
mathematics in a particularly simple case.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">A simple mathematical analysis</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
Consider our number line example once again. We had three
varieties of the same crop and measured the yields from 15 plots,
5 for each variety.<b><font color="red">
[[shown]] </font></b> 
---
The variability among the points of the same colour indicate the
contribution of the random error. The variability among point
clusters of the different colours is due to the contribution of
the varieties.
---
 To express these variabilities mathematically, let
us introduce a notation system. We shall denote the yields by the
letter $y.$<b><font color="red">
[[shown]] </font></b> Also we shall attach a numbering scheme to the
15 plots.
---
 Each plot will be indexed by a pair $(i,j),$<b><font color="red">
[[shown]] </font></b>
where $i$ denotes the <b>variety</b> sown in it ($i=1,2,3$)
and $j$ is like a <b>roll number</b> that runs from 1 to 5 within
the same variety.
---
 Then the yield from the $(i,j)$-th plot
will be denoted by $y_{ij}.$ 
---
Thus the dots of the same colour correspond to $y_{ij}$'s
with the same $i$ and different $j$'s. Let's mark the
centres of each colour cluster.<b><font color="red">
[[shown]] </font></b>
---
The centre of the $i$-th cluster is <b><font color="red">
[[finger]] </font></b> $\bar
y_{i\bullet}.$ Note the dot in place of the $j$ index,
over which which we have averaged.
---
The variability
in the first cluster therefore may be quantified as 
<b>$\sum_j(y_{1j}-\bar y_{1\bullet})^2.$</b>
---
 Similarly for the other
colours. So the total contribution of chance is the
sum <b>$\sum_i\sum_j (y_{ij}-\bar y_{i\bullet})^2.$ </b>
---
On the other hand, when we want to measure the contribution due to
the varieties, we consider dots of each color as a single class
and measure the variability between the classes.
---
 For this imagine all the points in the same class to be concentrated at the center
of the class.<b><font color="red">
[[finger]] </font></b> Let the overall mean be $\bar
y_{\bullet\bullet}.$<b><font color="red">
[[shown]] </font></b>
---
 Then the variability between the classes
corresponds to <b>$\sum 5 (\bar y_{i\bullet}-\bar
y_{\bullet\bullet})^2.$ </b>
---
Finally, the total variability present in the output, ie.,
all $y_{ij}$'s is <b>$\sum_i \sum_j (y_{ij}-\bar y_{i\bullet})^2.$ </b>
---
It comes as a pleasant surprise that we have the following
algebraic identity:
<b>$$
\sum \sum (y_{ij}-\bar y_{i\bullet})^2 = \sum J (\bar y_{i\bullet}-\bar
y_{\bullet\bullet})^2 + \sum_i\sum_j (y_{ij}-\bar y_{i\bullet})^2.
$$</b>
---
We shall prove this in the next video, and is exactly what we expected
intuitively. It is one of those moments where intuition is
borne out by mathematics exactly. 
---
When we write an ANOVA table we write precisely these quantities:
<b>
<pre>
------------
Source    SS
------------
Variety  BSS
Error    ESS
------------
Total    TSS
------------
</pre>
</b>

</fieldset>

</div>

<fieldset>
<legend>m4l1_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If we add 5 to all the outputs corresponding to i=1, then
  which sum of
  squares would not change? 
  defaultFeedback: &gt;
      The Error SS. It never points of one colour with points of
  a different colour.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 1, Video 1: ANOVA story">Module 4, Lesson 1, Video 1: ANOVA story</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Proof of the 1-factor ANOVA identity.
</fieldset>

</div>




<h2><a
name="Module 4: Lesson 2: ANOVA terms and data layout">Module 4: Lesson 2: ANOVA terms and data layout</a></h2>

<b>Total lesson duration
= 23.4</b>
<p></p>


<div class="header">
<h3><a
name="Module 4, Lesson 2, Video 1: ANOVA terms">Module 4, Lesson 2, Video 1: ANOVA terms</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.10</i></legend>

When dealing with an ANOVA problem, it is important to think about
the blackbox diagram<b><font color="red">
[[shown]] </font></b>. There must be<b><font color="red">
[[finger]] </font></b>
at least one input apart from 
the inevitable random error input, and exactly one output.
---
 The traditional theory of ANOVA requires the output to be <b>continuous,</b>
and the random error input to be continuous as well. The other
inputs may be categorical or continuous. 
---
If all the <b>non-random inputs are categorical we call it an ANOVA</b>
set up. If all the non-random inputs are <b>continuous we generally
call it a regression set up</b>.
---
 If there is at least one categorical and one continuous
non-random input, then it is an ANCOVA set 
up.<b><font color="red">
[[shown]] </font></b> Here ANCOVA means <b>ANalysis of COVAriance</b>.
---
The categorical inputs are called <b>factors</b> and the continuous
inputs are called <b>covariates</b> in ANOVA parlance. 
---
If there are exactly $k$ categorical inputs and no
continuous input, then we have a $k$-factor ANOVA.
---
 If we also have at least one
continuous input, then it is $k$-factor ANCOVA. The number
of covariates does not feature in the nomenclature.
---
 This terminology however, is not entirely standard. Some people use
the term $k$-factor ANOVA only when each factor combination
occurs exactly once in the data set. This is what Libreoffice
uses as well. It will be clear when we see examples in the lab.
---
In this module we shall focus on only 1-factor and 2-factor
ANOVA. 

Next let us understand the nature of data that we need in order
to carry out ANOVA.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Data</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=2.30</i></legend>
The main aim in ANOVA is to explore a blackbox system to see
which inputs have any appreciable effect on the output. For this
it is important to keep two  basic principles in mind: 
---
* First, <b>change each input</b>

* Second, <b>change them one at a time. </b>
---
Let's take a closer look at the first<b><font color="red">
[[shown]] </font></b>. If you are interested in
assessing if a drug is effective, it is not enough to apply the
drug to some patients and see its effect.
---
 You must also not give the drug to some other similar patients,
and see the difference between the patients who got the drug and those who
didn't.
---
 This gives rise to the wellknown concept of <b>placebo</b> in
clinical studies, where a dummy drug is given to one group and
the true drug is given to the other group.
---
 The dummy drug has no medical value, it is just to keep the patients happy so that
no psychological effect creeps in. The group of patients who did
not get the real drug is usually called the <b>control
group</b>. 
---
The other group that gets the real drug is the <b>treatment
group</b>.
---
Next we come to the second point: <b>changing the inputs one at a time</b>.
If you always change multiple inputs simulateneously it
will be impossible to figure out which input caused the output to
change.
---
 It's like I have a fever. My wife suggests visiting an
alopathic doctor while a friend suggests seeking homeopathic
medication. Unwilling to displease either, I take both types of
medicine, and get cured.
---
 Now which doctor should get the credit?
Or may be the medicines reacted together to form a new chemical
which actually cured me!
---
 Such a situation where two or more inputs have
changed together so that the change in output cannot be clearly
ascribed to either is called <b>confounding</b>. And this should better
be avoided. The next video will discuss how.
</fieldset>

</div>

<fieldset>
<legend>m4l2_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     In order to assess the effectiveness of a drug we take a
  random sample of  10 male patients and 10 female patients. Then
  we apply the drug   to the men and the placebo to the
  women. What's wrong with this   approach? 
  defaultFeedback: &gt;
      Gender effect is confounded with drug effect. If the men
  heal quickly, it could be because of the drug or because
  somehow men have better resistance to that particular ailment.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 3.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.70</i></legend>
To avoid confounding we have to collect our data carefully. Let's
consider this situation.<b><font color="red">
[[shown]] </font></b>
We have<b><font color="red">
[[finger]] </font></b> a drug whose effectiveness we are trying
to test, as measured by its influence on  blood pressure.
---
We suspect that smoking habit might influence the
effectiveness. 

So we have two inputs drug and smoking habit. We
know that each input should be changed.
---
 Now I have two values for <b>smoker</b>: <b>yes and no</b>. 
But I have only one <b>drug</b>. How do I change that
input? Well, in order to assess effectiveness of the drug we need
to compare it with the effect of applying no drug.
---
 And just not to let a patient feel any different psychologically, we disguise
the "no drug" externally as a "drug" (like a harmless tablet or
syrup, saline injection) called a placebo.
---
 Thus we have two possible values for the drug input: <b>real and
placebo</b>.
---
So I have a 2x2 layout<b><font color="red">
[[shown]] </font></b>. It is important that I have
representatives from all the 4 cells.<b><font color="red">
[[finger]] </font></b> That is, I must give the
real drug to some smokers, placebo to some other smokers,...
---
 the real drug to some non-smokers and also the placebo to some
non-smokers.
---
To assess the effecacy of the drug <b><font color="red">
[[finger]] </font></b> we can then
compare this group with this, and this group with this. But if we
just confine our study to these cells,...
---
 ie, give the real drug to only smokers and placebo to only
non-smokers then drug effect and smoking effect will be confounded.
---
 This way of laying out the
patients in a rectangular array allowing for all possible
combinations is called a <b>factorial design</b>. 
---
The same principle of changing only one input at a time when
applied to the random error input  gives
rise to another concept called <b>replication</b>. Remember that random
error is an input<b><font color="red">
[[finger]] </font></b>, that we have no control
on.
---
 So it is always changing. In order to let it change alone we just have to hold all
the other inputs fixed for some time. Let's understand this with
the drug example. We have the 2x2 layout.
 Let's have at least two patients per cell<b><font color="red">
[[shown]] </font></b>
---
 When we compare the outputs for the patients in the
same cell their variation must be solely due to the random error
input, and so we can form an idea about the inevitable amount of variability
that we may expect due to chance alone.
---
 As we know this amount is
very important, as this is going to be yard stick w.r.t which all
the other variabilities are to be measured.
---
If the units are human beings, then we often assign them to
different groups <b>blind</b>ly, ie, without telling them their true
groups. If there are two drugs, they are given identical external
appearance.
---
 Sometimes even the experimenter is not allowed to
know the true groups until the data collection and analysis is
finished. This is called <b>double blind</b>. Such experiments
are most commony used in the clinical trials. 
</fieldset>

</div>

<fieldset>
<legend>m4l2_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     The more repeated measurements we take keeping the
  (non-random) inputs  fixed, the better we get to know the
  variability due to the random error input. Can you connect this
  with the standard error formula of sample mean that you had
  seen in module 1?
  defaultFeedback: &gt;
      There we had sqrt(n) in the denominator. So more the sample
  size the more precise if sample mean as an estimator. These two
  are not exactly the same concept, but similar in their effect.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 2, Video 1: ANOVA terms">Module 4, Lesson 2, Video 1: ANOVA terms</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.6</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=0</i></legend>
In order to perform ANOVA we need our data to be laid out in a
particular format. While most softwares use a standardised form,
LibreOffice and MS-Excel use a slightly simpler format. Let's
understand the standard format first.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Standard format</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=4.5</i></legend>
When collecting data it is good to think in terms of the balckbox
diagram. Each input arrow (excluding the random error) and output
arrow corresponds to a variable, ie, a column in the data
matrix. Each time you measure the values for the inputs
(excluding the random error, of coure) and the output, you get a
case. 

Let's consider a concrete example. Each unit in the blackbox is
an insomnia patient. The inputs are drug, age group and
gender. The amount is the increase in amount of sleep after
medication. Let's say drug has three levels Drug 1, Drug 2 and
Placebo. There are three age groups, Young, Midle, Old. Two
genders Male and Female. So imagine that we approach each patient
with a blank form where the variable names are fields. We fill in
the blanks approapriately for each patient. Collating all the
filled in forms we get a data matrix like this. This is the data
layout required by most standard statistical softwares like R, SAS,
SPSS, Systat, Stata. 
</fieldset>

</div>


<fieldset>
<legend>m4l2_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     We have collected data on the marks of 10 boys and 8 girls
  in mathematics, statistics and physics examinations. Our aim is
  to see the effect of gender and subject on marks. If the data
  are presented as a data matrix in the format discussed in the
  video above, then what will the size of the matrix be?
  defaultFeedback: &gt;
      18 rows and 3 columns. One row per student. One column for
  gender, one for subject and one for marks.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 5.7</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=0.60</i></legend>
In the last video we learned about the data required by most
standard statistical software for ANOVA. However, LibreOffice expects a
different layout, which is somewhat more intuitive.
---
 First let me
admit that LibreOffice cannot handle ANOVA in its full
generality. It can handle only 1-factor and 2-factor ANOVA.
---
 The simple data layout that LibreOffice expects is possible only for
these simple cases. So let's simplify our example to retain only
the Drug input. Then we have a 1-factor ANOVA set up. <b><font color="red">
[[box
diag shown]] </font></b>
---
Here we have just a single input (except the random error
input), and a single output. Now LibreOffice expects the output
values for the different values of the input in different
columns. Let's see this in the lab.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
One-way LibreOffice layout.
oneway.ods
</fieldset>

</div>


<fieldset>
<legend>m4l2_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider this data set \frac ./exraux3/stan.xlsx. How would
  you arrange this for 1-factor ANOVA in LibreOffice? 
  defaultFeedback: &gt;
      \frac ./exraux3/lib.png

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 4.9</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=1.80</i></legend>
In this video we shall learn about the <b>2-factor layout</b> of
data. Just as for 1-factor layout, there are two versions
possible here, a simple one used by LibreOffice and a general one
used by most standard softwares. 
---
Either way, we start from the box diagram<b><font color="red">
[[shown]] </font></b>, two inputs (plus the
random error input) and one output. Both the inputs are factors,
ie, categorical variables,...
---
 like in an agricultural study<b><font color="red">
[[shown]] </font></b> the
variety of crop and the fertiliser. Here the unit is a plot of
land, and the output the yield.
---
 If there are 3 varieties and 2
fertilisers, then we have a 3x2 rectangular layout<b><font color="red">
[[shown]] </font></b>. In each cell
we should better have at least 2 plots to get a good idea about
the variation caused by the random error input by itself.
---
 However, LibreOffice
deals with the simplest case, where there is just a single plot
per cell. This suits well with its spreadsheet nature.
---
 You just 
write the yields as in a rectangle with row headings as well as
column headings.
---
We shall soon see this in action in the lab. But before that let
me tell you the general layout that is accepted by other
softwares.
---
 Here each arrow (input/output, but not random error)
is a variable, and hence get a column of its own.<b><font color="red">
[[shown]] </font></b> Each case gets
its row. If there are multiple plots per cell, then we just have
as many rows for that input combination.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Lab</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=3</i></legend>
Two-way in Libre Office
[twoway.ods]
</fieldset>

</div>


<fieldset>
<legend>m4l2_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider this data set \frac ./exraux3/stan2.xlsx for a 2-factor ANOVA in the standard
  format. Convert this to LibreOffice format. 
  defaultFeedback: &gt;
      \frac ./exraux3/lib2.png

</pre>
</fieldset>



<h2><a
name="Module 4: Lesson 3: ANOVA table">Module 4: Lesson 3: ANOVA table</a></h2>

<b>Total lesson duration
= 30.8</b>
<p></p>

<div class="header">
<h3><a
name="Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 1.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.50</i></legend>
We have aleady seen <b>ANOVA table</b>s. They sit at the heart of ANOVA
and is the traditional way to present the result of ANOVA. Let us
take an example. We are comparing <b>three varieties</b> of paddy and <b>two
fertilisers</b> in terms of the yield.
---
So we have 6 plots<b><font color="red">
[[shown]] </font></b>, these <b><font color="red">
[[finger]] </font></b> are under fertiliser 1,
these under 2. Similarly, these are under variety 1, these under
variety 2, these under 3.
---
To understand ANOVA tables it is important to visualise the set up
as a blackbox diagram<b><font color="red">
[[shown]] </font></b>.
---
 In the basic version of the ANOVA table there are as many rows as the
number of input arrows (including the random error).<b><font color="red">
[[shown]] </font></b>  Plus there
is one total row for the output arrow.<b><font color="red">
[[shown]] </font></b>
---
 The very first column is called the <b>Source</b> column and
describes each input arrow. The last 
non-total<b><font color="red">
[[finger]] </font></b> row is always reserved for the random
error.
---
The next column is for the variabilities called <b>sum of
squares</b>. To understand this we need 
to go back to our basic identity. And that's what we shall do in
the next video.
</fieldset>

</div>


<fieldset>
<legend>m4l3_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     What is the last row before the "total" row used for? 
  defaultFeedback: &gt;
     For the random error input. 

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=0</i></legend>
Now we shall learn about the ANOVA table. Many text books present
it as a crowd of formulae, which often look pretty
scary. Actually the 1-way ANOVA table is just a fancy way of
expressing...the basic identity. 
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Basic identity BSS, WSS. Their positions in ANOVA table. df. The
other columns. Explain F test.
</fieldset>

</div>

<fieldset>
<legend>m4l3_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     What is the interpretation if the entry in the F column is singificantly large? 
  defaultFeedback: &gt;
     The input  has significant effect on
  the output. 

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 3, Video 1: ANOVA table">Module 4, Lesson 3, Video 1: ANOVA table</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Fake data, one way.
[oneway.ods]
</fieldset>

</div>

<fieldset>
<legend>m4l3_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Use LibreOffice to construct 1-factor ANOVA table for the
  data in \frac ./exraux3/stan.xlsx. 
  defaultFeedback: &gt;
      \frac ./exraux3/aov1.png

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Start with 1-way set up, cast it as model. Explain a 2-way set
up. Mention additive. Refer to non-additive to come later.
</fieldset>

</div>

<fieldset>
<legend>m4l3_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Is the following model a 2-factor ANOVA model?
       y_ij = a + b_i + e_ij,
     where e_ij's come from a normal distribution with 0 mean.
  defaultFeedback: |-
      No, it is a 1-factor ANOVA model.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 6.3</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6.3</i></legend>
2-way ANOVA table. Brief explanation.
[aov2.png]
</fieldset>

</div>


<fieldset>
<legend>m4l3_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     Consider first and last columns of 2-factor ANOVA table:
      | Source     | ... | p-values |
      |------------+-----+----------|
      | Crop       | ... | 0.01     |
      | Fertiliser | ... | 0.13     |
      | Error      | ... | ---      |
      |------------+-----+----------|
      | Total      | ... | ---      |
     Which of the inputs has/have significant effect on the
  output at 5% level of significance?
  defaultFeedback: &gt;
      Crop, but not fertiliser, because the p-value from crop is
  below 0.05, but not the other p-value.

</pre>
</fieldset>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
2-way ANOVA with LibreOffice.
[twoway.ods]
</fieldset>

</div>

<fieldset>
<legend>m4l3_f.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Construct 2-factor ANOVA table with LibreOffice for the data
  set \frac ./exraux3/stan2.xlsx
  Which of the two inputs has/have significant effect on the 
  output at 5% level of significance? 
  defaultFeedback: &gt;
  \frac ./exraux3/aov2tab.png    
  Neither input has significant input, because both the p-values
  are &gt; 0.05.
</pre>
</fieldset>


<h2><a
name="Module 4: Lesson 4: Interaction">Module 4: Lesson 4: Interaction</a></h2>

<b>Total lesson duration
= 23.3</b>
<p></p>

<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.90</i></legend>
We had started our discussion of ANOVA with a story, the story of
a room with a lamp and two switches. The actual circuitry that
connected the switches to the lamp, ie,  the inputs to the output,
was unknown,...
---
 a blackbox, and we had to try out various values for
the inputs and observe the output values for them in order to get
an idea of the inner working of the blackbox. 
---
Well, we are back to a similar room<b><font color="red">
[[shown]] </font></b>, again two switches and a
lamp. But this time everything is turned off. Our aim as before
is to figure out the input output relation.
---
 Let's start by
flicking this switch<b><font color="red">
[[shown]] </font></b>. Well, nothing happens. Looks like this is
not our switch. OK, let's return
it to its original state.<b><font color="red">
[[shown]] </font></b> Now we flick the other
one.<b><font color="red">
[[shown]] </font></b>
---
 Oops, this also seems to be ineffective. Or may be the lamp is fused? In
dismay we play with the first switch again, turn it on<b><font color="red">
[[shown]] </font></b>. Wow, the
lamp turns on. But didn't we turn this switch on just now also,
but did not get any effect?
---
 How come it has suddenly come back to life?
May be this switch is alive too now. Let's turn it off<b><font color="red">
[[shown]] </font></b>. Wow, this
also seems to control the lamp. Earlier neither seemed to have
any control, and now mysteriously both are controlling the
lamp.
---
 So I can control the lamp by the top switch <b><font color="red">
[[flick],] </font></b> 
oops it has gone to sleep once again. Let's try the
other one. <b><font color="red">
[[flick]] </font></b> Boy, this is dead again too! This room sure is
spooky. Or is it?
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">The mystery</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.70</i></legend>

<b><font color="red">
[[shown]] </font></b>Well, there is nothing really spooky about the switches. They are
basically connected in series like this<b><font color="red">
[[shown]] </font></b>.
---
 So when any one of them is
off, the circuit is already broken, and  the other switch has no
control. But if one switch is turned on, then it fate of the lamp
is controlled by the other switch. 
---
Now we are not here to discuss electric circuits really. My point
here is that it is possible to have situations more complex than
what we had started with in our first example, where each switch
was either controlling the lamp or not.
---
 Each behaved independently of the other. So if we asked "Does
this switch control the lamp?" the answer was eiter a clear yes,
or a clear no. 
---
But here the effect of one switch
in influenced by the other switch. So if now ask "Does this
switch control the lamp?", then we do not have a simple yes
or no anwer to the question.
---
  The answer depends on the curent state of the other switch. If that switch is
off, then the answer is no, if that switch is on, then the answer
is yes. 
---
In such a situation where two inputs are kind of entangled
together, we say that there is interaction between the two
inputs.
---
Let's look at it in the context of statistics.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Interaction</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.40</i></legend>
In general think of our familiar blackbox diagram. We say
two inputs have <b>interaction</b> if the effect of one depends on the
the value of the other.
---
It does not always have to be just two inputs. It is quite possible for even three or
more inputs to be involved in an interaction. Their combined effect
is called the <b>interaction effect</b>.
---
Had there been no interaction,
then their individual effects are called <b>main effects</b>.
Remember: main effect makes sense only when interaction is absent.
---
In any ANOVA problem with at least two inputs (other than the
random error input), we should worry about their
interaction.
---
 Before we can meaningfully talk about the effect of
any single input, we need to ascertain that it is not involved in
an interaction with the some other input. It is possible to test
this using a statistical test of hypotheses. 
---
However, the math is a little involved. Also Libreoffice does not
allow us to compute the interaction. So we shall restrict
ourselves to a pictorial method of assessing presence or absence
of interaction. We shall see this in the next video.
</fieldset>

</div>

<fieldset>
<legend>m4l4_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
    We want to assess the effect of online versus offline
  teaching on students of different fields. For this purpose 50
  literature students and 50 physics students are selected
  randomly. Then half of each group are taught using online
  techniques while the other half is taught offline. Their grades
  in a final exam are recorded as output. In this scenario would
  you suspect interaction between the subject input (literature/physics) and the
  method input (online/offile)?
  defaultFeedback: &gt;
      Yes. For literature online and offline hardly makes much
  difference. But for physics it makes more difference (eg
  laboratory handling may make ofline teaching more efrctive
  while computer graphics techniques may make online teaching
  more attractive.)

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.7</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=0.70</i></legend>
In the last video we saw an expository example of
interaction. Here we shall see how interaction occurs in real
life.
---
 In any example of interaction we must have at least two
inputs (excluding random error). Our first example will be where
the two inputs linked by interaction are both categorical. 
---
Consider an agricultural experiment where we are interested in
assessing the effect on yield of <b>two varieties</b> of a crop and <b>three
different fertilisers</b>. Here is the ... blackbox diagram.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Here is the ... blackbox diagram<b><font color="red">
[[shown]] </font></b>. Two inputs
variety and fertiliser (plus of course the inevtiable random
error). The output is yield.
---
 Typically such an experiment will
need as 2x3 layout like this. In each of the 6 cells we shall
take at least two plots.<b><font color="red">
[[labelled grid shown]] </font></b>
---
 Let's take exactly 2 plots in each. By the way,
this is called a <b>balanced layout</b>, since we take the same number
of plots in each cell. We shall index the plots
as <b>$(i,j,k)$</b>,...
---
 where $(i,j)$ refers to the cell
ie, $i$-th variety and $j$-th fertiliser, and $k$
is serial number (which is either 1 or 2) within the cell. The yield
from the $(i,j,k)$-th plot will be called <b>$y_{ijk}.$ </b>
---
To appreciate interaction in this context let's compute the
average yield for each of the 6 cells:
<b>$$
\bar y_{ij\bullet} = \frac{y_{ij1}+y_{ij2}}{2}.
$$</b>
---
Now consider averages in the first row, ie for variety 1. We
plot them<b><font color="red">
[[shown]] </font></b> as three three points, and join them with
lines.
---
 Since we are working with categorical variables in the
horizontal axis, joining them with lines is not entirely
justified, but still it is a common practice  to aid visual
interpretation.
---
  The shape that we get like this will be called
the<b> profile</b> for variety 1. Next we shall draw the profile for
variety 2, on the same plot<b><font color="red">
[[shown]] </font></b>.
---
 The profiles turn out to be more or less parallel. This allows
us to meaningfully ask questions about 
the varieties and fertilisers separately.
---
 For instance, which fertiliser produces the highest yield? Answer is fertiliser
2. Which variety yields less? Variety 1. 
---
Now consider another example. Now the yields from the plots are such that the two
profiles are not parallel.<b><font color="red">
[[shown}{anim]] </font></b> Now the question "Which fertiliser
is the most productive?" does not have a clear answer.
---
 We need to know which variety we are talking about. Fertiliser 3 is the best
for variety 1 and fertiliser 3 for variety 2. 
---
Since the effect of variety now is influenced by the choice of
variety, we say that we have <b>interaction</b> between variety and fertiliser
here. In the earlier case there was no interaction.
---
 This chart is
called an <b>interaction chart</b>. It is a valuable graphical device in
ANOVA whenever there are multiple categorical inputs.

In the next video we shall see how to make such charts in LibreOffice.
</fieldset>

</div>

<fieldset>
<legend>m4l4_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If there are three profiles and two are parallel, while
  third is not parallel to them, then does that indicate presence
  of interaction? 
  defaultFeedback: &gt;
      Yes. For a situation to be free of interaction all the
  profiles must be more or less parallel.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Interaction chart.
[inter.ods: fert -&gt; row, var -&gt; col]
</fieldset>

</div>

<fieldset>
<legend>m4l4_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the data in \frac ./exraux3/stan2.xlsx. Make an
  interaction chart. Does it show interaction? 
  defaultFeedback: &gt;
    \frac ./exraux3/interplot.png
    Not much interaction really. Indeed the two profiles are
  basically the same with minor random ups and downs.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.40</i></legend>
We have seen examples of interaction between two categorical
inputs, or factors, as they are called.
---
 In this video we shall see how a factor input may be
locked in interaction with a continuous input, ie a
covariate.<b><font color="red">
[[fac-cov-inter shown]] </font></b> 
---
We shall consider a toy data set where we have three variables height,
weight and gender of some adult persons. We take 20 people,
10 male and 10 female, and record their heights and weights.
---
Our aim is to see if
height has influence on weight, and if the height weight relation
is the same for both genders. 
---
Let's make a scatterplot using colour coding for gender.<b><font color="red">
[[shown]] </font></b>
Here blue is for male, and red for female. Notice that all the 20
points follow the same linear pattern. 
---
 We are deliberately using a toy data set here, so that we can
demonstrate another possibility. What you see now is the first
possibility, where the effect of height on weight is not
influenced by the person's gender.
---
 If I ask the question:
<b>"how many extra kilos for an extra inch?"</b> the answer is the slope
of the linear pattern. The same value works for either
gender. This is the "no interaction" case.
---
Now suppose that the female points are rotated
slightly.<b><font color="red">
[[shown]] </font></b> Now the male cluster and the female cluster
each has a linear pattern, but the slopes are different.
---
 So in order to answer the same question, we now need to know the
gender in question, so that we know which linear pattern we
should take the slope of. 
---
Here we have interaction, interaction
between the categorical input gender and the continuous input height.
---
As we know main effect ceases to be meaningful in presence of
interaction. It is of course possible to average out the two
slopes and report that as the effect of height on weight
irrespectivel of gender. But that is actually meaningless. 
---
The next video will quickly remind us how to create a colour
coded scatterplot like the one used here.
</fieldset>

</div>

<fieldset>
<legend>m4l4_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If in the scatterplot shown in the video the male points and
  female points lay along two distinct but parallel lines, should
  we consider that as an interaction between gender and height? 
  defaultFeedback: &gt;
      No. The effect of height is only through the slope.

</pre>
</fieldset>

<div class="header">
<h3><a
name="Module 4, Lesson 4, Video 1: Interaction (through story)">Module 4, Lesson 4, Video 1: Interaction (through story)</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Color coded scatterplot.
hw2.ods
</fieldset>

</div>



<fieldset>
<legend>m4_add.yml</legend>
<pre>
- typeName: peerReview
  prompt: &gt;
    Consider this data set \frac ./exraux3/contam.csvwhere water contamination levels are
  given at 5 points in different localities. Create a 1-factor
  ANOVA table to test if contamination levels in the all the
  localities are more or less the same. Use 5% level of
  significance.

  Rubric:
    1) Make proper data layout for LibreOffice 
       (negligible should be converted to 0)
          \frac ./exraux3/rublayout.png (0.5 point)
    2) Get the table:
          \frac ./exraux3/rubaov.png (1 point)
    3) Conclude that since p-value 0.66 &gt; 0.05, we accept at 5%
    level of significance the
       null hypothesis that the contamination levels in the all
    the localities are more or less the same.
- typeName: peerReview
  prompt: &gt; 
    Here is an interaction chart: \frac ./exraux3/inter.png
    Does it indicate presence of interaction? Justify your answer.

  Rubric:
    1) No, there is no interaction (1 point)
    2) Justification: the profiles are parallel (1 point)

- typeName: multipleChoice
  prompt: &gt;
      A doctor wants to test the efficacy of a blood pressure
  drug that may be applied in two different doses other than the
  placebo. He tests it on a random sample of 10 males and 9
  females. Then what will be degrees of freedom for 
  gender in a 2-factor ANOVA table without interaction?
  defaultFeedback: &gt;
      
  shuffleOptions: true
  options:
  - answer: 1
    isCorrect: true
    feedback: &gt;
      
  - answer: 2
    feedback: &gt;
      
  - answer: 9
    feedback: &gt;
      
  - answer: 18
    feedback: &gt;
      
- typeName: checkBox
  prompt: &gt;
      Consider a 2-factor ANOVA model with interaction, where the
  two inputs are drug (drug 1, drug 2 and placebo) and age group
  (yong, middle aged, old). Which of
  the following questions is/are meaningless in presence of
  significant interaction?
  defaultFeedback: &gt;
      
  shuffleOptions: true
  options:
  - answer: Which drug is the best?
    isCorrect: true
    feedback: &gt;
      
  - answer: &gt;
      Is there any significant effect of the age
      groups on the output?
    isCorrect: true
    feedback: &gt;
      
  - answer: &gt;
      Which drug is the best for the old people?
    feedback: &gt;
      
  - answer: &gt;
      Do the drugs differ significantly from each other for the
      young people?
    feedback: &gt;
      
- typeName: multipleChoice
  prompt: &gt;
      We have an agriculture experiment with the following
  inputs: crop variety (3 values), fertilisers (2 values) and
  irrigation type (2 values). We also have a continuous input:
  area of the plot. Then which type of anlysis will be needed?
  defaultFeedback: &gt;
      
  shuffleOptions: true
  options: 3-factor ANCOVA model
  - answer:
    isCorrect: true
    feedback: &gt;
      
  - answer: 3-factor ANOVA model
    feedback: &gt;
      
  - answer: 2-factor ANOVA model
    feedback: &gt;
      
  - answer: regression model.
    feedback: &gt;
      

</pre>
</fieldset>

<h2><a
name="Module 4: Lesson 5: Review">Module 4: Lesson 5: Review</a></h2>

<b>Total lesson duration
= 0</b>
<p></p>

<div class="header">
<h3><a
name="Module 4, Lesson 5 , Video 1: Review">Module 4, Lesson 5 , Video 1: Review</a></h3>
</div>

<div class="header">
<h3><a
name="Module 4, Lesson 5 , Video 2: Review">Module 4, Lesson 5 , Video 2: Review</a></h3>
</div>



<h1><a
name="Module 5: Regression">Module 5: Regression</a></h1>


<h2><a
name="Module 5: Lesson 1: Regression concept">Module 5: Lesson 1: Regression concept</a></h2>

<b>Total lesson duration
= 25.5</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 1, Video 1: The concept">Module 5, Lesson 1, Video 1: The concept</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.30</i></legend>
We had mentioned scatterplots in the Basic Statistics course. They
are an important visual tool to assess the relation between two
continuous variables.
---
 For instance, consider a toy data set
for which the scatterplot looks like this<b><font color="red">
[[shown]] </font></b>. Here you can
see a linear pattern. I said a linear pattern, not a
line,...
---
 because not all the points are lying exactly on a line. But
still we can feel the presence of a line. Our aim is to capture
that line mathematically. Let's first try by eye estimation. 
---
Let's consider<b><font color="red">
[[shown]] </font></b> this line. Is this a good fit? Since not
all the points are exactly on a 
line, whatever line we draw is bound to miss some points.
---
 But this line completely ignores the shape of the point cloud. We need something
right through this cloud, like this<b><font color="red">
[[shown]] </font></b>. This is much better. Now
what about this<b><font color="red">
[[shown]] </font></b>?
---
 Well, this looks fine too. If I ask you to choose
one of these two good fits, which one would you choose? Hard to
say by visual inspection alone.
---
 So you see the two problems that
lie in the way of mathematically capturing the best line:

* First, how to do mathematically what the eye does easily:<b><font color="red">
[[finger]] </font></b>
avoiding such lines that are wide off the mark.
---
* Second, how to do what the eye cannot do: choosing objectively the
best among such good contenders.
---
Before we go into further details, let me tell you that the
problem is not specific to linear patterns alone. Consider this
scatterplot that shows a curved pattern like a
parabola<b><font color="red">
[[shown]] </font></b>.
---
 Surely this parabola is a bad fit<b><font color="red">
[[shown]] </font></b>, as is
this one<b><font color="red">
[[shown]] </font></b>. We 
would like to have a good fit like this<b><font color="red">
[[shown]] </font></b>. 
---
And somehow we also want
an objective satisfaction of having chosen the best fit.

This is the problem of <b>regression</b>. It has two aspects:
---
* Choosing the right form (like a straight line or parabola or
something else)

* and then picking the best line or curve of that form.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Parametric and nonparametric</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.10</i></legend>
Most of the time we choose the form either by <b>visual inspection</b>
or by <b>domain knowledge</b>, eg, we know that <b>projectiles</b> near the
ground move along parabolic paths....
---
 Or <b>Boyle's law</b> from physics
dictates the use of rectangular hyperbola for the relation
between pressure and volume of a gas under fixed temperature.
---
Any such mathematical form is
characterised by some <b>parameters</b> eg, a straight line by the
form <b><font color="red">
[[move]] </font></b><b>$y = a + bx,$</b> where $a$ and
 $b$ are the parameters.
---
 A parabola has the form <b>$y = a + bx + cx^2,$</b>
where $a,b,c$ are the parameters. 
---
Choosing some form is the first step.

Once we have finished choosing the form the next step is to
estimate the values of the parameters. 
---
Regression that proceeds in these two steps is called <b>parametric
regression</b>.
---
If, however, we want to automate even the first step, ie, want to
have some automatic way of choosing the form, then we need what
is called <b>nonparametric
regression</b>. This course will only discuss parametric regression.*
</fieldset>
 R code for above session
<font color="red">
<pre>
p(5,1,1,1)
set.seed(3535)
x = runif(30,-1,1)
y1 = x + rnorm(30)/5
y2 = x*x + rnorm(30)/5

f = function(i,p) {
   y = (1-p)*y1 + p*y2
   bareplot(x,y,pch=20,cex=3)
   abline(h=0,v=0,lwd=3) 
}
process('lin2non',f,30,ps=0,pe=1,bg=rgb(1,1,1,0.8))
</pre>
</font>


</div>


<fieldset>
<legend>m5l1_a.yml</legend>
<pre>

- typeName: textReflect
  prompt: &gt;
     Consider this data set: \frac ./exrauxb/reg1.csv. Make a
    scatterplot, and visually judge id the pattern s linear or
  nonlinear. Also is it increasing or decreasing?
 
  defaultFeedback: &gt;
    The scatterplot looks like this: \frac ./exrauxb/reg1sol.png 
    The pattern is nonlinear and decreasing.     
</pre>
</fieldset>
 R code for the above exercises
<font color="red">
<pre>
cw('exrauxb')
set.seed(32353)
x = trim(runif(20,1,10))
y = trim(1-x*x + rnorm(20))
csv(data.frame(x,y),'reg1.csv')
png('reg1sol.png')
plot(x,y,cex=2,pch=20)
dev.off()
</pre>
</font>


<div class="header">
<h3><a
name="Module 5, Lesson 1, Video 2: Mathematical formulation">Module 5, Lesson 1, Video 2: Mathematical formulation</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 5.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.20</i></legend>
In this video and the next video we shall introduce the mathematical model
behind linear regression. It is best understood through an example.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">An example</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
[Use shiny]
Here is a scatterplot. We discern a linear pattern. So we try to
fit a straight line to the data cloud. That means looking for a
line like $y=a+bx.$ Here $a$ and $b$ are the two
parameters. The first parameter is called the intercept, the
second the slope. We are looking for optimal values for them. To
appreciate the problem imagine that there are two sliders one
for $a$ the other for $b.$ If I move the $a$
slider then the line shifts up and down remaining parallel to
itself. If I move the $b$ slider then the line rotates
around the point where it hits vertical axis. Our aim is to
find $a$ and $b$ such that the line passes as close as
possible to all the data points. 
</fieldset>


</div>


<fieldset>
<legend>m5l1_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     How would you change the values of the a and b  parameter as
  given in the video to change the blue line to red one in \frac ./exrauxb/reg2.png
  defaultFeedback: &gt;
     Just increase the slpe b. Leave a unchanged (because both
  the lines intersect the vertical axis at the same point). 

</pre>
</fieldset>
 R code for the above exercise
<font color="red">
<pre>
cw('exrauxb')
png('reg2.png')
bareplot(0,xlim=c(-1,1),ylim=c(-1,1),ty='n')
abline(h=0,v=0,lwd=2)
abline(a=0.2, b=0.8,lwd=3,col='blue')
abline(a=0.2, b=1.3,lwd=3,col='red')
dev.off()
</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Computing with LibreOffice (plot only) for the same data set.
</fieldset>
 R code for the above screencast
<font color="red">
<pre>
p(5,1,3)
set.seed(334646) #seed from $t/lsq2/server.r
x = rnorm(50)
y = 1-x+rnorm(50,sd=0.5)
x = trim(x)
y = trim(y)
csv(data.frame(x,y),'regsimp.csv')
</pre>
</font>


</div>


<fieldset>
<legend>m5l1_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Repeat the procecure shown in the video with this data set \frac ./exrauxb/linest2.csv
  defaultFeedback: &gt;
     Part of the output: \frac ./exrauxb/linest2pic.png   

</pre>
</fieldset>


<font color="red">
<pre>
cw('exrauxb')
set.seed(46346346)
x = 1:10
y = trim(2+3*x + rnorm(10))
plot(x,y,cex=2,pch=20)
csv(data.frame(x,y),'linest2.csv')
bhul
lm(y~x)
</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Fit line using linest and regression menu
</fieldset>

</div>


<fieldset>
<legend>m5l1_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Repeat the procecure shown in the video with the same data
  set \frac ./exrauxb/linest2.csv as used in the last exercise.
  defaultFeedback: &gt;
     Part of the output: \frac ./exrauxb/linest2sol.png   
     If you are getting some error, then you have possibly
  selected entire columns (and not just the data part). Also make
  sure that you have either not selected the labels, or have
  selected this checkbox: \frac ./exrauxb/linest2hint.png
  

</pre>
</fieldset>


<font color="red">
<pre>
cw('exrauxb')
set.seed(46346346)
x = 1:10
y = trim(2+3*x + rnorm(10))
plot(x,y,cex=2,pch=20)
csv(data.frame(x,y),'linest2.csv')
bhul
lm(y~x)
</pre>
</font>








<div class="scrpt">
<b>Total video duration
= 4.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.70</i></legend>
What I described in the last few videos, fitting a line or curve
to a bunch of points, is more like looking at data.
---
 But as I have
been saying again and again, statistics is all about looking through
data at the underlying truth.
---
 So let's be clear about the underlying truth here. It will help to
visualise the set up as  measuring 
the length of a spring<b><font color="red">
[[shown]] </font></b> from which we are hanging different
weights.
---
 The weight we hang is <b>$x$</b> and the measured length
is <b>$y.$</b> Every time we choose... a weight to hang,
ie we choose
a value for $x,$ nature decides about the length in two
steps.*
</fieldset> 

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
ie we choose
a value for $x,$ nature decides about the length in two
steps. First, she uses two fixed numbers $\alpha $
and $\beta$ (depending on the spring used), and
computes $\alpha + \beta x.$ Then in the second step she
adds a little random error $\epsilon$ to it to get $y =
\alpha + \beta x + \epsilon,$ which she discloses to the
statistician. This is what happens in each measurement,
the $\alpha $ and $\beta $ remain the same, but the
random error keeps on changing. So we get this  model:
$$
y_i = \alpha + \beta x_i + \epsilon_i
$$
for $i=1,...,n.$ There is also some assumption on the random
errors. They are assumed to be independent with normal
distribution having mean 0 and some unknown variance $\sigma^2.$ 

This is called the regression model.

Now the problem is to estimate $\alpha$ and $\beta.$ A secondary problem is
to estimate $\sigma^2,$ which measures the accuracy of the
line. 
</fieldset>

</div>


<fieldset>
<legend>m5l1_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     Suppose that the errors have some unknown (common) mean
  instead of zero. Does that really change the regression model? 
  defaultFeedback: &gt;
      No, the unknown mean of the errors may be "absorbed" into
  the alpha term. Thus if the unnown common mean of the epsilons
  is gamma, then we define 
      new_epsilon = epsilon - gamma, 
      new_alpha = alpha + gamma.
  Then our model becomes
      y = new_alpha + beta * x + new_epsilon,
  which is of the type discussed in the video, because the mean
  of new_epsilon is 0.
</pre>
</fieldset>




<h2><a
name="Module 5: Lesson 2: Least squares">Module 5: Lesson 2: Least squares</a></h2>

<b>Total lesson duration
= 28.5</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 2, Video 1: Least squares">Module 5, Lesson 2, Video 1: Least squares</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.50</i></legend>
Here we shall describe the most popular technique for
regression. We shall explain the idea with fitting a straight
line, but the idea is equally valid for other types of lines, like
parabolas. 
---
We have already considered the fitting of a
line <b>$y = a + bx$</b> by imagining that we have sliders
for $a$ and $b.$*
</fieldset>

<fieldset><legend
    class="cu">
[r] <i>Duration=4</i></legend>
Show the vertical error lines. Mention about squaring.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Mathematically</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.60</i></legend>
For any given values of $a$ and $b$ we have the total
squared error 
<b>$$
S(a,b) = \sum (y_i - (a+b x_i))^2.
$$</b>
---
We want to minimise this w.r.t. $a$ and $b.$ A little
mathematics shows that the minimum occurs when $b$ is
<b>$$
\hat b = \frac{\sum (x_i-\overline x)(y_i-\overline y)}{\sum (x_i-\overline x)^2}
$$</b>
and $a$ is...
---
<b>$$
\hat a = \overline y - b \overline x.
$$</b>
---
This $\hat b $ is called the <b>estimated regression
coefficient.</b> and $\hat a$ is called
the <b>estimated intercept.</b>
---
An estimator for $\sigma^2 $
is <b><font color="red">
[[move]] </font></b>
<b>$\hat\sigma^2 = \frac{1}{n-2}\sum(y_i-\hat \alpha - \hat \beta x_i)^2.$ </b>
---
Here we recognise the sum as total squared error for the best
line. Why $n-2$ is the denominator? Well, it is hard to
explain without getting into math.
---
 A quick explanaion is that we
have $n$ cases, and based on them we have estimated two
coefficients, a and b. Hence n-2.*
</fieldset>

</div>


<fieldset>
<legend>m5l2_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If all the x values are multiplied by 2, and all the y
  values are multiplied by 3, then how would the estimated
  intercept and slope change?
  defaultFeedback: &gt;
      Estimated intercept will get multiplied by 3. Estimated
  slope will get multiplied by 3/2.

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 5.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.30</i></legend>
We have learned one way to estimate intercept
and slope in a regression set up. The estimators have some interesting properties
that we want to discuss here.
*
</fieldset>


<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
 First,
 the regression coefficient
is of the form $\frac{\cov(x,y)}{\var(x)}.$ And second the
estimated line always passes through the centre of the data cloud $(\bar x, bar y).$

As we have already mentioned in an earlier lesson, statisticians
always worry about the sampling distributions of the
estimators, in particular their bias and standard error. It may
be shown that both the estimators are unbiased.  Also their
standard errors are given by ... [Show that if x's are close
together then variance increases]

Don't be scared by the formulae. When you compute them for a
given data set, they are just two numbers. If the numbers are
large, then that is a cause for worry, because large standard
errors mean inaccurate estimators. 
</fieldset>

</div>

<fieldset>
<legend>m5l2_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If all the x values are doubled, then how would the
  estimated intercept and slope change? How will their standard
  errors change?
  defaultFeedback: &gt;
      Estimated intercept will not change, estimated slope will
  be halved. The standard error of the estimated intercept will
  not change. The other standard error will be halved.

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>
Hand computation of regression line.
<font color="red">
<pre>
set.seed(135319)
(x = 1:5)
y = as.numeric(format(2+x+rnorm(5),dig=2))
cbind(x,y)
(sx = sum(x)); (sy = sum(y))
(sx2 = sum(x*x)); (sy2 = sum(y*y))
(sxy = sum(x*y))
(mnx = mean(x)); (mny = mean(y))
(vx = var(x)); (vy = var(y)); (cxy = cov(x,y))
(b = lm(y~x)$coef)
as.numeric(format(lm(y~x)$fit))
</pre>
</font>

</fieldset>

</div>


<fieldset>
<legend>m5l2_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: |-
     Compute the least squares estimates of intercept, slope and error
     variance based on this toy data: 
           | x | y |
           |---+---|
           | 1 | 9 |
           | 2 | 7 |
           | 3 | 5 |
           | 4 | 3 | 
     Look carefully at the data. You should be able to guess the
  values even without any lengthy computation. But do check using
  the lengthy formulae given in the video.  
  defaultFeedback: &gt;
      estimated intercept is 11, estimated slope is -2. The fit
  is exact, so estimated error variance is 0.
</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 5.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.30</i></legend>
Notice that there is some assymmetry in the set up that we have
been working with so far: predicting y
based on x.
---
 It could be the other way round: predicting x based
on y. We shall discuss
that now.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">x on y</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
[x-on-y, y-on-x, beta*beta = r^2, sign]
</fieldset>

</div>

<fieldset>
<legend>m5l2_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If the estimated regression equation for y on x is 
          y = 1 + x/2,
     and the regression equation for x on y is
          x = -2 + 2*y,
     then find the correlation between x and y.
  defaultFeedback: &gt;
     The two regression coefficients are 1/2 ans 2. Their produce
  is 1. So the square of the correlation is 1. Also, corrleation
  has the same sign as the coefficients. So correlation must be 1. 

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 5.6</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.5</i></legend>
In the last video we switched the roles of x and y. But still
that did not achieve symmetry. It just went to the other extreme
of the assymmetry. There is however one version of regression that
is truely symmetric. It is called orthogonal regression. *
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Orthogonal regression</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Orthogonal regression.
</fieldset>

</div>


<fieldset>
<legend>m5l2_e.yml</legend>
<pre>
- typeName: Discussion
  prompt: &gt;
      If we multiply all the x values by 2 in an orthogonal
  regression set up, how do you think the estimated intercept and
  slope would change?
</pre>
</fieldset>



<h2><a
name="Module 5: Lesson 3: Real life data">Module 5: Lesson 3: Real life data</a></h2>

<b>Total lesson duration
= 27.1</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 3, Video 1: Real life data">Module 5, Lesson 3, Video 1: Real life data</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 5.1</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=0</i></legend>
In this lesson we shall take a look at some real life scenarios
that call for regression analysis. The scenarios are taken from
diverse fields to provide an idea of the range of
applications.
---
 In each case, the first step is to make a
scatterplot. 
---
Our first example is also historically the very first example of
regression analysis in statistics. In fact the very term 
<b>regression</b> was coined by <b>Sir Francis Galton </b> while
analysing this data set.
---
 At that time, however, it meant something quite
different from its present meaning.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Galton's height data</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
galton.ods: 1) Describe variables
            2) Make plot
            3) Insert 45 degree line
            4) Explain regression
            5) Discussion prompt: why? 
</fieldset>


<font color="red">
<pre>
p(5,3,1)
library(HistData)
csv(Galton,'galton.csv')
?Galton
</pre>
</font>


</div>

<fieldset>
<legend>m5l3_a.yml</legend>
<pre>
- typeName: discussion
  prompt: &gt;
      Galton's notion of regression stemmed from the observation
  that the fitted least squares line had slope less than 1. Is it
  true that the slope of any least squares line must be less than
  1? If so, why? If not, then why is it natural to have this for
  this data set? 
  defaultFeedback: &gt;
      

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Demo 2
boyle.ods
</fieldset>

</div>

<fieldset>
<legend>m5l3_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the same data set once again:
  \frac ./exrauxb/boyle.csv. But this time proceed
  differently. Define y = 1/height, and then plot 
  and regress y on pressure.
  defaultFeedback: &gt;
      \frac ./exrauxb/boylesol.png

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
bodyfat.ods: multiple regression
</fieldset>

</div>

<fieldset>
<legend>m5l3_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     The video ended with a model where some of the variables
  were redundant (ie, had p-values &gt;= 0.05). Refit the model
  to the same data \frac ./exrauxb/bodyfat.csv after removing those variables. What is the new R^2 value?

  defaultFeedback: &gt;
      LibreOffice does not allow you to perform multiple
      regression where the independent variables are not in
      contiguous columns. So we need to copy the relevant columns
      to a different sheet. Here is a relevant screenshot: 
          \frac ./exrauxb/bfatsol1.png
      The estimated coefficient are here:
          \frac ./exrauxb/bfatsol2.png
      The new R^2 value is 0.724. 

</pre>
</fieldset>


<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Nonparametric
</fieldset>

</div>

<fieldset>
<legend>m5l3_d.yml</legend>
<pre>
- typeName: discussion
  prompt: &gt;
      Suppose that we have a bivariate data set where the
  scatterplot  clearly shows a linear pattern. But a statistician
  does not use that pattern. Instead, she uses nonparametric
  regression as shown in the video. Discuss what advantages and
  disadvantages this approach might have over the parametric
  approach. 
  defaultFeedback: &gt;
      

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Demo 5: nonparametric.
[nonpar.ods]
</fieldset>
 R code for above screencast
<font color="red">
<pre>
p(5,3,5)
set.seed(3523)
x = trim(runif(100))
y1 = x#seq(0,1,len=100)
y2 = 3*sin(5*y1)
y = ifelse(y1&gt;y2,y1,y2)
y = trim(y + rnorm(100)/5)
plot(x,y)
csv(data.frame(x,y),'nonpar.csv')
</pre>
</font>


</div>


<fieldset>
<legend>m5l3_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
    Here is a bivariate data set \frac ./exrauxb/nonpar2.csv. Make
  a scatterplot, and perform nonparametric regression following
  the method shown in the video.      
  defaultFeedback: &gt;
   Scatterplot:
    \frac ./exrauxb/nonpar2sol1.png
   The fitted values:
    \frac ./exrauxb/nonpar2sol2.png
   Overlaid on the scatterplot:
    \frac ./exrauxb/nonpar2sol3.png
    
</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(35232)
x = trim(runif(100))
y1 = 2*x
y2 = 3*cos(5*y1)
y = ifelse(y1&gt;y2,y1,y2)
y = trim(y + rnorm(100)/5)
plot(x,y)
csv(data.frame(x,y),'nonpar2.csv')
</pre>
</font>





<h2><a
name="Module 5: Lesson 4: Residuals, outliers, leverage">Module 5: Lesson 4: Residuals, outliers, leverage</a></h2>

<b>Total lesson duration
= 23.4</b>
<p></p>

<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 1: Residuals (theory) ">Module 5, Lesson 4, Video 1: Residuals (theory) </a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 2</b>
<p></p>

<fieldset>
<legend>
(s1)[r] <i>Duration=2.00</i></legend>
We have mentioned that statisticians look through data rather
than look at data. They look through data at an underlying truth.
---
 Unless some part of this truth
is assumed to be known it becomes impossible to make any
progress. But how can part of the truth be already known
beforehand?
---
 Sometimes from past experience or expert opinion. But
more often it is just an untested  gut feeling of the
statistician that is accepted at face value.
---
 Clearly any method built on such
unverified premise has a possibility of going wrong. So a
statistician must carefully assess the success of a proposed
method to detect any mistake in the original assumptions.
---
 Indeed, this is the most important point where a professional
statistcian differs from an amateur one. It is not diffcult to become an
amateur statitician by learning a few 
standard statistical softwares and applying their packaged routines to a data
set.
---
 But that often leads to erroneous results, unless the
assumptions underlying the methods are carefully checked, and
appropriate measures taken in case the assumptions fail.
---
 It is like driving a car. Pressing on the gas pedal and turning the
steering wheel are easy when everything goes according to your
plan. The difficulty lies in controlling your vehicle on the road
where not everything goes according to your plan.
---
This idea of cross checking the basic assumptions after applying
a statistical method is a general principle applicable everywhere
in statistics.
---
 But its need is seldom felt more strongly than in
regression, because there the original assumptions often fail,
and a plethora of tools are available to rectify a failed
assumption.
---
 All these together are called <b>Regression
Diagnostics</b>. That's what we are going to learn in the coming videos.
</fieldset>

</div>



<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 2: Regression diagonsotics">Module 5, Lesson 4, Video 2: Regression diagonsotics</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="bu">
[r] <i>Duration=6</i></legend>

Remember we had the assumption $y_i = \alpha + \beta x_i +
\epsilon_i,$ where the $\epsilon_i$'s were supposed to be
iid with normal distribution having mean 0. As we cannot observe
the $\epsilon_i$'s, how can we hope to check this
assumption? Here is a way. After estimating $\alpha$
and $\beta $ as $\hat \alpha $ and $\hat \beta $
we can compute $\hat \epsilon_i = y_i-\hat \alpha - \hat \beta
x_i.$ These are not same as $\epsilon_i$'s, just
as $\hat \alpha $ is not the same as $\alpha.$ These
are called the residuals. and give an idea about the
actual $\epsilon_i$'s which are unobserved. Plotting these
residuals against the $x_i$'s produces a residual plot. This
plot should ideally show no pattern. Like this. But if it does, then that is
to be construed as a shortcoming of the originally assumed
model. 

For example if the residual plot looks like this, where there a
slightly curved pattern, then possibly we should have included a
curvature in the model to start with. 

The next video will show such an example.
</fieldset>

</div>


<fieldset>
<legend>m5l4_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Suppose that we have carried out a regression  analysis and
  the histogram of the residuals look like
  \frac ./exrauxb/residhist.png
  Name at least one model assumption that appears to have been violated.
  defaultFeedback: &gt;
      Errors (and as their approximations, residuals) are
  supposed to be normally distributed around 0. But here the
  distribution is not even symmetric!

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(46346)
resid = exp(rnorm(200))
resid = resid-mean(resid)
png('residhist.png')
hist(resid,prob=T,col='red')
dev.off()
</pre>
</font>



<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 3: Residuals (lab) ">Module 5, Lesson 4, Video 3: Residuals (lab) </a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
curve.ods: 1) Plot already shown (very linear)
           2) Regress with resid
           3) Plot resid
           4) Point out curvature and why it was not apparent in
              the original plot.
           5) Update fit (already shown in next sheet)
</fieldset>
 R code for above screencast 
<font color="red">
<pre>
p(5,4,3)
set.seed(3433)
x = rnorm(100,mean=100)
y = x + 0.5*x*x+rnorm(100)/2
plot(x,y)
fit = lm(y~x)
plot(x,fit$res)
csv(data.frame(x,y),'curve.csv')
</pre>
</font>


</div>

<fieldset>
<legend>m5l4_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the data \frac ./exrauxb/diag.csv.
     Fit a straight line and make the residual plot. Is it a good
  fit? Update the model, fit again and **make the residual plot again**.
  Is it a good fit now? Can you improve the fit?
  defaultFeedback: &gt;
      You'll need a cubic term.

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(34331)
x = rnorm(100,mean=100)
y = x + 0.5*x*x*x+rnorm(100)/2
plot(x,y)
fit = lm(y~x+I(x*x))
plot(x,fit$res)
csv(data.frame(x,y),'diag.csv')
</pre>
</font>



<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 4: Outiers, leverage (theory) ">Module 5, Lesson 4, Video 4: Outiers, leverage (theory) </a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 3.4</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.70</i></legend>
We have talked about <b>outliers</b> in our Basic Statistics course
already. Roughly speaking these are points that lie far away from
the main bulk of the data.
---
 For instance<b><font color="red">
[[shown]] </font></b> these two points in this
scatterplot are outliers<b><font color="red">
[[finger]] </font></b>, while these points are
not.
---
 There are
two main types of outliers. Those with <b>leverage</b> and those without
leverage. In this and the next video we shall learn about the
concept of leverage.
---
Before going further let me repeat once again: outliers, whether
with or without leverage, must be studied 
carefully. They are born out of unexpected reasons, which may be
just a typo, or a glimpse of an unknown behaviour.
---
 Whether an outlier has leverage or not is determined by how much effect it
has to turn the fitted model towards itself. Consider this
scatterplot<b><font color="red">
[[shown]] </font></b>. The least squares line is this<b><font color="red">
[[finger]] </font></b>. 
---
Now consider an
outlier<b><font color="red">
[[shown]] </font></b> here<b><font color="red">
[[finger]] </font></b>. Here the least squares
line<b><font color="red">
[[finger]] </font></b> moves only slightly 
away due to the presence of the outlier.
---
 We say that this outlier
has <b>low leverage</b>. Such outliers are less harmful. 
---
Next consider an outlier here<b><font color="red">
[[shown]] </font></b>. Here the least squares line swings
a lot towards that point. Somehow this point exercises so much
pull on the line that this single point can force the line to
move away from bulk of the points.
---
 We say that this point has <b>high leverage</b>. 
Clearly such outliers are more dangerous. It is
important to remove them if we want to fit a line showing the
true pattern of the points.*
</fieldset>
 R code for this session
<font color="red">
<pre>
p(5,4,4,1)
set.seed(23346)
x = rnorm(20)
y = x + rnorm(20)/5
outx1 = 0 
outy1 = 2
outx2 = 3
outy2 = 5
xlim = range(x,outx1,outx2)
ylim = range(y,outy1,outy2)
trans = rgb(1,1,1,0.5)
png('scat%d.png',bg=trans)
bareplot(x,y,xlim=xlim,ylim=ylim,cex=2,pch=20)
abline(h=0,v=0,lwd=2)
points(outx1,outy1,col='red',cex=2,pch=20)
points(outx2,outy2,col='blue',cex=2,pch=20)

bareplot(x,y,xlim=xlim,ylim=ylim,cex=2,pch=20)
abline(h=0,v=0,lwd=2)
coef = lm(y~x)$coef
abline(coef,lwd=3)

x1 = c(x,outx1); y1 = c(y,outy1)
x2 = c(x,outx2); y2 = c(y,outy2)

coef1 = lm(y1~x1)$coef
coef2 = lm(y2~x2)$coef

bareplot(x,y,xlim=xlim,ylim=ylim,cex=2,pch=20)
abline(h=0,v=0,lwd=2)
points(outx1,outy1,col='red',cex=2,pch=20)

abline(coef,lwd=3)
abline(coef1,lwd=3,col='red')

bareplot(x,y,xlim=xlim,ylim=ylim,cex=2,pch=20)
abline(h=0,v=0,lwd=2)
points(outx2,outy2,col='blue',cex=2,pch=20)

abline(coef,lwd=3)
abline(coef2,lwd=3,col='blue')

dev.off()
</pre>
</font>


<center>
<img src="image/jingle.png"><font color="blue" size="+3">Why leverage</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.60</i></legend>
As may be guessed from the two examples<b><font color="red">
[[shown]] </font></b>, leverage of a point
depends on how far it is from the centre of the data along
the $x$-direction.
---
 This point<b><font color="red">
[[finger]] </font></b>, though away from the bulk of
the points, is close to the centre along
the $x$-direction. However, this point is far away from the
bulk of the points along the $x$-direction.
---
In fact, leverage is a property of the all the points, whether an
outlier or not. Every point has some leverage. Points near the
extreme along the $x$-direction have greater leverage.
---
 In general, points with high leverage are not bad. In fact, if you
have a point<b><font color="red">
[[shown]] </font></b> here, it is good for the least
squares fit, because 
it strengthens our faith in the linear pattern...
---
 when we see that even this
far away point still follows the same pattern. However, outliers
with high leverages must be removed before we accept the fitted
line. 
---
As outliers with high leverage tend to swing the fitted lines
towards themselves, these points may not always have high residuals. So
a residual plot need not show these points clearly.
---
 They are like
corrupt persons in power, who bend the law so much in their
favour that legal machineries fail to detect them. 
---
There are, however, quite a few sophisticated techniques to catch them, I mean the
outliers, not the corrupt persons. 
We shall not go into these techniques. For our course simple visual
inspection of the scatterplot is enough.*
</fieldset>
 R code for this session
<font color="red">
<pre>
p(5,4,4,2)
set.seed(233465)
x = rnorm(20)
y = x + rnorm(20)/5
outx1 = 0 
outy1 = 2
outx2 = 3
outy2 = 5
outx3 = 3
outy3 = 3

xlim = range(x,outx1,outx2,outx3)
ylim = range(y,outy1,outy2,outy3)
trans = rgb(1,1,1,0.5)
png('scat%d.png',bg=trans)
bareplot(x,y,xlim=xlim,ylim=ylim,cex=2,pch=20)
abline(h=0,v=0,lwd=2)
points(outx1,outy1,col='red',cex=2,pch=20)
points(outx2,outy2,col='blue',cex=2,pch=20)

bareplot(x,y,xlim=xlim,ylim=ylim,cex=2,pch=20)
abline(h=0,v=0,lwd=2)
points(outx3,outy3,col='purple',cex=2,pch=20)
dev.off()
</pre>
</font>


</div>

<fieldset>
<legend>m5l4_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     We have a bivariate data set with 100 cases and two variables x and
  y. Here are some summary values: 
     |      |    x |     y |
     |------+------+-------|
     | mean | 34.8 | 100.2 |
     | min  | 15.2 |  20.8 |
     | max  | 53.6 | 193.2 |
   Two new points are added to this data set: (x_101,y_101)=(54,100)
  and (x_102,y_102)=(15,193). Which of these has the higher
  leverage? 
  defaultFeedback: &gt;
      The first one, because it is near an extremity of the x-values.

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="header">
<h3><a
name="Module 5, Lesson 4, Video 5: Outiers, leverage (lab) ">Module 5, Lesson 4, Video 5: Outiers, leverage (lab) </a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
crowdedness.ods: 1) Source
                 2) Explain crowdedness, fertility 
                 3) Plot fertility vs crowdedness
                 4) Try to regress by selecting whole columns: fail
                 5) Try to regress by selecting labels: fail
                 6) Copy to different sheet, remove incomplete cases
                 7) Regress
                 8) Plot residuals, show outliers.
</fieldset>

</div>

<fieldset>
<legend>m5l4_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Fit a straight line to this data set:
     \frac ./exrauxb/out.csv. 
     Find an outlier. Remove it and refit. 
  defaultFeedback: &gt;
     The last case is an outlier. 

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(3534)
x = trim(runif(50,10,20))
y = trim(1+x+rnorm(50)/3)
badx = 25 ; bady = 30
x = c(x,badx); y = c(y,bady)
plot(x,y)
fit=lm(y~x)
abline(fit$coef)
plot(x,fit$resid)
csv(data.frame(x,y),'out.csv')
</pre>
</font>




<fieldset>
<legend>m5_add.yml</legend>
<pre>
- typeName: peerReview
  prompt: &gt;
    Consider the bivariate data set stored in
  \frac ./exrauxb/m5add1.csv. Regress y on x using some 
  appropriate form of line or curve. Comment on the adequacy of
  the fit. Report relevant plots.
  
  Rubric:
  ------
      1) 1 point for reporting this:
            \frac ./exrauxb/m5add1hint1.png
      2) 1 point for proceeding with residual plot:
            \frac ./exrauxb/m5add1hint2.png
         and understanding that there is scope for improvement.

- typeName: peerReview
  prompt: &gt;
      A forgetful statistician has regressed y on x as well as x
  on y from the same data set and has obtained these two lines:
     y = 4+5x and y = 1 + 3x.
  Unfortunately, he has forgotten which is which, and has also
  lost track of the data. How can he
  figure out which is which by just looking at these two
  equations? 

  Rubric:
  -------
    1) Solution: 
      Let's try the first possibility: 
      y on x : y = 4 + 5x
      x on y : y = 1 + 3x, or x = -1/3 + y/3.

      The product of the slopes are 5/3&gt; 1, which is impossible
      since the product is square of the correlation and hence
      must be in [0,1].

    Let's try the second possibility: 
      y on x : y = 1 + 3x
      x on y : y = 4 + 5x, or x = -4/5 + y/5.

      The product of the slopes are 3/5, which is in [0,1].
    
    So the second possibility is the correct one.
   
    2) 1 point for understanding that one of the equations must
    be turned around before computing the slopes.   
     
- typeName: peerReview
  prompt: &gt;
     The x-on-y and y-on-x least squares regression lines
  obtained from the same data set are, respectively,
    x = 2-3y and y = 5-0.1*x
  What are the means of x and y?
  
  Rubric:
  -------
    1) 1 point for understanding the two lines intersect at
  (xbar, ybar).
    2) 1 point for finding: xbar = -130/7, and ybar = 48/7.
      
- typeName: peerReview
  prompt: &gt;
    Consider the data set in \frac ./exrauxb/outliers.xlsx. Detect
  the outliers. Then remove the one with higher leverage and fit
  a straight line.  
   
    Rubric:
    ------
     1) The outliers are cases 13 and 40. Should show scatterplot. (1 point)
     2) Case 40 has higher leverage. After removing the fitted
    line is like this:  
      \frac ./exrauxb/outliersol.png

- typeName: peerReview
  prompt: &gt;
    Consider the body fat data set  \frac ./exrauxb/bodyfat.csv
  that we have already used in one of our videos. Perform
  multiple regression of the bodyfat variable on hip, knee,
  biceps and forearm. Only one of these variables seems to be
  important based on the output. Which one? How did you decide?
  
  Rubric:
  -------
    1) The variables are not in contiguous columns. So they need
  to be copied to a different sheet. 
    2) The relevant portion of the multiple regression is 
        \frac ./exrauxb/bodyfatsol1.png (1 point)
    3) Only the hip variable seems to be important, because its
  p-value (yellow cell) is less than 0.05. (1 point)
      

</pre>
</fieldset>
 R code for the above
cw('exrauxb')
set.seed(343364)
x = rnorm(100,mean=100)
y = -x + 0.5*x*x+rnorm(100)/2
plot(x,y)
fit = lm(y~x)
plot(x,fit$res)
csv(data.frame(x,y),'m5add1.csv')

cw('exrauxb')
set.seed(34336)
x = runif(50,10,20)
y = x + rnorm(50)/3
x[13] = 15; y[13] = 12
x[40] = 22; y[40] = 18
plot(x,y)
csv(data.frame(x,y),'outliers.csv')
<font color="red">
<pre>

</pre>
</font>



<h1><a
name="Module 6: Time series analysis">Module 6: Time series analysis</a></h1>


<h2><a
name="Module 6: Lesson 1: Time series concept">Module 6: Lesson 1: Time series concept</a></h2>

<b>Total lesson duration
= 21.9</b>
<p></p>

<div class="header">
<h3><a
name="Module 6, Lesson 1, Video 1: Concept: What it is">Module 6, Lesson 1, Video 1: Concept: What it is</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 3.3</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.60</i></legend>
To be able to predict the future has always been the dream of
man. From science to science fiction, from astronomy to
astrology,...
---
 from stock market to politics, from mythology to
meteorology we see ample evidence of man's fascinaion with 
predicting the future.
---
 Ancient mythology of every culture is rife with people endowed with the  super human
power of seeing the future. Weather forecast is part of our every
day life.
---
 Politicians analyse popularity trend and try to predict
the outcome of polls, share holders try to predict market trends.
---
Except in pure fiction, all attempts to predict the future,
whether successful or not, depend on the same basic technique:
---
analysing past data, and looking for patterns in how the values have been
evolving over time, and hoping that the same pattern will persist
in the future as well. 
---
And that is what time series analysis is all about, mostly. A time series 
consists of values of one or more  variables
over time.
---
 The aim is to understand its temporal behaviour, how the
quantity evolves over time.  In such a data
set time is always one of the variables, and there is at least
another variable whose values are evolving with time.
---
A time series is  different from the other types of
data we have handled so far. Let's understand this difference carefully.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">How a times series differs from other types of data</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.60</i></legend>
So far in this course, we have been working with the mental
picture of some underlying distribution from which the data are
being generated.  
---
 The idea is something like a coin toss, where the same coin is
being tossed again and again. So the more tosses you have the
more information you have about the same coin.
---
However, the situation is different for a time
series<b><font color="red">
[[shown]] </font></b>. Here the underlying distribution may itself evolve with
time. Thus, a value in a time series need not come from
the same distribution as the preceding one.
---
 This makes time series
analysis much more challenging than the type of data analysis we
have encountered so far.
---
 It is as if I have
many coins laid out in a row. I toss each coin once, and try to
infer about the probabilities of head for all the coins.
---
 This is an impossible task to achieve, because the coins are different, and
I have just a single toss for each coin.
---
 As a result all time series analysis methods start by 
assuming that  the underlying process does not evolve too
much, ie, these distributions are somehow connected through a
small number of unknown parameters.
---
These assumptions are called <b>time series models</b>.
We shall see some of these in this module.*
</fieldset>

</div>

<fieldset>
<legend>m6l1_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     We have a time series model where the observations are all
  independent. Initially the mean is some unknown number m1 for
  some unknown stretch of time, T, and then the mean abruptly
  changes to another unknown value m2. Here is a plot of such a
  time series.
     \frac ./exrauxb/change.png
 Provide rough estimates for m1, m2 and T.
  defaultFeedback: &gt;
     Consider this plot:
     \frac ./exrauxb/changesol.png
     m1 is a bit below 10 (blue level), m2= 20 (green level) 
     and T is slightly before 60 (purple vertical line).
</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(5747)
x = c(10+rnorm(57), 20+rnorm(43))
png('change.png')
plot(x,ty='l',lwd=3,col='red')
dev.off()
png('changesol.png')
plot(x,ty='l',lwd=3,col='red')
m1 = mean(x[1:57])
m2 = mean(x[58:100])
abline(h=c(m1,m2),v=57.5,lty=2,col=c('blue','green','purple'),lwd=3)
dev.off()
</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 5.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.00</i></legend>
Whatever we do takes time. And data collection is no
exception. In that sense should we not call most data sets  time
series data, unless all the cases where recorded in parallel?
---
 For instance, if I am interviewing  100 households regarding some
demographic survey I am doing so sequentally over time. First
this household, then the next, then the one after that, and so
on. Should I call this a time series data?
---
 Not really, because I do not expect
the underlying situation to have evolved much  during that
period.
---
 On the other hand if we are measuring the amount of
suspended particulate matter in the air at some crossing in a
city for a year,...
---
 then it is definitely a time series, because the
amount of dust in the air is likely to behave differently
over the weekdays and weekends.
---
 The main aim behind any statistical analysis is to account for
the variation in the observed
values.
---
 If you suspect that time possibly accounts for a
significant portion of the variation, then and only then you
designate that data as a time series.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Time: continuous, regular, irregular</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.70</i></legend>
Time inherently is continuous. However, when we measure something
over time, we generally do so at discrete time points. Certain
electronic measuring devices like the ECG,...
---
 may do this fast enough to create
the illusion of continuous monitoring, but even they are actually
making measurements at discrete time points.
---
Most time series analysis methods expect these time points to be regularly
spaced in time. This temporal resolution is often mentioned in the
title of the data set, like daily rainfall, or monthly sales or
annual production.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Time: point or interval?</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.60</i></legend>
If we consider time as flowing along a  line<b><font color="red">
[[shown]] </font></b>, the time points
are like regularly spaced points along that line<b><font color="red">
[[shown]] </font></b>. For each point
we have a measured value<b>?<b><font color="red">
[[shown]] </font></b></b>. 
---
However, a value in a time series is often not really 
associated with the time point, but rather with a time
interval<b><font color="red">
[[shown]] </font></b>.
---
 Like total daily rainfall is not the rainfall at a
single instant, it is the total over an entire day, a time
interval. 
---
Usually
each value asociated with a time interval is some kind of a
summary value of multiple actual measurements made during the
interval.
---
 For example it could be the total sales for a month, or
average pollution level over a day. Indeed, associating a value
with an interval rather than a single time point allows the data
collectors to avoid a serious problem, the problem of missing
data.
---
 Since we cannot go back in time, so if we failed to collect
data in time, then there is no way we can get it back later. By
the time we come back, the process has possibly evolved into
something new.
---
 But when we report a single average value for a week, we
can still make up for the missing value by making other
measurements during the same week. *
</fieldset>


</div>

<fieldset>
<legend>m6l1_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     A hospital keeps record of the birth weight and as well as
  the birth time of newborns. Do these data constitute a time
  series? 
  defaultFeedback: &gt;
      Not really, as we do not expect the birth time to have much
  effect on the birth weight. However, if we average the birth
  weights over different months, and want to explore if average
  birth weights vary systematically over the years (eg, babies
  born in winter are lighter, or birth weights show an increasing
  trend over the years), then we shall have a valid time series.

</pre>
</fieldset>



<div class="scrpt">
<b>Total video duration
= 9</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=4.30</i></legend>
Why do we care about time series data? 
The primary aim of time series analysis is, as I have already
mentioned, prediction.
---
 Trying to extrapolate patterns from the
past to guess what is going to happen in future. I shall mention
here three surprisingly different  situations where
time series are analysed with the chief goal of predicting the
future.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Predicting the stock market</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.60</i></legend>
* There is nothing surprising in the  first example. It is
something  we already know about: predicting the
stock market.
---
 ``To be forewarned is to be fore armed'' is the motto
in economics. That is why we try to predict GDP, cost of houses,
or agricultural yield.
---
* A drastically different application of prediction is active
noise cancellation that is used to cancel noise by an
artificially generated anti-noise!*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Active noise cancellation</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=1.80</i></legend>
Imagine a <b>noisy</b> environment. The sound that it produces is a basically
a time series<b><font color="red">
[[shown]] </font></b> of the vibration of the air molecules. Now the
vibration is caused by applying force on the molecules<b><font color="red">
[[shown]] </font></b>.
---
 So if we
can apply just the opposite force on the air molecules at the
right time<b><font color="red">
[[shown]] </font></b>, then the sound can be cancelled.
---
 Such a counterbalancing force may be applied by
generating another noise<b><font color="red">
[[shown]] </font></b>, which is a mirror image of the
noise time series.
---
This mirror image is  called an <b>antinoise</b> But for this trick
to work, the antinoise needs to mimick the
original noise very closely in order to be able to cancel it.
---
 So the game is like this. A sensor picks up the noise,
which is mathematically inverted, and the resulting  anti-noise
is emitted.
---
 All this takes a fraction of a second, but
even within that fraction the force applied by the original sound
has changed (it is a vibration after all, so forces are changing
to and fro all the time).
---
 So the anti-noise we are producing
now should actually mimick the actual noise to be produced a
fraction of a second in the future.
---
 So that's where prediction
comes into play. Predicting just a fraction of a second ahead,
but still that's a prediction nonetheless. 
---
Any error in that prediction will mean the antinoise will not
cancel the true noise, and may actually add to that noise.
---
* Another <b><font color="red">
[[clear]] </font></b>quite different type of prediction is needed in
tracking an air craft.
---
 Indeed, this application has given time
series analysis a frequntly used tool called Kalman
Filtering.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Kalman filtering</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s4)[a] <i>Duration=2.00</i></legend>
 The situation is like this<b><font color="red">
[[shown]] </font></b>. We are tracking a flying
aircraft. Our camera has scoured the entire sky and has finally 
located<b><font color="red">
[[shown]] </font></b> the aircraft as a tiny dot in the vast sky.
---
The camera is now locked on to its target and is
steadily tracking it always keeping the crosshair on the
aircraft.<b><font color="red">
[[shown]] </font></b>
---
 After a while the aircraft enters a cloudy region<b><font color="red">
[[shown]] </font></b>, and the camera
loses its target. Of course, the air craft is sure to
emerge out of the cloud and the camera should wait for that.
---
 But where exactly should it point in order to pick up its lost
target? Starting to look for it all over the sky again is quite time consuming, and
the air craft may very well pass out of sight during the time the
camera is searching the sky.
---
 A little time series analysis helps
here. During the time the camera was locked onto the air craft,
we knew how its position was changing over time, a time
series.<b><font color="red">
[[shown]] </font></b>
---
 So we have an idea of its speed, acceleration, turning etc
just the moment before it disappeared. Well, it won't be very
unreasonable to assume that the aircraft would continue the motion
more or less in the same way even after being hidden by the
cloud.
---
 So we can predict<b><font color="red">
[[shown]] </font></b> where the aircraft will be at a given
time point in the future. Remember we are not talking of a distant
future here, may be just a few seconds or at most a minute
ahead.
---
 So we continue to move the camera according the predicted
path, also keeping an eye on the sky in its vicinity. This
markedly improves the chance of picking up the lost aircraft
again after it emerges. 
---
It might interest you to know that this rather common sense
approach has given birth to a powerful time series analysis
method called <b>Kalman filtering</b>.
---
So we learned about three types of
prediction. The next video will take up some different applications
of time series.*
</fieldset>

</div>

<fieldset>
<legend>m6l1_c.yml</legend>
<pre>
- typeName: discussion
  prompt: &gt;
      Think of at least one more scenario where you use time
  series for prediction. Try to give an example as different as
  possible from the ones mentioned in the video.
  defaultFeedback: &gt;
      

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="header">
<h3><a
name="Module 6, Lesson 1, Video 2: Concept: Diverse applications">Module 6, Lesson 1, Video 2: Concept: Diverse applications</a></h3>
</div>

<div class="scrpt">
<b>Total video duration
= 4.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.50</i></legend>
Prediction is definitely the most important aim behind analysing
a time series. However, that is not always the only 
aim.
---
 Sometimes time series data are analysed for the
purpose of understanding the process generating the time
series.
---
 We shall mention three examples chosen from as diverse
fields as possible to provide an idea about the gamut of
applications.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Understanding market volatility</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.20</i></legend>
Let's do a thought experiment. Imagine that you are driving
down an uneven road. Naturally 
everything inside a car is shaking.
---
 An accelerometer
embedded in the car is keeping track of the bumpiness of the ride
and produces this  time series<b><font color="red">
[[shown]] </font></b>.
---
 You can see  ups
and downs in it. They are more or less regular except here, where
they seem much more rapid. Clearly, the car was shaking a lot
here, possibly over a particularly rough patch in the road.
---
 Now imagine the same plot, but not in the context of a car ride. This
time consider this as a plot of the market price<b><font color="red">
[[shown]] </font></b> of some
stock. Then this rough portion denotes a period of greater
volatility.
---
 These periods are of great importance to market
analysts. They often try to explain the volatility of the market
in terms of what happened before it. Quite a few time series
models like <b>ARCH</b> have resulted from this approach.
---
Now let's look at another application.
*
</fieldset>
 R code for above session
<font color="red">
<pre>
p(6,1,4,2)
png('arch.png',bg=rgb(1,1,1,0.7))
set.seed(3534)
x = 10+c(rnorm(100),rnorm(50,sd=3),rnorm(100))

bareplot(x,ty='l',ylim=range(0,x),lwd=3,col='blue')
abline(h=0,v=1,lwd=3)
dev.off()
</pre>
</font>


<center>
<img src="image/jingle.png"><font color="blue" size="+3">Signal processing</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s3)[a] <i>Duration=0.90</i></legend>
 I am sure you use a mobile phone. You may not be aware that
rather sophisticated time series analysis is needed to keep it
running.
---
 Indeed, so important is this application area, that it
has a special name <b>Signal processing</b>,...
---
 which is often considered a
subject of its own, extending well beyond staistical time series
analysis. Some of the most important tools used in statistical
time series analysis come from this application area.
---
 Roughly speaking, it tries to look for waves or <b>periodic
behaviour</b>s in a time 
series corresponding to  different wavelengths.
---
 Information is encoded in terms of those waves. The same idea is
used even to detect cycles in the economic markets.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Exoplanets</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s4)[a] <i>Duration=1.20</i></legend>
One recent example of time series analysis is the search for
exoplanets,  <b>planets around
stars other than the sun</b>.
---
This is not directly related to this course, but still I mention it because
it is such a unique application.
---
 NASA tries to find places in the universe where life may be
supported. Naturally, the focus of attention is on earth-like
planets around far away stars. 
---
 Now the stars themselves appear to be tiny dots, so a tiny
planet like the earth circling around it is just not visible to even
the most powerful telescope.
---
 Quite surprisingly, time series analysis helps to detect such
planets in many cases. When the planet passes in front of the
star, the apparent brightness of the star diminishes a little.
---
 Astronomers<b><font color="red">
[[blank plot]] </font></b> observe the brighnesses of the stars as a times
series<b><font color="red">
[[up to dip]] </font></b>. It is more or less a constant. Then when
the planet passes in front of it, there is a slight dip in the
brightness level<b><font color="red">
[[dip]] </font></b>.
---
 When the planet moves away, the brightness is restored <b><font color="red">
[[after
dip]] </font></b>. So scientists try to detect if there is any temporary dip
in the time  series.
---
 If so, they can estimate the size of the planet as well as its orbiting
period by the lengths and positions of the dips.
</fieldset>
 R code for the above session
<font color="red">
<pre>
p(6,1,4,4)
set.seed(89997)
x = 15+c(rep(1,50), seq(1,0,-0.2), rep(0,20), seq(0,1,0.2), rep(1,50))
x = x + rnorm(length(x))/10
trans = rgb(1,1,1,0.7)
f = function(i,p) {
  bareplot(x[1:p],ty='l',xlim=c(1,length(x)),ylim=c(0,20),lwd=3,col='blue')
  abline(h=0,v=1,lwd=3)
}
process('exop',f,90,1,1,120,bg=trans)
</pre>
</font>


</div>


<fieldset>
<legend>m6l1_d.yml</legend>
<pre>
- typeName: discussion
  prompt: &gt;
      Think of another example where we use time series for a
  purpose other than prediction. Make you example different from
  the ones presented in the video. [Hint: One of the earlier exercises in
  this module may give you an idea.]
  defaultFeedback: &gt;
      Change point analysis.

</pre>
</fieldset>



<h2><a
name="Module 6: Lesson 2: Plotting and real life examples">Module 6: Lesson 2: Plotting and real life examples</a></h2>

<b>Total lesson duration
= 33.7</b>
<p></p>

<div class="scrpt">
<b>Total video duration
= 5.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.70</i></legend>
In this lesson we are about to see real life examples of time
series. These data sets are all freely available, and are
retrieved from the internet.  
---
What is the first thing we do when have a time series data set?
The answer is: we plot it. That should always be the very first
step. The best type of plot for a time series is a <b>line chart</b>.
---
 A simple line chart can
disclose many important facts about a time series. And that's
what we are about to see now. 
*
</fieldset>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
covid.ods: Just plot and see.
</fieldset>

</div>

<fieldset>
<legend>m6l2_a.yml</legend>
<pre>
- typeName: discussion
  prompt: &gt;
     Consider the time series plot that you saw in the
  video. Can you think of a reason behind the sudden drop in the
  "recovered" line? 
  defaultFeedback: &gt;
      

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
newpol.ods: Make both plots. Compare.
</fieldset>

</div>

<fieldset>
<legend>m6l2_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the time series plots shown in the video. What is
  the overall trend? Increasing or decreasing? 
  defaultFeedback: &gt;
     If you think that the trend is decreasing, then you haven't
   the wierd thing in the time series plot:
  The time axis was flipped! Could you create the correct time
  series plot? 
      

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
sunspot.ods: mouseover to get peak positions
</fieldset>
 R code for the above screencast
<font color="red">
<pre>
p(6,2,3)
year = 1700:1988
sunspot=sunspot.year
csv(data.frame(year,sunspot),'sunspot.csv')
</pre>
</font>


</div>

<fieldset>
<legend>m6l2_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
    Consider a monthly sunspot data set from Jan, 1749 to Dec,
  1983:
    \frac ./exrauxb/sspot.csv
  Make a time series plot and guess an approximate period of
  repetition. 

  defaultFeedback: &gt;
      \frac ./exrauxb/sspotsol1.png

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
sfe
year = 1749:1983
years = rep(year,rep(12,length(year)))
months =
rep(c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'),
    length(year))
sunspot=sunspots
csv(data.frame(year,months,sunspot),'sspot.csv')
</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[n] <i>Duration=6</i></legend>
coffee.ods: differencing
</fieldset>

</div>

<fieldset>
<legend>m6l2_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider time series data in \frac ./exrauxb/quad.csv.
     Plot it and consider the trend pattern. It is not linear. 
     Try to difference it once, plot the differenced series. 
     Has the trend vanished? If not, repeat the differencing on
    the differenced series. How many diffeencing steps are needed
  to remove the trend in the original time series.
  defaultFeedback: &gt;
     The original series looks like this:
       \frac ./exrauxb/quadsol1.png
     After first differencing:
       \frac ./exrauxb/quadsol2.png
     There is still a linear  trend. Difference once again:
       \frac ./exrauxb/quadsol3.png
     Now the trend is gone. Here is how the second differencing
     is done.
       \frac ./exrauxb/quadsol4.png

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(4647)
day = 1:100
value = trim(10+0.01*tm+0.01*(tm-50)^2+rnorm(100))
plot(value,ty='l')
plot(diff(value,diff=2),ty='l')
csv(data.frame(day,value),'quad.csv')
</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
World bank data transposing and filtering
corrupt.ods:
</fieldset>

</div>


<fieldset>
<legend>m6l2_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     The data set used in the video is in
     \frac ./exrauxb/timeseries.xlsx.
     Repeat what the procedure shown in the video for 
     corruption data for Cameroon.
  defaultFeedback: &gt;
     \frac ./exrauxb/timeseriessol1.png 
     While copy-n-pasting to a different sheet after filtering 
     don't make the mistake of selecting the entire columns. 
     Then even the hidden cases will get selected. Select only
  the visible cells.
  
</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
World bank time series with error bars
corrupt2.ods:
</fieldset>

</div>


<fieldset>
<legend>m6l2_f.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Continue with the last exercise to arrive a timeseries with
  error bars.  
  defaultFeedback: &gt;
      \frac ./exrauxb/timeseriessol2.png

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="header">
<h3><a
name="Module 6, Lesson 2, Video 1: Plotting">Module 6, Lesson 2, Video 1: Plotting</a></h3>
</div>



<h2><a
name="Module 6: Lesson 3: Analysis">Module 6: Lesson 3: Analysis</a></h2>

<b>Total lesson duration
= 32.6</b>
<p></p>

<div class="header">
<h3><a
name="Module 6, Lesson 3, Video 1: Trend+Seasonal+Error">Module 6, Lesson 3, Video 1: Trend+Seasonal+Error</a></h3>
</div>


<div class="scrpt">
<b>Total video duration
= 2.2</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=2.20</i></legend>
We had earlier talked about looking through data as opposed to
looking at data. We look through data at the underlying
process. 
---
 Writing down a clear
statement about what we assume known/unknown about this
underlying process is called the <b>statistical model</b>. 
---
So far in our course, this process has been a just a distribution, which we
visualised as the shape of a histogram. This sufficed because
the variables that we were working with varied only with chance.
--- 
Like a coin toss, the same coin was tossed in each case. So we
could model it using the probability statement
<b>P(head) = p.</b>
---
This p was unknown, a parameter to be estimated. The same p
worked for all the tosses. 
---
But in a time series set up, the unknown truth may evolve with
time. So any time series model 
must start by clearly postulating the role of time. The simplest
way of achieving this is to take a mixture of a function of time and randomness.
---
We shall take a look at a few of them now. They will all take the
common form 
<b>$$y_t = f(t) + \epsilon_t,$$</b>
where $f(t)$ is the <b>non-random temporal evolution part</b>,
and $\epsilon_t$ is the <b>random error</b> part.
---
This form, where the temporal part and the random part are nicely
separate and combined by addition, is called an <b>additive
model</b>.
---
 Certain models, called <b>multiplicative models</b>,  combine them
using multiplication:
<b>$$y_t = f(t) \times \epsilon_t.$$</b>
---
Usually, a multiplicative model is used where all the values are
positive. Taking logarithm of the multiplicative model gives us an additive
model. So we shall focus our attention more on additive models
than multiplicative ones.
---
In the next video we shall see some common examples of additive models.*
</fieldset>

</div>

<fieldset>
<legend>m6l3_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Here is a toy data set generated from a multiplicative
  model: \frac ./exrauxb/mult.csv
  Make a time series plot. Then take logarithm to transform the
  data to arrive at an additive model. Plot again. Familiarise
  yourself with the difference.
  defaultFeedback: &gt;
      The original time series: \frac ./exrauxb/multsol1.png
      After taking logarithm: \frac ./exrauxb/multsol2.png

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
3rwf
set.seed(43523)
x = sin(0.1*(1:100)) + rnorm(100)
y = trim(exp(x))
plot(y,ty='l')
time=1:100
csv(data.frame(time, y),'mult.csv')
</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 3.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.40</i></legend>
There are many different time series models proposed in the
literature ranging from simple to extremely complex. Near the
simpler end of the spectrum...
---
 we have the ones that keep the
temporal and the random parts separate, as we discussed in the
last video, either in the additive way:
<b>$$y_t = f(t) + \epsilon_t,$$</b>
---
or in the multiplicative way:
<b>$$y_t = f(t) \times \epsilon_t.$$</b>
---
A multiplicative model may be reduced to an additive model by taking
logarithms. So let's focus on additive models only<b><font color="red">
[[shown]] </font></b>.
---
Any such  model needs us to specify some form of $f(t)$ and
some assumption on the random behaviour of
the $\epsilon_t$'s. 
---
Let's start with the $\epsilon_t$'s. We typically assume that
they have <b>zero mean</b>, and some unknown <b>constant
variance $\sigma^2,$</b> ie, variance free 
of $t,$ and are <b>uncorrelated</b> over time.
---
 This last assumption prevents $\epsilon_t$'s from
introducing temporal artifacts.  These assumptions 
are so very common that we have name for $\epsilon_t$'s
satisfying these: <b>white noise. </b>
---
Next let's choose various forms for $f(t).$*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Mathematical forms</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.60</i></legend>
The simplest possible $f(t)$ is just a constant<b><font color="red">
[[shown]] </font></b>, but then
there is no time in it at all.
---
 So it hardly deserves to be called
a time series model. Instead, let's make it just a little bit
non-trivial: a linear function in time:
<b>$$f(t) = \alpha + \beta t.$$</b>
---
The parameters $\alpha$ and $\beta $ are fixed and
unknown, and are good candidates to be estimated, along with the
variance $\sigma^2 $ of the random errors.
---
How does a typical plot look like for this model? It looks like
this<b><font color="red">
[[shown]] </font></b>. A straight line with some random
fluctuations.
---
 It is basically a linear regression set up, and we can easily
estimate $\alpha $, $\beta $ and $\sigma^2$ from
data using the least squares technique.
---
Let's step back from the math for a moment, and focus on the
plot. There are ups and down in the values, but on the whole it
is increasing.
---
 This overall behaviour, beyond local ups and
downs, is called the <b>trend</b>. Here the trend is linear, but it could
also be quadratic<b><font color="red">
[[shown]] </font></b>, like this, or
exponential<b><font color="red">
[[shown]] </font></b>, or something else.
---
 But as long as there is a mathemaical curve behind the trend, the least squares
technique is always applicable.
*
</fieldset>
 R code for the above session
<font color="red">
<pre>
p(6,3,2,2)
set.seed(34614)
tm = 1:30
y1 = scale(tm)+rnorm(length(tm),sd=2)
y2 = 10*scale(tm*tm)+rnorm(length(tm))
y3 = 10*scale(exp(tm/5))+rnorm(length(tm))
rng = range(0,y1,y2,y3)
y1 = y1 - rng[1]
y2 = y2 - rng[1]
y3 = y3 - rng[1]
f = function(i,p) {
  if(i&lt;30) {
    alpha = i/30
    y = (1-alpha)*y1 + alpha*y2
  }
  else {
    alpha = i/30-1
    y = (1-alpha)*y2 + alpha*y3
  }
  bareplot(tm,y,ylim=rng,ty='l',lwd=3)
  abline(h=0,v=1,lwd=3)
  if(i==1) 
    title(main="Linear") 
  else if(i==30)
    title(main="Quadratic") 
  else if(i==60) 
    title(main="Exponential") 

}
trans = rgb(1,1,1,0.7)
process('tstrans',f,60,2,bg='transparent')
</pre>
</font>


</div>

<fieldset>
<legend>m6l3_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
    What type of curve would you fit to this data set:
  \frac ./exrauxb/findtype.csv?      
    Make a plot to guess?
  defaultFeedback: &gt;
      Quadratic looks like a possible choice. So is
  exponential. Don't be fooled by the fact that the series can
  take negative values. When we say quadratic or exponential, we
  mean the general shape. You can always add a constant to move
  the series up or down without changing the shape!

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(3461)
tm = 1:50
y = 10*scale(tm*tm)+rnorm(length(tm))
csv(data.frame(tm,y),'findtype.csv')
plot(y,ty='l')
</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Least squares curve fit lab.
expo.ods: 1) Make plot
          2) Insert exponential trend line
</fieldset>
 R code for the above screencast
<font color="red">
<pre>
p(6,3,3)
set.seed(33344)
tm = 1:100
y = exp(-tm/20)+rnorm(100)/25
plot(tm,y,ty='l')
csv(data.frame(tm,y),'expo.csv')
</pre>
</font>


</div>

<fieldset>
<legend>m6l3_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
    Fit an exponential curve to the time series in \frac ./exrauxb/fitcurve.csv      
  defaultFeedback: &gt;
      \frac ./exrauxb/fitcurvesol.png

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(3334)
tm = 1:50
y = trim(2*exp(-tm/15)+rnorm(50)/20)
plot(tm,y,ty='l')
csv(data.frame(tm,y),'fitcurve.csv')
</pre>
</font>




<div class="scrpt">
<b>Total video duration
= 3.8</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=3.80</i></legend>
In the last video we learned about a class of time series models
where the time component and chance components were added
together, and we had a precise mathematical formula for the
temporal part. 
---
  Such models, while not always applicable, are the
easiest to interpret. However, there are a couple of drawbacks to
them.  
---
First, since we assume a precise mathematical
form for it, like linear or quadratic as we did in the last
video, the resulting model becomes too inflexible to use in
most real life scenarios over a long duration.
---
Second, there are only two extreme types of variations
allowed. The overall long term behaviour, the trend, and the very
short term random fluctuations due the $\epsilon_t$'s. 
---
In practice there may be variations in between these extremes. 
Here, for example, <b><font color="red">
[[shown]] </font></b> is a time series of monthly
sales. How would you describe it in words? It has an overall 
slowly rising trend, and  there are lots of ups and downs.
---
 On a closer look we notice that even the ups and downs are of
two types. There seems to be a wave that repeats every 12
months, <i>i.e.</i>, an annual cycle.
 Other than  that the remaining ups and downs appear to
be just random. 
---
Our mathematical curve model did not allow for the annual
cycles. Coming up with an exact math formula for them is not
easy. Instead statisticians use a different technique.
---
They propose the following model:
<b>$$
y_t = T_t + S_t + \epsilon_t.
$$</b>
---
Here $T$'s are the trend component, the $S$'s are the
seasonal component, and $\epsilon_t$'s are the random errors
as before.
---
 Along with this we also impose a periodic behaviour on
the seasonal component: <b>$S_{t+12} = S_t.$</b> For
example, if $t$ is a January, then $t+12$ is the next
January.
---
 Thus $S_t$ being equal $S_{t+12}$ means the
seasonal effect is due just to the month, not the year. Just to make sure
that the trend deals with just the mean behaviour,...
---
 we force the
seasonal component to have zero mean: <b>$S_1+\cdots+S_{12}=
0.$</b>
---
Also we do not postulate any mathematical form for $T_t$ or
the $S_t$ terms.
---
In absence of any mathematical form, the big question now is: How
to estimate them from the data? That's something we shall explore
in some later videos.
---
But before that let's understand the general form of the new
model carefully. We have a trend term, a seasonal term with known
period and a random error.
---
 The period of the cyclical
compomnt is usually obtained from domain knowledge, eg, for
monthly data we expect an annual variation, for daily data a
weekly variation and so on.
---
 Sometimes people superpose two
seasonal terms with two different periods. Like for a monthly
sales data, the shorter cycle is annual and the longer one has a
period of a few years and captures regular market ups and down.
---
 However, the more terms one puts in a model, the more difficult it gets to
interpret the fitted model.*
</fieldset>
 R code for the above session
<font color="red">
<pre>
p(6,3,4,1)
set.seed(46346)
trans = rgb(1,1,1,0.5)
png('perplot.png',bg=trans)
seas = c(0,1,2,3,-5,-6,0,3,5,10,5,1)
trnd = 5*sin((1:120)/120)
rnd = rnorm(100,sd=1)
ytmp = trnd + seas/2 + rnd
y = ytmp - min(ytmp) + 3
bareplot(y,ty='l',ylim=range(y,0),lwd=3)
abline(v=1,lwd=3)
abline(v=seq(12,120,12),lwd=3,lty=2,col='grey')
axis(1,seq(0,120,12),2000:2010,lwd=3)
dev.off()
</pre>
</font>


</div>

<fieldset>
<legend>m6l3_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     If we have a seasonal component of period 5, then we might
  as well consider the period as 10, because what repeats after
  every 5 values, must also repeat after every 10 values (since
  10 is a multiple of 5). Which one is better to use in a model?
  5 or 10? 
  defaultFeedback: &gt;
      5, because that requires only 5 terms. The less terms to
  estimate, the better are the estimators. Also, repetition after
  5 implies repetition after 10, but not the other way around!

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Here we shall play god and create and plot step by step.
Create a daily time series from scratch. The trend values along a
line and some day-of-the-week values adding up to 0. Finally
errors as rand()-1. 
god.ods: trend=10,10.3,...
         seas=5,6,2,0,-2,-4,-4
         err=randbetween(-3,3)
</fieldset>

</div>

<fieldset>
<legend>m6l3_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Repeat what you have learned in the video to create a
  synthetic time series of length 20 with  trend values 
  15, 15.1, 15.2,..., seasonal components 3, 4, -5, -2 and error
  terms generated by randbetween(-4,4). Make a plot of your time series.
  defaultFeedback: &gt;
      Here is what I got (your plot should surely differ):
      \frac ./exrauxb/synthsol.png
</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 5.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.50</i></legend>
In the last video we saw how we may make a time series from the
various components. Now that was just for the purpose of
illustration.
---
 In practice we need to be able to do the reverse
operation: given the time series we need to somehow isolate the different
components. Let's understand this with an  example.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
Here is a time series plot. The first step to analyse it is
to make a plot. The plot will give a visual check that our model
is a reasonable one and also give us a rough idea about the
period of cycle given. In many cases, however, the period is more easily
obtained from the data domain, <i>e.g.</i> a weekly cycle for a daily
data, or annual cycle for monthly data. 

Here we have a daily data with weekly cycles. Let's focus on the
very first cycle present in the data. Remember that the trend
captures only long term variation and the random error captutes
the very short term variations. The cyclical variation is
midway. So we may reasonally assume that the trend values do not
change appreciably over the a week, while the random errors
change a lot. So if you average these values, then the trend
value, being nearly a constant,  comes out. The random errors,
having mean 0, are cancelled to a great extent. The cyclical terms
cancel out perfectly, as we have assumed that they have mean zero
over any single period. So the average value basically gives us
the trend value. However, the trend value is really not a
constant, so it's more reasonable to say that the average gives
the trend value at the centre. Now we move the window one place
to the right, and repeat the exercise to get the next trend
values. This idea of moving the window and taking the average, is
called the moving average. Once we get the trend value like this
we subtract the trends from the series to get something which is
made of only the cyclical and random error parts.  To extract the
cyclical terms we perform averaging in a different way. We take
all the terms with $C_1.$ Averging them should roughly
cancel out the random errors, and yield an estimate
of $C_1.$ Similarly for the other $C_t$'s. 

Finally what remains are the random errors. Computing their
variance gives us an estimate of $\sigma^2.$ 

Of course, there are little points hee and there that I have
glossed over. Like how to get the trend values at the two ends,
or what to do if the cycles have an even period. 

We shall see these in the lab in the next video.

Then we perform a moving
average. This means taking average of one period worth of the
series at a time. Here we have a weekly cycle. So we average the
first 7, and plot it at the centre. It is as if we are looking at
the series through a window of length 7 and averaging the part of
the seris through the window. Next we move the window one step to
the right and take average. And we go on like this, move the
window and average the visible part. This is what is called
moving average. The result is a shorter series that has less ups
and downs. This is our estimated trend. Now we subtract the trend
from the original series (ignoring the two end). This part should
contain only the cyclical and error part. We stack the weeks one
above the other and average. Thus we get an average value for the
Mondays, another for the Tuesdays, and so on. Finally we subtract
this from the series, and consider the remainder as the error.

The next video will show this in action.*
</fieldset>

</div>

<fieldset>
<legend>m6l3_f.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Suppose that we have seasonality period p=7. If 
     S_t + ... + S_{t+6} is not equal to zero, as assumed in the
  video, what will go wrong? 
  defaultFeedback: &gt;
      The average value of the seasonality terms over a single
  period will get subtracted from the seasonality terms and get
  added to the trend.  

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Moving average lab.
dog.ods: trend=10,10.3,...
         seas=5,6,2,0,-2,-4,-4
         err=randbetween(-3,3)
         Start with components and plot. Find trend, plot. Find
seas, compare.
         Remember: avoid #NA while averaging.
</fieldset>

</div>

<fieldset>
<legend>m6l3_g.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the synthetic time series you  generated  as
  solution to an exercise a couple of videos back. Decompose that
  time series and see how close you are getting to the original
  values. 
 
  defaultFeedback: &gt;
    You'll face a little problem here, as the period is 4, which
    is even. So the moving averages do not align with the
    original series, but sit midway. This is easily rectified by
    taking a further 2-point moving average of the 4-point moving
    averages. The exact values depend on the random series
    generated by you.

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>






<h2><a
name="Module 6: Lesson 4: Prediction techniques">Module 6: Lesson 4: Prediction techniques</a></h2>

<b>Total lesson duration
= 33.3</b>
<p></p>


<div class="scrpt">
<b>Total video duration
= 6.1</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.70</i></legend>
So far we have given some idea about times series, what they are,
why they are important, and also given a very simple introduction
to modelling them in terms of trend and seasonal components. 
---
As we have already mentioned prediction, is the most important aim
of time series analysis. In this lesson we shall learn a few
prediction techniques.
---
 The techniques will depend on the type of
model we are assuming behind the time series. We shall start with
the curve fitting model.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Curve fitting model</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=1.20</i></legend>
The curve fitting models are of the form 
<b>$$y_t = f(t) + \epsilon_t,$$</b>
where $f(t)$ is some non-random function of time, 
involving some unknown parameters.
---
 We have already discussed how we may estimate these parameters
using the least squares approach. After this is done we have an
estimator <b>$\hat f (t)$</b> of $f(t).$
---
 Now suppose we want to predict for a new
time point <b>$t_{new}.$</b> Then we use <b>$\hat f(t_{new})$</b> as
our prediction.
---
 We may even provide an error bar for our
prediction in terms of <b>$\hat\sigma^2,$</b> the estimated variance of
the $\epsilon_t$'s.
---
 Something like 
<b>$$\hat f(t_{new}) \pm a \hat \sigma^2,$$</b>
where $a$ is some constant depending on the distribution of
the errors.
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Gaussian prediction interval</font><img src="image/jingle.png"></center>

<fieldset><legend
    class="bu">
[r] <i>Duration=4</i></legend>
 For example, if the random errors are assumed to
have <b>$N(0,\sigma^2)$</b> distribution, then we know that 
<b>$\frac{\epsilon_t}{\sigma}\sim N(0,1).$</b> And so
<b>$P(\left|\frac{\epsilon_t}{\sigma}\right| &lt; 1.96) = 0.95.$</b>
So an approximately 95% prediction interval would be
<b>$$
\hat f(t_{new}) \pm 1.96 \hat \sigma^2.
$$</b>
The next video will demonstrate this in the lab.*
</fieldset>

</div>

<fieldset>
<legend>m6l4_a.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     In the video we used the multiplier 1.96 to construct a 95%  prediction
  interval. How can you find the multiplier that we should  use to construct a 90%
  prediction interval? 
  defaultFeedback: &gt;
      To get a p% prediction interval we use a multiplier m, such
  that a standard normal random number has probability p% of
  lying in [-m,m]. See \frac ./exrauxb/normexpl.png. Here the green
  and red areas are equal and hence 0.5*(1-p/100) each. For p=90, 
  we can find m using LibreOffice Calc as \frac ./exrauxb/normexpl2.png

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Curve fitting prediction (point and interval).
cpred.ods: 1) Plot already shown (with gap for prediction)
           2) Insert square column
           3) Regress
           4) Predict (point)
           5) Locate sigma-hat
           6) Prediction interval
           7) error bar
</fieldset>
 R code for the above screencast
<font color="red">
<pre>
p(6,4,2)
set.seed(33344)
tm = 1:100
y = 10-tm/50+tm^2/100+rnorm(100)
plot(tm,y,ty='l')
csv(data.frame(tm,y),'cpred.csv')
</pre>
</font>



</div>

<fieldset>
<legend>m6l4_b.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Consider the time series data in \frac ./exrauxb/cpred2.csv. 
     Follow the procedure shown in the video to construct a 90%
  prediction interval for the value at time 110.
  defaultFeedback: &gt;
     \frac ./exrauxb/cpred2sol.png

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(3334)
tm = 1:100
y = trim(10-tm/50+tm^2/100+rnorm(100))
plot(tm,y,ty='l')
csv(data.frame(tm,y),'cpred2.csv')
</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 4.5</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=1.10</i></legend>
In the last video we learned about prediction based on  curve fitting models.
While this is conceptually simple, it suffers seriously from its
depndendence on the form of the curve.
---
 Often we may have
different contending curves that fit the data equally well, but
yet produce widely differing predictions. 
---
The next natural approach to prediction may appear to be one
based on the trend plus seasonal variation model:
<b>$$
y_t = T_t + S_t + \epsilon_t.
$$</b>
---
Unfortunately, this model is not particularly useful for
 prediction because here we do not assume any relaton
 between $T_t$ and $t$ that we may
 extrapolate.
---
 However, we may try to plot the trend and just
 graphically extrapolate it to some extent, and add the seasonal
 variation on top of it to get an informal prediction.
---
 But this approach is not very popular. 
We shall instead discuss a more popular approach called
 exponential smoothing.*
</fieldset>

<center>
<img src="image/jingle.png"><font color="blue" size="+3">Exponential smoothing</font><img src="image/jingle.png"></center>

<fieldset>
<legend>
(s2)[a] <i>Duration=0.30</i></legend>
This method should be used only for a time series without any
seasonal variation or any sytematic trend component. We shall
later discuss a variant that will be free of these restrictions.
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=3</i></legend>
We start with a time
series:
$$
x_1,x_2,...
$$
that is streaming in. As more an more data come in, we shall keep on
predicting the next value. When we get just the first
value $x_1$ our prediction for $x_2$ is simply $\hat
x_2 = x_1.$ Nothing very exciting. Now when we get the
actual $x_2$ value we quickly compute the error in our
prediction: $e_2 = x_2-\hat x_2,$ and we use this while
predctng the next value:
$$
\hat x_3 = \hat x_2 + \alpha e_2.
$$
Here $\alpha $ is called a tuning parameter that controls
how much importance we want to give to the last error. We
continue like this in each step. In general, we have
$$
\hat x_{n+1} = \hat x_{n} + \alpha e_{n}.
$$
The choice of $\alpha$ is not very critical. It is chosen as
something between 0 and 1. Values less than 0.3 are more commonly
used. Values closer to 0 give more importance to the past data,
while values closer to 1 give more importance to the recent data.  
There is no single correct method to choose $\alpha.$ Some
statisticians suggest tryin out diffeent values and then picking
the one that gives the least error.

We shall see this in action in the lab in the next video.*
</fieldset>

</div>

<fieldset>
<legend>m6l4_c.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Apply exponential smoothing to predict Y_3 from the two
  values: Y_1 = 3 and Y_2 = 4. Use alpha=0.2.
  defaultFeedback: &gt;
     Based on Y_1 along the predicted Y_2 is 3. After learning
  Y_2 = 4, the predicted value for Y_3 is 0.2*4 + (1-0.2)*3 = 3.2.

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 6</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=6</i></legend>
Lab showing exponential smoothing, and comparing different alpha
values.
exsm.ods
</fieldset>
 R code for the above screencast
<font color="red">
<pre>
p(6,4,4)
set.seed(463463)
tm = 1:100
x = rnorm(100,mean=37)
plot(tm,x,ty='l')
csv(data.frame(tm,x),'exsm.csv')
</pre>
</font>


</div>

<fieldset>
<legend>m6l4_d.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     Apply exponential smoothing with parameter value 0.3 on the
  time series data in \frac ./exrauxb/exsmex.csv
  defaultFeedback: &gt;
      \frac ./exrauxb/exsmexsol.png
  Be careful. You must select only the cells containing the
  series, and not the entire column!

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(4634)
tm = 1:50
x = trim(rnorm(50,mean=25))
plot(tm,x,ty='l')
csv(data.frame(tm,x),'exsmex.csv')
</pre>
</font>



<div class="scrpt">
<b>Total video duration
= 5.7</b>
<p></p>

<fieldset>
<legend>
(s1)[a] <i>Duration=0.70</i></legend>
We have mentioned that exponential smoothing is not suitable for
a time series with systematic trend and seasonal component in
it.
---
 There is a popular variant of exponential smoothing, however,
that takes care of these. It is called <b>Holt-Winters
prediction</b>.
---
 It assumes a locally linear trend of the
form $a+bt$ plus a seasonal component<b><font color="red">
[[shown]] </font></b>, and...
---
 uses three
exponential smoothings in parallel. One for $a$ one
for $b$ and the third on for the seasonal component. 
</fieldset>

<fieldset><legend
    class="bu">
[r] <i>Duration=5</i></legend>
$$\begin{eqnarray*}
\hat y_{t+1} &amp; = &amp; a_t + b_t  + s_{t+1-p}\\
a_t &amp; = &amp; \alpha (y_t-s_{t-p}) + (1-\alpha)(a_{t-1}+b_{t-1})\\
b_t &amp; = &amp; \beta(a_t-a_{t-1})+(1-\beta) b_{t-1}\\
s_t &amp; = &amp; \gamma (y_t-a_t) + (1-\gamma) s_{t-p}
\end{eqnarray*}$$
Let's digest this slowly. First imagine that we are trying to
predict each point from the past by fitting a straight
line $a+bt.$ At $t=0$ the line passes through the
current point. Putting $t=1$ we should get the next
point. Hence $\hat y_{t+1} = a_t + b_t.$ How to
choose $a_t?$ One guess is $y_t$ itself. Another comes
from $a_{t-1}+b_{t-1}.$ So we take an average. What
about $b_t?$ One guess is $a_t-a_{t-1}$ the other is
just the last value $b_{t-1}.$ Again we take an average. *
</fieldset>

</div>

<fieldset>
<legend>m6l4_e.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     This might lok like a silly exercise. But it will help you
  wrap your brain around the Holt-Winters formula. Just assume
  that all the seasonality term are zero. Now write down the
  Holt-Winters formulae given in the video (dropping mentions of
  all the S-terms). 
  defaultFeedback: |-
     yhat_{t+1}  =  a_t + b_t
a_t  =  alpha (y_t-s_{t-p}) + (1-alpha)(a_{t-1}+b_{t-1})
b_t  = beta(a_t-a_{t-1})+(1-beta) b_{t-1}
      

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>


<div class="scrpt">
<b>Total video duration
= 5</b>
<p></p>

<fieldset><legend
    class="cu">
[r] <i>Duration=5</i></legend>
Holt Winter's lab.
hw.ods: 1) Just for demo
        2) No seasonality (show formula)
        3) Place a_2, b_2 first (not a_1, b_1)
        4) Predict Y_3 (explain)
        5) Place a_3 lock alpha
        6) Place b_3 lock beta
        7) Drag and explain for 0,0
        8) Change to 1,1
        9) Mention need for init values.
</fieldset>

</div>

<fieldset>
<legend>m6l4_f.yml</legend>
<pre>
- typeName: textReflect
  prompt: &gt;
     The video did not really use Holt-Winters method for
  predicting the time series, though the formula for Yhat_{t+1}
  was shown. Watch the values shown in the video, and predict the
  next value in the time series. Use alpha=beta = 0.2.
  defaultFeedback: &gt;
      Yhat_{t+1} = alpha_t + beta_t = 11.03+0.53 = 11.56.

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>

</pre>
</font>




<fieldset>
<legend>m6_add.yml</legend>
<pre>
- typeName: peerReview
  prompt: &gt;
    Consider the time series values in \frac ./exrauxb/exposm.csv. 
    Perform exponential smoothing with alpha = 0.15.
    Overlay the smooth over the original time series.
  
  Rubric:
  -------
    1) The first few lines: \frac ./exrauxb/exposmsol2.png (1 point)
    2) The plot looks like \frac ./exrauxb/exposmsol1.png (1 point)

- typeName: peerReview
  prompt: &gt;
     \frac ./exrauxb/predint.xlsx contains a times series data. 
     Use the curve fitting approach to construct a 90% prediction
  interval for the value at time 53. Choose an appropriate parametric curve.
  defaultFeedback: &gt;
      
     Rubric:
     -------
       1) Here is the solution: 
         \frac ./exrauxb/predintsol.png
         The pattern is linear. So we have fitted a straight line.
       2) 1 point for fitting
       3) 1 point for the prediction interval

- typeName: multipleChoice
  prompt: &gt;
      In which of the following application areas are times
  series with irregularly spaced time points most commonly encountered?
  shuffleOptions: true
  options:
  - answer: Astronomy data 
    isCorrect: true
      
  - answer: Meterological data
      
  - answer: Econometric data
      
  - answer: Audio data.

- typeName: peerReview
  prompt: &gt;
    Consider the daily data stored in \frac ./exrauxb/decomp.xslx. 
    Make a times series plot.
    Assuming an additive model with seasonal period of length 5,
    estimate the trend component.  Overlay on the original plot.
  Rubric:
  -------
    1) The original plot: 
         \frac ./exrauxb/decompsol1.png (0.5 point)
    2) Part of the estimated trends:
         \frac ./exrauxb/decompsol2.png (1 point)
    3) Overlaid plot: 
         \frac ./exrauxb/decompsol3.png (0.5 point)
      
- typeName: peerReview
  prompt: &gt;
    Continue with the last problem. But this time estimate the 5
  seasonal components.
  Rubric:
  -------
    1) Part of the computation together with answer in last
       column: \frac ./exrauxb/decompseas.png (2 points)
    2) The yellow entry was redundant, but used to check that 
       seasonal components are repeating as expected.
    
      
      

</pre>
</fieldset>
 R code for the above
<font color="red">
<pre>
cw('exrauxb')
set.seed(463452)
tm = 1:100
x = trim(rnorm(100,mean=35))
plot(tm,x,ty='l')
csv(data.frame(tm,x),'exposm.csv')

cw('exrauxb')
set.seed(3334)
tm = 1:50
y = 10+tm/5+rnorm(50)
plot(tm,y,ty='l')
csv(data.frame(tm,y),'predint.csv')

cw('exrauxb')
set.seed(33341)
day=1:120
seas = sample(10,5)
seas = seas-mean(seas)
trnd = 5*sin((1:120)/120)
rnd = rnorm(100,sd=1)
ytmp = trnd + 3*seas + rnd
y = ytmp - min(ytmp) + 3
csv(data.frame(day,y),'decomp.csv')
bareplot(y,ty='l',ylim=range(y,0),lwd=3)
abline(v=1,lwd=3)
abline(v=seq(12,120,12),lwd=3,lty=2,col='grey')
axis(1,seq(0,120,12),2000:2010,lwd=3)

ddd
</pre>
</font>


<hr/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/>
</body>
</html>
