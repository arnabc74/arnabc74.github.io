<NOTE>
@{
<HEAD1>Exercise set 1</HEAD1>
<OL>
<LI>Do all the exercises at the end of chapter 2 of the
textbook.</LI>
<LI>Solve the following approximate system using R:
<MULTILINE>
3a + 4b + c & \approx & 3.4\\
3a + 4b + c & \approx & 3.5\\
4a + 3b + 2c & \approx & 10.1\\
4a + 3b + 2c & \approx & 9.8\\
6a + 5b + 2c & \approx & 5.6
</MULTILINE>
</LI>
<LI>Let's see how R tackles a linear model where the design matrix that is not full
column rank:
<D>
X = <MAT>
1 & 1 & 0\\
1 & 1 & 0\\
1 & 0 & 1\\
1 & 0 & 1\\
</MAT>,\quad 
\v y = <MAT>3.4\\3.5\\10.5\\10.3</MAT>.
</D>
Here the first column of <M>X</M> is a column of <M>1</M>'s. So
you may just type the last two columns in R, and omit the <CODE>-1</CODE>.
</LI>
<LI>
In the problem above R produced <I>one</I> least squares
solution. But we know that there are infinitely many. Write down
two more solutions. Can you write a general form for all least
squares solutions here?
</LI>
<LI>R automatically stores various qunatities computed
by <CODE>lm</CODE>. We shall explore some of them here. Let's
work with the linear model from the last exercise. Create the
full design matrix (including its first column) and type:
<R>
myfit = lm(y~X-1)
</R>
The variable <CODE>myfit</CODE> now contains lots of the
information about the fit. You may extract the computed least
squares solution <M>\hv \beta </M> as 
<R>
myfit$coef
</R>
This may be used in future computations. Compute <M>\h y = X\hv
\beta.</M> Remember that <CODE>%*%</CODE> is the R notation for
matrix multiplication. This <M>\h y</M> is the foot of the
perpendicular dropped from <M>\v y</M> to <M>\col(X).</M>
Usually <M>\hv y</M> is called the <B>fitted</B> vector. R already
computes them:
<R>
myfit$fitted
</R>
The vector <M>\v y - \hv y</M> is called the <B>residual</B>
vector:
<R>
myfit$resid
</R>
There are many other pieces of information packed
in <CODE>myfit</CODE>:
<R>
names(myfit)
</R>
</LI>
<LI>Consider a linear model <M>\v y = X \beta +\epsilon,</M>
where <M>X</M> is not full col rank. Pick any basis
of <M>\col(X).</M> Stack these vectors side by side a columns to
get a matrix <M>B.</M> Let <M>\v w = B(B'B) ^{-1} B' \v y.</M>
Show that <M>\v w = \hv y</M> irrespective of the choice
of <M>B.</M></LI>
<LI>Consider a linear model with design matrix 
<D>
X = <MAT>
1 & 1 & 0\\
1 & 1 & 0\\
1 & 0 & 1\\
1 & 0 & 1\\
</MAT>.
</D>
If <M>\v \beta = (\beta_1, \beta_2, \beta_3)',</M> then show that
whatever least squares solution <M>\hv \beta </M> you take, <M>\h
\beta_2-\h \beta_3</M> is always the same. Characterise all
vectors <M>\v \ell\in\rr^3</M> such that <M>\v \ell' \hv \beta</M>
does not depend on the choice of the least squares solution.
</LI>
<LI>Generalise the characterisation from the last problem to
arbitrary design matrix.</LI>
<LI><IMG web="basicex1.png"/></LI>
<LI>Redo the above problem with the extra condition: <M>\beta_0-\alpha_0 = (\beta_1-a_1) x_0.</M></LI>
</OL>
<DISQUSE id="lmex1" url="http://arnabc74@github.io/linmod/ex1.html"/>
@}
</NOTE>
