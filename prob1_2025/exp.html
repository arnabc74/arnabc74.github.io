<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE-BAN-OTF-new.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v19.0" nonce="Q7jTbrCq"></script>

<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else {
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script>
<style>
body,table {
  margin: 0;
  font-size: 40;
  //background: #000;
  //color: #fff;
}

.ans {
  display:none;
  background: #ccffcc;
}

.sticky {
  position: fixed;
  top: 0;
  width: 100%;
  background: #555;
  color: #f1f1f1;
}

.cu {
  background: #ffcccc;
}

.bu {
  background: #ccccff;
}

.scrpt {
  margin:10px;
  border-left: 5px solid black;
}

.box {
  background-color: yellow; 
  //border: 2px solid black;
  display: inline-block;
}

.hl {
  list-style-type: upper-alpha;
}
</style>
<script>
window.onscroll = function() {myFunction()};
window.onload = function() {myInit()};

var header, tphldr;
function myInit() {
  header = document.getElementsByClassName("header");
  tphldr = document.getElementById("topholder");
}

function myFunction() {
  var index = -1
  for(i=0;i<header.length;i++) {
    if (window.pageYOffset > header[i].offsetTop) {
       index = i
    }
    else {
       break
    }
  }

  if(index < 0) 
    tphldr.innerHTML = "";
  else
    tphldr.innerHTML = header[index].innerHTML
}
</script><script type="text/javascript" src="https://arnabc74.github.io/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="file:///home/asu/na/v/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head><body>
<div class="sticky" id="topholder"> </div>
<a href="http://web.isical.ac.in/~arnabc/">[Home]</a>
<h3>Expectation</h3>
<ul>
<li>
<a href="#Expectation of a random variable">Expectation of a random variable</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Simple random variables">Simple random variables</a>
</li>
<li>
<a href="#Random variables taking countably many values">Random variables taking countably many values</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#A couple of warnings">A couple of warnings</a>
</li>
<li>
<a href="#Properties of expectation">Properties of expectation</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Relation of $E(X)$ with values of $X$">Relation of $E(X)$ with values of $X$</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Transformation properties">Transformation properties</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Expectation of a function">Expectation of a function</a>
</li>
<li>
<a href="#Indicator trick">Indicator trick</a>
</li>
<li>
<a href="#Finite existence of $E(X)$">Finite existence of $E(X)$</a>
</li>
<li>
<a href="#Problems for practice">Problems for practice</a>
</li>
</ul>
<hr/>
<title xmlns="">Expectation</title>

<h1><a
name="Expectation of a random variable">Expectation of a random variable</a></h1>

<p></p>
For many random variables we see a striking example of
statistical regularity. As an example, consider this gambling game: 
<blockquote>A fair die is rolled. If it shows an odd number then I pay you Rs 20, else you pay me Rs 10.</blockquote>
A typical plot of my running average gain per game against number of games is as follows:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/explotnow.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
It is produced by the following code. 
<font color="red">
<pre>
w = sample(6,1000,rep=T)
profit =c(-20,10,-20,10,-20,10)
X = profit[w]
avgX = cumsum(X)/(1:1000)
#png('image/explotnow.png')
plot(avgX,ty='l',xlab="#games played",ylab="My running avg gain")
#dev.off()
</pre>
</font><input type="button"
value="Run in cloud"
onclick="javascript:aha(encodeURI(`
w = sample(6,1000,rep=T)
profit =c(-20,10,-20,10,-20,10)
X = profit[w]
avgX = cumsum(X)/(1:1000)
#png('image/explotnow.png')
plot(avgX,ty='l',xlab="#games played",ylab="My running avg gain")
#dev.off()
`));"/>
In fact, it is this phenomenon that first let man to study
probability. If you run a gambling game a large number of time
the running average profit per game becomes more and more stable. Gamblers wanted
to guess this stable value beforehand. They argued as follows:
<blockquote>
If I play this game a large number of times (say $n$ times),
then
approximately $\frac n2$ times I should get $10$
and the remaining $\frac n2$ times I should get $-20.$ So
approximately my total gain would be
$$
\frac n2\times 10 + \frac n2\times (-20).
$$
So the average should be approximately this divided by $n,$
<i>i.e.</i>,
$$
\frac 12\times 10 + \frac 12\times (-20) = -5.
$$
</blockquote>
Indeed, this simple argument turns out to be remarkably
accurate. Gamblers could not understand why it becomes so
accurate as $n$ becomes large. But nevertheless they used this formula to
find out what they could expect the random variable to do in the
long run.
<p></p>

<h2><a
name="Simple random variables">Simple random variables</a></h2>
A random variable is called <b><font color="red" size="40">simple</font></b>  if takes only finitely many values. 
<p></p>

<fieldset>
<legend><b>Definition: Expectation of simple random variables</b></legend>
Let  $X$ be a simpe random variable taking only the values
values $x_1,x_2,...,x_k$ with
probabilities $p_1,p_2,..., p_k$.
Then we define the <b><font color="red" size="40">expectation</font></b> of $X$ as
$$
E(X) = \sum_1^k p_i x_i.
$$
</fieldset>

<p></p>
::<p>
<b>EXERCISE 1:</b>&nbsp;(Easy)A random variable $X$  takes the values $-2, -1, 0, 1 $  and $2$  with
 probabilities $p,q,1-2p-2q, p$  and $q,$  respectively. Find $E(X).$</p>

<p></p>
::<p>
<b>EXERCISE 2:</b>&nbsp;(Easy)A random variable takes the values $1,2,...,10$  with probabilities
 $p_1,p_2,...,p_{10},$  respectively, where $\sum_i p_i = 1.$  Prove that $1\leq
 E(X)\leq 10.$  Also find $p_i$'s if $E(X) = 10.$  </p>

<p></p>

<h1><a
name="Random variables taking countably many values">Random variables taking countably many values</a></h1>

<fieldset>
<legend><b>Definition: </b></legend>
If $X$  takes only countably many <b>nonnegative</b> values $x_1, x_2, ...$   with probabilities
 $p_1,p_2,...$  where $\sum p_i = 1,$ then $E(X)$
 is defined as
$$E(X) = \sum p_i x_i.$$
</fieldset>

<p></p>
For any random variable $X$ we define $X_+ =
\max\{X,0\}$ and $X_- = -\min\{X,0\}.$ Notice that
both $X_+$ and $X_-$ are nonnegative. 
<p></p>

<p>
<b>EXERCISE 3:</b>&nbsp;(Easy)Which of the following must be true?
<ol class="hl">

<li>$X = X_++X_-.$</li>

<li>$X = X_+-X_-.$</li>

<li>$X = X_--X_+.$</li>

<li>None of the above.</li>

</ol>
</p>

<p></p>

<fieldset>
<legend><b>Definition: </b></legend>
If $X$  takes only countably many  values $x_1, x_2, ...$   with probabilities
 $p_1,p_2,...$  where $\sum p_i = 1,$ then $E(X)$
 is defined as
$$E(X) = \left\{\begin{array}{ll}
E(X_+)-E(X_-)&\text{if }E(X_+), E(X_-)&lt;\infty  \\
\infty &\text{if }E(X_+)=\infty, E(X_-)&lt;\infty  \\
-\infty &\text{if }E(X_+)&lt;\infty, E(X_-)=\infty  \\
\end{array}\right..$$
</fieldset>

<p></p>
Notice that we leave the case $E(X_+), E(X_-)=\infty $
unmentioned in the definition. This means $E(X)$ is
undefined in this case.
<p></p>

<p>
<b>EXAMPLE 1:</b>&nbsp;A random variable takes the values $\pm 2^n$  for $n\in{\mathbb N}$  with probabilities
 $P(X=2^n) = P(X=-2^n) = 2^{-n-1}$  for $n\in{\mathbb N}$. What is $E(X)$?
<p></p>
<b>SOLUTION:</b>
Here $X_+$  takes the values 0, 2, $2^2, 2^3,...$ with probabilities $2^{-1}, 2^{-2}, 2^{-3},...$.  
<p></p>
So $E(X_+) = 0 + \frac 12+\frac 12+\frac 12+\cdots = \infty$.
<p></p>
Similarly, $E(X_-) = \infty$.
<p></p>
Hence $E(X)$  is undefined.
 ■
</p>  

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X$  takes only countably many values $x_1, x_2, ...$   with probabilities
 $p_1,p_2,...$  where $\sum p_i = 1,$ and $\sum |p_i x_i|&lt; \infty,$ then 
$$E(X) = \sum p_i x_i.$$
</fieldset>

<p></p>
It is possible to define expectation of random variables that are more general (taking uncountably many values). We shall
 give most general definition in the next semester. The following properties actually hold for the genral definition. 
Unless noted otherwise, we
 shall only prove them for the case of simple random variables. These proofs are actually the first steps in the general proofs
 that will come next semester. 
<p></p>

<h2><a
name="A couple of warnings">A couple of warnings</a></h2>

<img src="image/alert.png">Many students somehow get the idea that if a (discrete) random variable $X$  takes
 the values $x_1,x_2,..$. with probabilities $p_1,p_2,...$ where $\sum p_i = 1$,
 then $E(X) = \sum_i x_i p_i$. <b>But this is wrong! It is wrong even if the sum converges!</b> 
This formula is true in the following special cases:
<ol type="">
<li>If $X$  is simple, <i>i.e.</i>, takes only finitely many values.</li>

<li>If $X$  takes only nonnegative (or only nonpositive) values (and here the formula holds even if the sum
diverges).</li>

<li>If $\sum_i |x_i| p_i &lt; \infty$.</li>

</ol> 

<p></p>
Here is another point that students sometimes get wrong. When we say $E(X)$  is <b>undefined</b>  we mean $E(X)$ 
 is meaningless in that context, and <i>not</i> that $E(X)$  can be anything there. 
<p></p>

<h1><a
name="Properties of expectation">Properties of expectation</a></h1>

<h2><a
name="Relation of $E(X)$ with values of $X$">Relation of $E(X)$ with values of $X$</a></h2>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X$ is a degenerate rv (<i>i.e.</i>, takes only one value with
probability 1), then $E(X)$ equals that value.
</fieldset>

<p>
<b><i>Proof:</i></b>Easy.<b><i>[QED]</i></b>
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X$ always takes values in $[a,b],$ then $E(X)$
must exist finitely, and lie in $[a,b].$
</fieldset>

<p>
<b><i>Proof:</i></b>
Easy.
<b><i>[QED]</i></b>
</p>
The condition "$X$ always lies in $[a,b]$" may be
written as $P(X\in[a,b])=1.$
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>Let $X$  and $Y$  be random variables taking only finitely many values, and  $X\leq
 Y.$  Then  $E(X)\leq E(Y).$  </fieldset>

<p>
<b><i>Proof:</i></b>
Here $X\leq Y$  means $\forall \omega\in\Omega~~X(\omega)\leq Y(\omega).$
<p></p>
As already mentioned, we shall restrict the proof to only the case where $X$  and $Y$  are both simple.
<p></p>
Let $X$ take values $x_1,...,x_m,$  and $Y$  take values $y_1,...,y_n.$
<p></p>
Let $p_{ij} = P(X=x_i, Y=y_j).$  
<p></p>
Clearly, if $p_{ij}&gt;0,$  then we must have $x_i\leq y_j.$  
<p></p>
Now 
$$\begin{eqnarray*}E(X) &amp; = &amp; \sum_i x_i P(X=x_i) = \sum_i (x_i \sum_j p_{ij}) =\sum_i\sum_j (x_i p_{ij})\\
&amp; \leq &amp;  \sum_i\sum_j (y_j p_{ij}) ~~[\because p_{ij}&gt;0\Rightarrow x_i\leq y_j]\\
&amp; = &amp;  \sum_j\sum_i (y_j p_{ij})[\because \mbox{addition is associative and commutative}]\\
&amp; = &amp; \sum_j (y_j \sum_i p_{ij}) = \sum_j y_j P(Y=y_j) = E(Y).
\end{eqnarray*}$$
<b><i>[QED]</i></b>
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $a\in{\mathbb R}$ be any number. If $P(X\leq a)=1,$
then $E(X)=a$ if and only if $X$ is degenerate at $a.$
</fieldset>

<p>
<b>EXERCISE 4:</b>&nbsp;(Easy)Prove this for simple $X$.</p>

<p></p>
However, if $a\in{\mathbb R}$ is replaced by $\infty,$ then the
result fails, <i>i.e.</i>, 
It is possible to have a random
variable $X$ that is always finite (any real-valued random
variable will do, since $\infty\not\in{\mathbb R}$) such
that $E(X)=\infty.$ Of course, we cannot get a counterexample using simple random variables.
 However, such counterexamples exist for random variables taking countably many values, as shown below. 
<p></p>

<p>
<b>EXAMPLE 2:</b>&nbsp;
It is a standard fact that $\sum\frac{1}{n^2}&lt;\infty.$ Let the
sum be $c.$ (The exact value of $c$
which is $\frac{\pi^2}{6},$ is of no importance here).
<p></p>
Then consider a random variable $X$ that takes values
in ${\mathbb N}$ and $P(X=n)=\frac{1}{cn^2}.$ 
<p></p>
Then $E(X) = \frac 1c\sum\frac 1n=\infty.$
 ■
</p>
By the way, if $X$ can take values $x_1,x_2,...$, there
is no guaranty that $E(X)$ will equal any of
the $x_i$'s. For example, if  $X$ is the outcome of
a fair die, then $E(X)=3.5,$ which is not a possible
outcome.
<p></p>

<h2><a
name="Transformation properties">Transformation properties</a></h2>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X$ be a random variable and let  $a,b$ be constants. Then $E(a+bX) = a+bE(X).$
</fieldset>

<p>
<b><i>Proof:</i></b>
Prove it for simple $X$.
<b><i>[QED]</i></b>
</p>

<p></p>
::<p>
<b>EXERCISE 5:</b>&nbsp;(Easy)
If $E(X) = \mu\in{\mathbb R},$ then what is $E(X-\mu)?$
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X,Y$ be two random variables both defined on the same probability space.
We assume that both $E(X)$ and $E(Y)$ both exist finitely.
<p></p>
Then $E(X+Y)$ also exists finitely and we have
$$
E(X+Y) = E(X)+ E(Y).
$$
</fieldset>

<p></p>
Next we shall need a new concept, that of a convex
function. Graphically, $f(x)$ is a convex function if its
graph is like a bowl opening upwards (possibly slanted). Some
examples are shown below.
<center>
<table width="100%">
<tr>
<th><img width="" src="image/convexex.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Mathematically we may define a convex function as follows.
<table align="right" width="20%" border="1">
<tr>
<td bgcolor="pink">While this definition is graphically quite intuitive, you
may have seen other definitions of convexity
elsewhere. Read <a href="convex.html">here</a> to learn more
about equivalences between different definitions of convexity.</td>
</tr>
</table>

<fieldset>
<legend><b>Definition: Convex function</b></legend>
A function $f:{\mathbb R}\rightarrow{\mathbb R}$ is called <b>convex</b>
if $\forall a\in{\mathbb R}$ there is a line $y = \ell_a(x)$
through $(a,f(a))$ that lies on or below the graph
of $f(x),$ <i>i.e.</i>, 
$$
\forall x\in{\mathbb R}~~ \ell_a(x) \leq f(x).
$$
</fieldset>
In the following diagram the blue line is $\ell_a.$ Both the
red lines are candidates for $\ell_b.$
<center>
<table width="100%">
<tr>
<th><img width="" src="image/suppline.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<fieldset>
<legend><b><i>Jensen's inequality</i></b></legend>
Let $X$ be a random variable and $f:{\mathbb R}\rightarrow{\mathbb R}$  be any convex function. We assume that $E(X)$ 
and $E(f(X))$ both exist finitely.  Then $f(E(X))\leq E(f(X)).$
</fieldset>

<p>
<b><i>Proof:</i></b>
Let $\mu = E(X).$ Consider $\ell_\mu(x)$ as mentioned
in the definition of convexity.
<p></p>
Since the graph of $\ell_\mu(x)$ is a straight line passing
through $(\mu,f(\mu)),$ hence it must be of the form
$$\ell_\mu(x) = f(\mu)+m(x-\mu),~~x\in{\mathbb R}.$$
So 
$$
E(f(X)) \geq E(\ell_\mu(X)) = E(f(\mu))+mE(X-\mu) = f(\mu)+0 = f(E(X)),
$$
as required.
<b><i>[QED]</i></b>
</p>

<p></p>
::<p>
<b>EXERCISE 6:</b>&nbsp;(Medium)Which is larger $(E(X))^2$ or $E(X^2)?$ Assume
that both exist finitely.</p>

<p></p>

<h2><a
name="Expectation of a function">Expectation of a function</a></h2>

<p>
<b>EXAMPLE 3:</b>&nbsp;
Suppose I have a random variable that takes values $-1,0$ and $1$
with probabilities $0.1, 0.5$ and $0.4,$ respectively.
What is $E(X^2)?$
<p></p>
<b>SOLUTION:</b>
Here $X^2$ is a new random variable. Call it $Y,$ say. Then $Y$
takes values $0$ and $1$ with probabilities $0.5$
each.
<p></p>

<p></p>
So $E(Y) = \frac 12.$
 ■
</p>

<p></p>
Here is another technique to arrive at the same result. 
$$
E(X^2) = 0.1\times (-1)^2 + 0.5\times 0^2 + 0.4\times 1^2 = 0.5.
$$
This technique is often easier because here we do not need to
find the distribution of $Y=X^2$ first. Both these
techniques will always give the same answer. 
<p></p>

<fieldset>
<legend><b><i>Law of the lazy statistician</i></b></legend>
Let a (discrete) random variable $X$ take
values $x_1,x_2,...$ with
probabilities $p_1,p_2,...$. Let $h(\cdot)$ be any
function defined on the set $\{x_1,x_2,...\}.$ If $\sum |p_i h(x_i)|
&lt;\infty,$ then we must have 
$$
E(h(X)) = \sum p_i h(x_i).
$$
Also, if $\sum|p_i h(x_i)|=\infty$ and all but finitely many
$h(x_i)$'s are $&gt;0$ (resp, $&lt;0$),
then $E(h(X))=\infty $(resp, $-\infty$).
</fieldset>

<p>
<b><i>Proof:</i></b>
If $X$ takes only finitely many values, then the result
follows from distributivity of multiplication over addition. 
<p></p>
If $X $ takes countably infinitely many values, and $h(X)$  is non-negative, then define 
$$
U_n =\left\{\begin{array}{ll}h(X)&\text{if }X=x_1,...,x_n\\ 0&\text{otherwise.}\end{array}\right.
$$  
and proceed as for the proof of $E(X)=\sum p_i x_i.$ 
<b><i>[QED]</i></b>
</p>

<p></p>

<h1><a
name="Indicator trick">Indicator trick</a></h1>
Often we have to find $E(X)$  where $X$  is the count of something, <i>e.g.</i>, number of heads in 100 tosses of coin,
 or number of times something interesting happens. If you want to find $E(X)$  directly from the definition, then you
 need to find the distribution of $X$  first, which is often difficult. In such situatons the
 <b><font color="red" size="40">indicator trick</font></b> may provide a short cut. 
<p></p>

<p>
<b>EXAMPLE 4:</b>&nbsp;We have a ring of 20 lamps. A wind blows and a random subset of lamps go out. Find
 the expected number of singleton lights (<i>i.e.</i>, lighted lamps with both neighbours off).
<center>
<table width="100%">
<tr>
<th><img width="" src="image/lamps.png"></th>
</tr>
<tr>
<th>The singletons are shown with arrowheads</th>
</tr>
</table>
</center>

<p></p>
<b>SOLUTION:</b>
Let $X$  be the number of singletons. Finding the distribution of $X$  is not very difficult, but still we shall
 demonstrate the use of the indicator trick. 
<p></p>
We shall use the arrowheads as our random variables. Let the lamps be numbered from 1 to 20. Define $L_i=1$ 
 if $i$-th lamp is on and is a singleton, and $0$  else. In other words, $L_i=1$  means we have put an
 arrow head at position
 $i.$  
<p></p>
Each $L_i$  is called an <b><font color="red" size="40">indicator variable</font></b>. 
<p></p>
Clearly $X = L_1+\cdots+L_{20}.$
<p></p>
So $E(X) = E(L_1)+\cdots+E(L_{20}) = 20 E(L_1),$  since by symmetry all the $L_i$'s have the same distribution.
<p></p>
To find $E(L_1)$  we need to find just $P(L_1=1)$, which involves only lamp 1 and its two neighbours. It should
 be clear that $P(L_1) = \frac{1}{8}.$  
<p></p>
Hence $E(X) = \frac{20}{8} = \frac 52.$  
    ■
</p>

<p></p>

<h1><a
name="Finite existence of $E(X)$">Finite existence of $E(X)$</a></h1>
We know from the definition of expectation that it may come in four varieties: it may be finite, or $\infty$  or $-\infty$ 
 or undefined. 
The finite case is the most useful, and it 
sometimes helps to know some sufficient conditions for this.
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X$ is simple, 
then $E(X)$ must be finite. 
</fieldset>

<p>
<b><i>Proof:</i></b>
Goes without saying!
<b><i>[QED]</i></b>
</p>

<p></p>
Non-negative random variables have the advantage that their expectation is always defined (though may be $\infty$). Now,
 from any random variable $X$  we can easily manufacture a non-negative random variable, viz, $|X|.$  It is 
good to be able to relate $E(X)$  with $E(|X|).$
<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
$E(|X|)$  is finite if and only if $E(X)$  is finite. 
</fieldset> 

<p>
<b><i>Proof:</i></b>
We define $X_+, X_-$  as usual. 
<p></p>
Then $X = X_+-X_-$  and $|X| = X_++X_-$. 
<p></p>
Then finiteness of $E(|X|)$  is equivalent to finiteness of both  $E(X_+), E(X_-).$  
<p></p>
Again, finiteness of $E(X)$  is equivalent to finiteness of both  $E(X_+), E(X_-).$  
<p></p>
Hence the result.
<b><i>[QED]</i></b>
</p>

<p></p>
::<p>
<b>EXERCISE 7:</b>&nbsp;(Medium)If $E(|X|)=\infty,$  then what can you say about $E(X)?$
<p><a
href="javascript:hideShow('lab1')"><b>[Hint]</b></a><div
class="ans" id="lab1">May be $\infty$  or $-\infty$  or undefined.</div></p>

</p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X,Y$ be random variables defined on the same probability space. Let $|X|\leq|Y|.$
  If $E(Y)$  is finite, then $E(X)$  must also be finite. 
</fieldset>

<p>
<b><i>Proof:</i></b>
Since $E(Y)$  is finite, hence  $E(|Y|)$  is finite. So $E(|X|)\leq E(|Y|)$ 
 is also finite. Hence $E(X)$  is finite. 
<b><i>[QED]</i></b>
</p>

<p></p>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X,Y$  be random variables defined on the same probability space. Let $E(X)$  and $E(Y)$  both be finite.
 Then $E(\max\{X,Y\})$  must also be finite.
</fieldset>

<p>
<b><i>Proof:</i></b>
<b>Step 1:</b>  First we shall prove it for nonnegative $X,Y$. Since $E(X)$  and $E(Y)$  are both finite,
 so must be $E(X+Y)$. 
<p></p>
Now, thanks to nonnegativity of $X,Y$  we must have $\max\{X,Y\}\leq X+Y$. 
<p></p>
So, by the last theorem, $E(\max\{X,Y\})$  must be finite. 
<p></p>

<b>Step 2:</b>  Apply <u>step 1</u>  to $|X|$  and $|Y|$  and use the fact that
 for any $a,b\in{\mathbb R}$  we have $|\max\{a,b\}|\leq \max\{|a|,|b|\}$.
<b><i>[QED]</i></b>
</p>
It is possible to combine the two steps into a single line proof. But stepwise thinking starting with the nonnegative case
 is a good habit. 
<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $m&lt;n$ be any two positive integers. If $E(X^n)$
exists finitely, then $E(X^m)$ must also exist finitely.
</fieldset>

<p>
<b><i>Proof:</i></b>
Use the fact that $|X^m| \leq \max\{1,|X^n|\}.$ Now use the
last theorem.
<b><i>[QED]</i></b>
</p>

<p></p>
The following theorem often proves very useful. 
<p></p>

<fieldset>
<legend><b><i>Markov inequality</i></b></legend>
Let $X$  be any non-negative random variable. Let $\epsilon&gt;0.$  Then 
$$E(X)\geq \epsilon P(X\geq\epsilon).$$
</fieldset>

<p>
<b><i>Proof:</i></b>
Let $Y =\left\{\begin{array}{ll}\epsilon&\text{if }X\geq \epsilon\\ 0&\text{otherwise.}\end{array}\right.. $
Then clearly, $X\geq Y$. So $E(X)\geq E(Y) = \epsilon P(X\geq \epsilon)$.
<b><i>[QED]</i></b>
</p>

<p></p>

<h1><a
name="Problems for practice">Problems for practice</a></h1>
::<p>
<b>EXERCISE 8:</b>&nbsp;(Easy)
What is $E(X)$ if $X$ takes the
values $1,2,...,n$ with probability $\frac 1n$ each?
<p><a
href="javascript:hideShow('lab2')"><b>[Hint]</b></a><div
class="ans" id="lab2">$\frac{n+1}{2}.$</div></p>

</p>
::<p>
<b>EXERCISE 9:</b>&nbsp;(Medium)Show that if $X$  is a random variable taking only non-negative
integer values, then 
$$E(X) = \sum_{k=1}^\infty P(X\geq k).$$
This formula often proves useful for finding expected counts.
<p><a
href="javascript:hideShow('lab3')"><b>[Hint]</b></a><div
class="ans" id="lab3">
Let $p_n = P(X=n)$  for $n=0,1,2,3,...$
Then
$$\begin{eqnarray*}
P(X\geq 1) &amp; = &amp; p_1 + p_2 + p_3+\cdots\\
P(X\geq 2) &amp; = &amp; \phantom{p_1 +} p_2 + p_3+\cdots\\
P(X\geq 3) &amp; = &amp; \phantom{p_1 + p_2 +} p_3+\cdots\\
\cdots
\end{eqnarray*}$$
Now add columnwise. Non-negative series do not change value when 
you rearrange the terms.
</div></p>

</p>
::<p>
<b>EXERCISE 10:</b>&nbsp;(Medium)For a group of $n$  people find the expected number of days of the year which are
 birthdays of exactly $k$  people. (Assume 365 days and that all arrangements are equally
 probable.) Also find the expected number of multiple birthdays. How large should $n$  be to
 make this expectation exceed 1?
<p><a
href="javascript:hideShow('lab4')"><b>[Hint]</b></a><div
class="ans" id="lab4"> 
Let $X_i = \left\{\begin{array}{ll}1&\text{if }\mbox{exactly $k$ people have birthdays on day} i\\ 0&\text{otherwise.}\end{array}\right..$
<p></p>
Then $X = \sum_1^{365} X_i.$  
<p></p>
So $E(X) = \sum_1^{365} E(X_i).$
<p></p>
Expected number of days of the year which are  birthdays of exactly $k$  people is $\binom{n}{k}\frac{364^{n-k}}{365^{n-1}}.$
<p></p>
Expected number of multiple birthdays is $365\left\{1-\left(\frac{364}{365}\right)^n - \frac{n\times 364^{n-1}}{365^n}\right\}.$
<p></p>
</div></p>

</p>
::<p>
<b>EXERCISE 11:</b>&nbsp;(Medium)A man with $n$  keys wants to open a door (where exactly one key works). He tries the
 keys independently at random. Find the expected number of trials needed to open the door if 
 keys are tried (a) with replacement (b) without replacement.
<p><a
href="javascript:hideShow('lab5')"><b>[Hint]</b></a><div
class="ans" id="lab5">(a) $\sum_1^ \infty k\cdot \left(1-\frac 1n\right)^{k-1}\cdot\frac 1n = \cdots = n.$ 
<p></p>
 (b) $\sum_1^n \frac kn = \frac{n+1}{2}.$</div></p>

</p>
::<p>
<b>EXERCISE 12:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv5.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab6')"><b>[Hint]</b></a><div
class="ans" id="lab6">
Here $p$  is what he says, and $p^*$  is what he believes. The meteorologist is not an honest one, and may 
say something different from what he believes. His only aim is to maximise the expected score. 
<p></p>
The expected score is 
$$p^*(1-(1-p)^2) + (1-p^*)(1-p^2).$$
This is to be maximised wrt $p$  (with $p^*$  fixed). 
<p></p>
Differentiate (or think of the graph) to see that the maximising value of $p$  is $p^*.$
</div></p>

</p>
::<p>
<b>EXERCISE 13:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv6.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab7')"><b>[Hint]</b></a><div
class="ans" id="lab7">
Company pays the amount $A$  with probability $p.$  It pays $0$  with probability $1-p.$
<p></p>
So its expected payoff is $pA.$  
<p></p>
Suppose that it charges $B.$  Then expected profit is $B-pA.$  To keep it 10% of $A$  we need $B = (p+0.1)A.$
</div></p>

</p>
::<p>
<b>EXERCISE 14:</b>&nbsp;(Medium)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv7.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab8')"><b>[Hint]</b></a><div
class="ans" id="lab8">
(a) $E(X)$  would be larger, because when a student is selected at random, he is more likely to come from the larger
 buses. So $E(X)$  is a weighted average of the bus sizes where the larger buses get more weight. 
<p></p>
But $E(Y)$  is the simple average of the bus sizes.
<p></p>
(b) $E(X) = \frac{40^2+33^2+25^2+50^2}{40+33+25+50}.$
<p></p>
$E(Y) = \frac{40+33+25+50}{4}.$
<p></p>
</div></p>

</p>
::<p>
<b>EXERCISE 15:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv8.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p></p>
We assume that no draw is possible. 
<p><a
href="javascript:hideShow('lab9')"><b>[Hint]</b></a><div
class="ans" id="lab9">
By the pigeon hole principle, the winner will be decided by at least 2 and at most 3 games. 
So the sample space is $\{AA, BB, ABA, BAA, BAB, ABB\}.$  The probabilities are, respectively, 
$p^2, q^2, p^2q, p^2q, pq^2, pq^2,$ where $q=1-p.$
<p></p>
 If $X$  is the random variable denoting the number of
 games played, then it takes the values, respectively, $2,2,3,3,3,3.$
<p></p>
So $E(X) = 2(p^2 + q^2) + 3(2p^2q + 2pq^2)  = 2(1+pq).$
<p></p>
This is maximised when $pq = p(1-p)$  is maximised, which is when $p=\frac 12.$
</div></p>
</p>
::<p>
<b>EXERCISE 16:</b>&nbsp;(Easy)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv9.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
<p><a
href="javascript:hideShow('lab10')"><b>[Hint]</b></a><div
class="ans" id="lab10">
(a) $E\big( (2+4X)^2 \big) = 4E(1+4X+4X^2) = 4(1+4E(X)+4E(X^2)) = \cdots$.
<p></p>
(b)$ E(X^2+(X+1)^2) = E(2X^2+2X+1) = 2E(X^2)+2E(X)+1 = \cdots$.
<p></p>
</div></p>
</p>
::<p>
<b>EXERCISE 17:</b>&nbsp;(Medium)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv11.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab11')"><b>[Hint]</b></a><div
class="ans" id="lab11">
(a) Let $X_i = \left\{\begin{array}{ll}1 &\text{if }i\mbox{-th draw is white}\\ 0&\text{otherwise.}\end{array}\right.$  for $i=1,...,10.$
<p></p>
Then $E(X_i) = P(i$-th draw is white$)=\frac{17}{40}.$  
<p></p>
So $E(X) = 10\times \frac{17}{40} = \frac{17}{4}.$  
<p></p>
(b) Let $Y_i = \left\{\begin{array}{ll}1&\text{if }i\mbox{-th white ball is selected}\\ 0&\text{otherwise.}\end{array}\right.$  for $i=1,...,17.$
<p></p>
Then $E(Y_i) = P(i$-th white ball is delected$)=\frac 14.$  
<p></p>
So $E(X) = 17\times \frac 14 = \frac{17}{4}.$  
<p></p>
</div></p>

</p>
::<p>
<b>EXERCISE 18:</b>&nbsp;(Medium)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv12.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
<p><a
href="javascript:hideShow('lab12')"><b>[Hint]</b></a><div
class="ans" id="lab12">
Let $X_i$  be as given in the hint. 
<p></p>
Let $X = $ number of intact marriages.
<p></p>
Then $X = \sum_1^{100} X_i$
<p></p>
Now $E(X_i) = \binom{198}{50}/\binom{200}{50}=\frac{150\times149}{200\times199}.$
<p></p>
So $E(X) = \frac{150\times149}{2\times199}.$
</div></p>
</p>
::<p>
<b>EXERCISE 19:</b>&nbsp;(Easy)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv13.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab13')"><b>[Hint]</b></a><div
class="ans" id="lab13">
$E(X) = \frac 52.$
</div></p>

</p>
::<p>
<b>EXERCISE 20:</b>&nbsp;(Medium)<center>
<table width="100%">
<tr>
<th><img width="" src="image/rossrv14.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Here we assume that $E(X)$  exists finitely. The inequality holds even if 
$E(X^2)$  is not finite (with the interpretation that $\forall a\in{\mathbb R}~~\infty \geq a$.)
<p><a
href="javascript:hideShow('lab14')"><b>[Hint]</b></a><div
class="ans" id="lab14">
You may either use Jensen's inequality with the convex function $f(x)=x^2$  or the fact that $V(X)\geq 0.$
<p></p>
Equality if and only if $V(X)= 0$, <i>i.e.</i>, if $X$  is a degenerate random variable.
</div></p>

</p>

<p></p>
::<p>
<b>EXERCISE 21:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/fellrv1.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
You may use some approximations in parts (c) and (d) of this problem. For instamce there are
 $\frac nk$  groups of $k$  patients each, even if $\frac nk$  is not an integer. You
 may also differentiate w.r.t. $k.$ 
<p><a
href="javascript:hideShow('lab15')"><b>[Hint]</b></a><div
class="ans" id="lab15">(a) $1-(1-p)^k.$
<p></p>
(b) For a group of size $k$  the random variable $X$  takes the value 
$k+1$  with probability $1-(1-p)^k$  and the value $1$  with probability $(1-p)^k.$  
<p></p>
So $E(X) = (k+1)(1-(1-p)^k)+(1-p)^k = k+1-k(1-p)^k.$
<p></p>
If there are $N$  people in all, where $N = qk+r$  with $r\in\{0,...,k-1\}$, 
then this applies to each of the $q$  groups and also the reaminder group of size $r.$  
<p></p>
So the required expectation is 
 $$q(k+1-k(1-p)^k)+r+1-r(1-p)^r.$$
If $k$  divides $N$, then this is 
$$\frac Nk(k+1-k(1-p)^k) = N+\frac Nk-N(1-p)^k = N\left(1+\frac 1k-(1-p)^k\right).$$
<p></p>
(c) Enough to minimise $1+\frac 1k-(1-p)^k$  wrt $k$  for given $p.$
<p></p>
Treating $k $ as a continuous variable, the derivative is 
$$-\frac{1}{k^2}-(1-p)^k\log(1-p).$$
<p></p>
</div></p>

</p>
::<p>
<b>EXERCISE 22:</b>&nbsp;(Medium)<center>
<table width="100%">
<tr>
<th><img width="" src="image/fellrv2.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab16')"><b>[Hint]</b></a><div
class="ans" id="lab16">
Here $P(X=k) = \binom{k-1}{r-1}p^rq^{k-r}$  for $k=r,r+1,...$  where $q = 1-p.$
<p></p>
So 
$$E\left(\frac rX\right) = \sum_{k=r}^\infty \binom{k-1}{r-1}p^rq^{k-r}\frac rk.$$
Ignoring the  terms free of $k$, and massaging the rest a little, the sum  reduces to 
$$\sum_{k=0}^\infty \frac{k(k-1)\cdots(k-r+2)}{k+1} q^k.$$
This may be handled by repeated term by term integration and differentiation of the power series
$$1+q+q^2+\cdots = \frac{1}{1-q}$$
for $|q|&lt;1.$
<p></p>
You may like to deal with the $r=1$  case first.
</div></p>

</p>
::<p>
<b>EXERCISE 23:</b>&nbsp;(Medium)<center>
<table width="100%">
<tr>
<th><img width="" src="image/most4.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab17')"><b>[Hint]</b></a><div
class="ans" id="lab17">Let $X = $ the number of trials needed to get the first 6. 
<p></p>
Then $P(X=k) = \left(\frac 56\right)^{k-1}\frac 16$  for $k=1,2,3,....$
<p></p>
So $E(X) = \sum_{k=1}^\infty k \left(\frac 56\right)^{k-1}\frac 16.$
<p></p>
Now, we know that $\frac{1}{1-x} = 1+x+x^2+x^3+\cdots$
if $|x|&lt;1.$  This may be differentiated term by term (needs a justification that you should learn in your real analysis
 class) to give
$$\frac{1}{(1-x)^2} = 1+2x + 3x^2+\cdots.$$
Put $x=\frac 56$  to find the required expectation.
</div></p>

</p>
::<p>
<b>EXERCISE 24:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/most6.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab18')"><b>[Hint]</b></a><div
class="ans" id="lab18">
Assuming the dice to be fair, the answer does not depend on the number 
the gambler bets on. 
<p></p>
Let $X$  be the loss for unit stake on 1.
Then
$$X = \left\{\begin{array}{ll}
1&\text{if }\mbox{no die shows 1}\\
-1&\text{if }\mbox{exactly 1 die shows 1}\\
-2&\text{if }\mbox{exactly 2 dice show 1}\\
-3&\text{if }\mbox{all 3 dice show 1}\\
\end{array}\right..$$
So $P(X=1) = \left(\frac 56\right)^3$, $P(X=-1) = 3\left(\frac 16\right)\left(\frac 56\right)^2$, 
$P(X=-2) =
 3\left(\frac 16\right)^2\left(\frac 56\right)$  and $P(X=-3) = \left(\frac 16\right)^3.$
<p></p>
Hence   
$$E(X) = \left(\frac 16\right)^3(5^3-3\times 5^2-6\times5-3).$$
</div></p>

</p>
::<p>
<b>EXERCISE 25:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/most14.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab19')"><b>[Hint]</b></a><div
class="ans" id="lab19">
Let $T_1 = 1.$
<p></p>
Let $T_i = $  waiting time for the $i$-th new coupon after the $(i-1)$-th coupon has been encountered, for
 $i=2,3,4,5.$
<p></p>
Consider the following example to understand the definition of $T_i$'s. Suppose that the coupons arive in the order:
<blockquote> 
<font color="#ff0000">3</font> 3 <font color="#ff0000">4</font> 3 <font color="#ff0000">5</font> 4 3 4 <font color="#ff0000">2</font> 3 4 5 <font color="#ff0000">1</font>.</blockquote>
The first occurences of each type of coupon have been shown in red. They are at positions 
$$S_1 = 1, S_2 = 3, S_3 = 5, S_4=9\mbox{ and } S_5=13.$$
We are defining $T_i = S_i-S_{i-1}$  for $i=1,...,5$  where $S_0=0.$
<p></p>
Then the $T_i$'s are independent random variables. 
<p></p>
$T_1$  is degenerate at 1,  and for $i=2,...,5$  we have
$$P(T_i = k) = q_i^{k-1}p_i$$  for $k\in{\mathbb N}$  where $q_i = \frac{i-1}{5}$  and $p_i = 1-q_i.$
<p></p>
We can easily find $E(T_i)$'s. 
<p></p>
The answer to the problem is $E(T_1+\cdots+T_5) = 1+E(T_2)+\cdots+E(T_5).$
<p></p>
[Thanks to Nipam for correcting a mistake here.]
</div></p>

</p>
::<p>
<b>EXERCISE 26:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/most15.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab20')"><b>[Hint]</b></a><div
class="ans" id="lab20">
<center>
<table width="100%">
<tr>
<th><img width="" src="image/bachmod.png"></th>
</tr>
<tr>
<th>The same person may be part of two marriageable couples.</th>
</tr>
</table>
</center>
The guys are all distinct, and so are the girls (though it is not clear from my wonderful artwork!). 
<p></p>
The diagram shows 8 <i>run</i>s, <i>i.e.</i>, stretches of same gender. A single girl or a single guy consitute the shortest possible
 run. Notice that the number of marriageable couples  is one less than the number of runs.
<p></p>
Thus, the number of arrangements with $k$  marriageable couples is
 the same as the number of arrangements with $k+1$ runs.
 Here $k$  can take any value between $1$  and 14.
<p></p>
As an example let us find $P(k=3).$  
<p></p>
The total number of arrangements is of course $15!.$
<p></p>
We need $3+1=4$  runs: either male-female-male-female or female-male-female-male. 
<p></p>

<ul>
<li>
<b>Step 1:</b> Arrange the guys: 8! ways</li>

<li>
<b>Step 2:</b> Arrange the girls: 7! ways</li>

<li>
<b>Step 3:</b> Insert a separator to split  the guys into two runs: 7 ways</li>

<li>
<b>Step 4:</b> Insert a separator to split  the girls into two runs: 6 ways</li>

<li>
<b>Step 5:</b> Mix them: 2 ways (M-F-M-F or F-M-F-M)</li>

</ul>
So 
$$P(k=3) = \frac{8!\times7!\times7\times6\times2}{15!}.$$
Find these for all possible values $k$, and then compute expectation. 
<p></p>
Or...use the indicator trick!!!
</div></p>

</p>
::<p>
<b>EXERCISE 27:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/most34.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab21')"><b>[Hint]</b></a><div
class="ans" id="lab21">
Let there be $n$  workers. Let $X$  be the number of working days. 
<p></p>
Let $X_i=\left\{\begin{array}{ll}1&\text{if }i\mbox{-th day is a working day}\\ 0&\text{otherwise.}\end{array}\right..$
<p></p>
Then $X = \sum_1^{365} X_i.$
<p></p>
Now $E(X_i) = P(i$-th day is a working day$)= \left(\frac{364}{365}\right)^n.$
<p></p>
So $E(X) = 365\times \left(\frac{364}{365}\right)^n.$
<p></p>
Hence expected number of man-days is 
$365n\times \left(\frac{364}{365}\right)^n=f(n)$, say.
<p></p>
We want to maximise this wrt $n.$  
<p></p>
Now 
$$\frac{f(n+1)}{f(n)} = \frac{n+1}{n}\times \frac{364}{365}.$$
This is $&gt;/=/&lt; 1$  according as $364 &gt;/+/&lt; n.$
<p></p>
So the function is maximised at $n=364$  and $365.$
</div></p>

</p>

<p></p>
::<p>
<b>EXERCISE 28:</b>&nbsp;(Hard)<center>
<table width="100%">
<tr>
<th><img width="" src="image/most40.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p><a
href="javascript:hideShow('lab22')"><b>[Hint]</b></a><div
class="ans" id="lab22">
Let $X=$ number of cards required to be turned. 
<p></p>
Then $P(X=k)=\frac{4\times {}^{48}P_{k-1}(52-k)!}{52!}.$
<p></p>
So you can now find $E(X) = \sum_{k=1}^{49} k P(X=k)$. 
<p></p>
But here is a smarter solution (taken from Mosteller's book and also given by a student via Whatsapp):
<center>
<table width="100%">
<tr>
<th><img width="" src="image/mostsol.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>

<p></p>
</div></p>

</p>

<p></p>
<hr xmlns="http://www.w3.org/1999/xhtml"/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/></body></html>
