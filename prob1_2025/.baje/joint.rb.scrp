<LI>if all but finitely many terms in the sum are positive, we
define <M>E(h(X))=\infty.</M></LI>
<LI>if all but finitely many terms in the sum are negative, we
define <M>E(h(X))=-\infty.</M></LI>
<LI>if there are infinitely many positive and negative terms,
then <M>E(h(X))</M> is undefined.</LI>
</OL>
</LI>
</UL>
-------EOD-------

-------EOD-------
then <M>E(h(X))</M> is defined component by component, and is said
to exists finitely iff all the component expectations exist finitely.
<P/>
-------EOD-------
<THM name="The law of the lazy statistician">
Let <M>X</M> be an <M>\rr^n</M>-valued discrete random
variable with PMF <M>p(x)</M>. Let <M>f:\rr^n\to \rr</M> be any
function. Then <M>E(h(X))</M> may be computed as follows.
<OL>
<LI>If <M>\sum_x |h(x)| p(x) < \infty,</M> then 
<D lab="*">E(h(X)) = \sum_x h(x) p(x).</D>
</LI>
<LI>If <M>h(x)</M> takes only nonnegative (or only nonpositive) values, then (*) holds even if the series divereges.</LI>
</OL>
</THM>
-------EOD-------
This result leads to simple trick that we discuss next.
-------EOD-------

-------EOD-------
