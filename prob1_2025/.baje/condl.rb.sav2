@{<NOTE><TITLE>Conditional distribution</TITLE>
<M>\newcommand{\ind}{\mathbf I}</M>
<HEAD1>Conditional distribution</HEAD1>
<DEFN name="Conditional distribution">
Let <M>X:\Omega\to S</M> and <M>Y:\Omega\to T</M> be joint distributed discrete random
variables. Let <M>x\in S</M> be some constant such
that <M>P(X=x)> 0.</M> Then the <B>conditional
distribution of <M>Y</M> given <M>X=x</M></B> is the probability
distribution on <M>T</M> 
<D>
A\mapsto P(Y\in A | X = x).
</D>
</DEFN>

<EXM>
A 7-segment display shows any number from 0 to 9 at random (equal
probabilities). 
<CIMG web="7seg.png"/>
Let <M>X</M> be the indicator random variable of
whether the blue segment is on. Similarly, <M>Y</M> is the
indicator for the red segment. Find the conditional distribution
of <M>Y</M> given <M>X.</M>
<SOLN/>
Here <M>X,Y</M> both take values in <M>\{0,1\}.</M> 
We need to find <M>P(Y=y | X=x)</M> for <M>x,y\in\{0,1\}.</M>
<P/>
Now <M>P(Y=1|X=1) = P(X=1,Y=1)/P(X=1).</M> 
<P/>
Both the blue and the red segments are on in only the numbers
3,4,5,6,8,9. So <M>P(X=1,Y=1) = [[6][10]].</M>
<P/>
The blue segment is on in the numbers 2,3,4,5,6,8,9. So <M>P(X=1) =[[7][10]].</M>
<P/>
Hence <M>P(Y=1|X=1) = P(X=1,Y=1)/P(X=1) = [[67]].</M>
<P/>
You should now be able to work out the other three conditional
probabilities similarly.
</EXM>

We can define conditional CDF or conditional PMF in the obvious
way.

<DEFN name="Conditional expectation / variance">
Expectation (or variance) computed baed on a conditional distribution is
called <B>conditional expectation (variance)</B>.
</DEFN>

It is important to understand that the conditional
expectation/variance is a random variable, which is a function of
the conditioning random variable.

<HEAD2>Unconditionals in terms of conditionals</HEAD2>
Arriving at the unconditional distribution from the conditional distribution is easy. There are different ways, all of which
 depend on these two facts:
<UL><LI><M>P(A\cap B) = P(B)P(A|B)</M>,</LI><LI>
the theorem of total probability: 
<D>
P(A) = P(B) P(A|B) + P(B^c)P(A|B^c),
</D></LI></UL>
In both these facts we have expressed unconditional probabilities in terms of conditional ones. 

The following two exercises have simple solutions, but are conceptually very important. 

::<EXR>(Easy)
Let <M>(X,Y)</M>  be discrete with <M>X</M>  having support <M>\{x_1,x_2,...,x_m\}</M>  and <M>Y</M>  having support
<M>\{y_1,y_2,...,y_n\}</M>.

 For <M>B\seq \rr</M>, let <M>f(x_i, B) = P(Y\in B | X=x_i)</M>. Then for <M>A\seq \rr</M>  show that
<D>P(X\in A,\, Y\in B) = E(#(f(X,B)\ind_{\{X\in A\}})#).</D>
Here <M>\ind_{\{X\in A\}}</M>  is the indicator variable for the event <M>\{X\in A\}</M>.

The expectation in the RHS is <M>E(#(f(X,B)\ind_{\{X\in A\}})#) =  \sum_{i:x_i\in A} f(x_i, B)P(X=x_i)</M>.
<ANS>Directly from the theorem of total probability.</ANS></EXR>

What may not be readily obvious is that the converse of the exercise is also true. This is given in
 the following exercise.

::<EXR>(Easy) Let <M>(X,Y)</M>  be as in the last exercise. For <M>B\seq\rr</M>, let  <M>g(x_i, B)</M>  be such
 that for every <M>A\seq \rr</M>  we have 
<D>P(X\in A,\, Y\in B) = E(#(g(X, B)\ind_{\{X\in A\}})#).</D>
Then show that we must have <M>g(x_i, B) = P(Y\in B|X=x_i)</M>  for <M>B\seq\rr</M>  and <M>i=1,...,m</M>.
<ANS>Take <M>A=\{x_i\}</M>. [Thanks to Samyak for correcting a typo even in this short answer!]</ANS></EXR>
The above exercise will prove very important later, since it provides a definition of <M>P(Y\in
 B|X=x_i)</M>  free of explicit division by <M>P(X=x_i)</M>. This  freedom will later allow us to defined conditional distribution
 of <M>Y</M>  given <M>X</M>   when <M>X</M>  is continuous. 

We can do similar things with conditional
expectation/variance also. 

<THM name="Tower property">
<M>E(Y) = E(E(Y|X))</M>  assuming that the expectations all exist finitely. 
</THM>
<PF>(For simple <M>X,Y</M>): 
Let <M>X</M> take values <M>x_1,x_2,...,x_m</M> and <M>Y</M> take
values <M>y_1,y_2,...,y_m</M>. Let the joint PMF of <M>(X,Y)</M> be 
<D>
P(X=x_i~\&~Y=y_j) = p_{ij}.
</D>
Then <M>P(Y=y_j | X=x_i) = [[p_{ij}][p_{i\bullet}]].</M>
<P/>
So <M>E(Y|X=x_i) = \sum_j y_j [[p_{ij}][p_{i\bullet}]].</M>
<P/>
Expectation of this is 
<D>
\sum_i E(Y|X=x_i) p_{i\bullet} = \sum_i \sum_j y_j
[[p_{ij}][p_{i\bullet}]]p_{i\bullet} = \sum_i \sum_j y_j p_{ij} =
\sum_j y_j  \sum_i p_{ij} = \sum_j y_j   p_{\bullet j} = E(Y),
</D>
as required.

</PF>
The proof uses little more than rearranginf terms in a sum using associativity and commutativity
 of addition. If <M>(X,Y)</M> 
 could take countably infinitely
 many values, then also
 the same proof would have worked, because all the infinite series involved would have been absolutely
 convergent (thanks to the assumption
 of finite existence of the expectations involved). 

Many expectation problems can be handled step-by-step using this
result. Here are some examples.

<EXM>
A casino has two gambling games:
<OL>
<LI>Roll a fair die, and win Rs. <M>D</M> if <M>D</M> is the
outcome. </LI>
<LI>Roll two fair dice, and win Rs 5 if both show the same
number, but lose Rs 5 otherwise.</LI>
</OL>
You throw a coin with <M>P(Head)=[[13]]</M> and decide to play game
1 if <M>Head,</M> and game 2 if <M>Tail.</M> What is your
expected gain?
<SOLN/>
Let <M>X</M> be your gain (in Rs), and let <M>Y</M> be the outcome of the
toss. 
<P/>
Then <M>E(X|Y=Head) = 3.5</M> and <M>E(X|Y=Tail) = 5\times[[6-30][36]]=-[[10][3]].</M> 
<P/>
So, by the tower property, <M>E(X) = E(X|Y=Head)\times P(Y=Head)+E(X|Y=Tail)\times P(Y=Tail) = \cdots.</M>

[Thanks to Arindrajit for correcting a typo here.]
</EXM>

The tower property is very useful for computing expectations
involving a random number of random variables. Here is an
example.

<EXM>
A random number <M>N</M> of customers enter a shop in a
day, where <M>N</M> takes values in <M>\{1,...,100\}</M> with
equal probabilities. The <M>i</M>-th customer pays a random amount <M>X_i</M>,
where <M>X_i</M> takes values in <M>\{1,2,...,10+i\}</M>
ith equal probabilities. Assuming that <M>N,X_1,...,X_N</M> are
all independent, find the total expected payments by the
customers on that day.
<SOLN/>
We have <M>E(X_i) = [[11+i][2]].</M> 
<P/>
So <M>E(*(\sum_1^N X_i|N)*) = \sum_1^N E(X_i|N) = \sum_1^N E(X_i) = \sum_1^N [[11+i][2]] = 5.5N+[[N(N+1)][4]].</M>
<P/>
By tower property, the required answer is <M>E(*(5.5N+[[N(N+1)][4]])*)=\cdots.</M>
</EXM>

<EXM>
10 holes, numbered 1 to 10, in a row. 5 balls are dropped
randomly in them (a hole may contain any number of balls). Call a
ball "lonely" if there is no other ball in its hole or the
adjacent holes. Find the expected number of lonely balls. 
<SOLN/>
Define the indicators <M>I_1,...,I_5</M> as
<D>
I_i = <CASES>1<IF>i\mbox{-th ball is lonely}</IF>0<ELSE/></CASES>
</D>
Then the total number of lonely balls is <M>X = \sum I_i.</M>
<P/>
So we are to find <M>E(X) = \sum E(I_i).</M>
<P/>
Let <M>Y_i = </M> the hole where the <M>i</M>-th ball has fallen.

Then <M>E(I_i|Y_i=1)</M> is the conditional probability that
all the balls except the <M>i</M>-th one has landed in
holes <M>3,...,10</M> given that the <M>i</M>-th ball has landed
in hole 1.

[Thanks to Nuhad for correcting a typo in the line above.]

You should be able to compute this easily. Similarly, you can
compute <M>E(I_i|Y_i=k)</M> for <M>k=1,...,10.</M>

Notice that <M>Y_i</M> can take values <M>1,...,10</M> with equal probabilities.
<P/>
So tower property should provide the answer as
<D>
E(X) = \sum E(E(I_i|Y_i)) = \cdots.
</D>
</EXM>

<THM>
<M>V(Y) = E(V(Y|X)) + V(E(Y|X)).</M>
</THM>
<PF>
This follows directly from the tower property. 
<P/>
We know
<D>
V(Y|X) = E(Y^2|X) - E^2(Y|X),
</D>
and hence
<D>
E(V(Y|X)) = E(E(Y^2|X)) - E(E^2(Y|X)) = E(Y^2) -  E(E^2(Y|X)).
</D>
Again, 
<D>
V(E(Y|X)) = E(E^2(Y|X)) - E^2(E(Y|X)) = E(E^2(Y|X)) - E^2(Y). 
</D>
So 
<D>
E(V(Y|X)) + V(E(Y|X)) = E(Y^2)-E^2(Y) = V(Y),
</D>
as required.
</PF>

<HEAD2>More than 2 variables</HEAD2>
If <M>X,Y,Z</M> are jointly distributed random variables, then we
can talk about conditional distribution of <M>Z</M>
given <M>(X,Y)</M> or <M>X</M> given <M>Z</M> or <M>(X,Z)</M>
given <M>Y,</M> etc. We can even condition step by step. For
example, we can talk about <M>E(E(Z|X,Y)|X).</M> This is a
function of <M>X</M> alone. 

<HEAD2>Substitution</HEAD2>
<THM name="Substition property">
Conditional distribution of <M>f(X,Y)</M> given <M>X=x</M> is the
same as the conditional distribution of <M>f(x,Y)</M> given <M>X=x.</M>
</THM>
<PF>
This follows immediately from the definition of conditional probability.
</PF>
<HEAD1>Problems for practice</HEAD1>

::<EXR>(Easy) <EIMG web="condist1.png"></EIMG>

Here the word "density" is used to mean "PMF". 
<ANS>
(a) Once you realise that <M>f_X(x) = P(X=x)</M>, <M>f_Y(y) = P(Y=y)</M>  and 
<M>f_{Y|X}(y|x) = P(Y=y|X=x),</M>  the given equality is just theorem of total probability. 

(b) The RHS is <M>E(E(Y|X))</M>  and so the equality is just the tower property.
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="condist2.png"></EIMG>
<ANS>
<M>E(S_N) = E(E(S_N|N)) = E(N\mu) = \mu E(N).</M>
<D>E(S_N^2) = E(E(S_N^2|N)) = E(N\sigma^2 + N^2\mu^2  ) = \sigma^2E(N^2)+\mu^2E(N^2).</D>

The third equality follows directly from these two.
</ANS>
<HR/></EXR>
::<EXR>(Easy) <EIMG web="condist3.png"></EIMG>
<ANS>
(a) <M>[[23]].</M>

(b) <M>[[29]].</M>

(c) <M>[[13][27]].</M>
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="condist4.png"></EIMG>

You might like to solve (b) first.
<ANS>
(b) <M>P(X=Y) = [[1][N+1]].</M>

(a) <M>P(X< Y) = P(Y < X) </M>  and <M>P(X< Y) + P(Y < X) +P(X=Y)=1.</M>

Hence <M>P(X> Y) = [[12]]\times(*(1-[[1][N+1]])*) = [[N][2(N+1)]].</M>

So <M>P(X\geq Y) = [[1][N+1]]+[[N][2(N+1)]] = [[N+2][2(N+1)]].</M>

[Thanks to Tanmay for pointing out a couple of silly mistakes here.]
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="condist5.png"></EIMG>

Here Exercise 14 means the last exercise.
<HR/>
<ANS>
(a) Let <M>U = \min(X,Y).</M>   Then <M>U</M>  can take values <M>0,...,N.</M>  

<M>P(U=k) = P(U\geq k)-P(U\geq k+1).</M>

Now <M>P(U\geq k) = P(X,Y\geq k) = P(X\geq k)P(Y\geq k) = (*([[N-k+1][N+1]])*)^2.</M>

Similarly, <M>P(U\geq k+1) = (*([[N-k][N+1]])*)^2.</M>

So <M>P(U=k) = [[(N-k)^2-(N-k+1)^2][(N+1)^2]] = ... .</M>

(b) Let <M>T = \max(X,Y).</M>   Then <M>T</M>  can take values <M>0,...,N.</M>  

<M>P(T=k) = P(U\leq k)-P(T\leq k-1).</M>

Now <M>P(T\leq k) = P(X,Y\leq k) = P(X\leq k)P(Y\leq k) = (*([[k+1][N+1]])*)^2.</M>

Similarly, <M>P(T\leq k-1) = (*([[k][N+1]])*)^2.</M>

So <M>P(T=k) = [[(k+1)^2-k^2][(N+1)^2]] = [[2k+1][(N+1)^2]].</M>

(c)   <M>R=|Y-X|</M>  can take values in 0,1,...,<M>N.</M> 

<M>P(R=0) = P(X=Y) = [[1][N+1]].</M>

For <M>k=1,...,N,</M>  we have <M>P(R=k) = P(R=k \& X < Y) + P(R=k \& X=Y) + P(R=k \& X > Y).</M>

Now <M>P(R=k \& X=Y) =0.</M>

Also <M>P(R=k \& X < Y) =P(R=k \& X > Y).</M>

For <M>\{R=k\& X < Y\}</M>  to happen we must have <M>X = 0,...,N-k</M>  and correspondingly <M>Y = k,...,N.</M>  

So <M>P(R=k\& X < Y) = [[N-k+1][(N+1)^2]].</M>

Hence <M>P(R=k) = [[2(N-k+1)][(N+1)^2]].</M>

[Thanks to Nuhad for correcting a couple of typos here.]
</ANS>
</EXR>
::<EXR>(Easy) <EIMG web="condist6.png"></EIMG>
<ANS>
(a) <M>P(X=x) = \sum_y P(X=x,Y=y) = \sum_y g(x)h(y) = g(x)\sum_y h(y).</M>

(b) <M>P(Y=y) = \sum_x P(X=x,Y=y) = \sum_x g(x)h(y) = h(y)\sum_x g(x).</M>

(c) We know that <M>\sum_x\sum_y P(X=x,Y=y) = 1.</M>  Hence <M>\sum_x\sum_y g(x)h(y) = 1,</M>  i.e., <M>\sum_xg(x)\sum_y h(y) = 1.</M>

(d) To show <M>\forall x, y~~P(X=x,Y=y) = P(X=x)P(Y=y).</M>

Take any <M>x,y.</M>

Then <M>P(X=x)P(Y=y) = [#[\sum_y h(y) ]#]g(x)[#[\sum_x g(x) ]#]h(y) = g(x)h(y) = P(X=x,Y=y).</M>
</ANS>
<HR/></EXR>
::<EXR>(Easy) <EIMG web="condist7.png"></EIMG>
Here "density" means "PMF". 
<ANS>
(a) <M>(X_1,...,X_r)</M>  can take values <M>(x_1,...,x_r)</M>  where each <M>x_i</M>  is a nonnegative integer and <M>\sum_1^r x_i = 2r.</M> 
 
We consider the random experiment of dropping the balls one by one into the boxes. For each ball have <M>r </M> posible destinations.
 
So <M>|\Omega| = r^{2r}.</M>

Now fix some <M>(x_1,...,x_r)</M>  as above. The event <M>A=\{(X_1,...,X_r) = (x_1,...,x_r)\}</M>  may be obtained as follows.

Pick and order <M>x_i</M>  balls to drop into box <M>i</M> one by one. 

So <M>|A| = [[(2r)!][x_1!\times\cdots\times x_r!]].</M>

Hence 
<M>P\{(X_1,...,X_r) = (x_1,...,x_r)\}= [[ |A| ][ |\Omega| ]].</M>

(b) Let <M>B</M>  be the event in question. Then we can compute <M>|B|</M>  as follows. 
<UL>
<LI><B>Step 1:</B>  Choose two balls to go into box 1: <M>\binom{2r}{2}</M>  ways.</LI>
<LI><B>Step 2:</B>  Choose two other balls to go into box 2: <M>\binom{2r-2}{2}</M>  ways.</LI>
<LI>...</LI>
<LI><B>Step 1:</B>  Choose the last two balls to go into box r: <M>1</M>  way.</LI>
</UL> 
So <M>|B| = [[(2r)!][2^r]]</M>. 

Hence <M>P(B) = [[|B|][|\Omega|]] = [[(2r)!][(2r^2)^r]]</M>.
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="condist8.png"></EIMG>
<ANS>
(a) <M>P(X_1+X_2=k) = \binom{n}{k} (p_1+p_2)^k p_3^{n-k}</M>  for <M>k=0,1,...,n.</M>

(b) 
<MULTILINE>
P(X_2=y|X_1+X_2 = z)
& = & [[P(X_2=y \& X_1+X_2=z)][P(X_1+X_2=z)]] \\
& = & [[P(X_1=z-y\&X_2=y)][P(X_1+X_2=z)]] \\
&  = & [[ [[n!][(z-y)!y!(n-z)!]] p_1^{z-y} p_2^y p_3^{n-z} ][ \binom{n}{z} (p_1+p_2)^z p_3^{n-z} ]]
& = & \cdots.
</MULTILINE>
</ANS>
<HR/></EXR>
::<EXR>(Easy) <EIMG web="condist9.png"></EIMG>
<ANS>
(a) <M>1-(*([[5][6]])*)^6.</M>

(b) For <M>n</M>  rolls <M>P(</M> at least one 6<M>)=1-(*([[56]])*)^n.</M>

We need <M>n</M>  such that <M>1-(*([[56]])*)^n\geq [[12]].</M>  

Direct computation shows <M>n\geq 4.</M>
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="condist10.png"></EIMG>

Imagine this set up: A coin with <M>P(H)=p</M>  is repeadly tossed. Success means <M>H.</M>
<ANS>
<M>(1-p)^{x_r-r} p^r.</M>

Thanks to Amit Prakash Jena for correcting a mistake here.
</ANS>

<HR/></EXR>
::<EXR>(Medium) <EIMG web="condist11.png"></EIMG>

This is a continuation of the last problem.
<ANS>
<M>P(T_1=x|N_n=1) = [[P(T_1=x\& N_n=1)][P(N_n=1)]] = [[p(1-p)^{n-1}][np(1-p)^{n-1}}] = [[1n]].</M>
</ANS>
<HR/></EXR>
::<EXR>(Hard) <EIMG web="condist12.png"></EIMG>

This is a continuation of the last problem.
<ANS>
Same logic as in the last solution.
</ANS><HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond1.png"></EIMG>
<ANS>
By symmetry, the answer is <M>[[1n]]</M>  if <M>k=1.</M>  So, for general <M>k</M>  the answer is <M>[[kn]].</M>
</ANS>
<HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond2.png"></EIMG>
Here you should use the fact that for continuous joint distribition <M>P(X_i=X_j)=0</M>  for <M>i\neq j</M>. Thus, you may
 assume that all the <M>X_j</M>'s are distinct. 
<ANS>Let <M>I_j</M> be the indicator variable for whether there is a
record at position <M>j.</M> Let <M>R_j</M>  be the rank of <M>X_j</M>. The smallest of the
 <M>X_j</M>'s has rank <M>1</M>, the largest has rank <M>n</M>. Then
 <M>P(I_j=1)</M> may be computed
by total probability:
<D>
P(I_j=1) = \sum_{k=j}^n P(R_j=k)P(I_j=1|R_j=k).
</D>
Similarly for <M>P(I_jI_k=1).</M>

[Thanks to Nuhad for pointing out a serious mistake here.]
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="morecond3.png"></EIMG>
<ANS>(a) <M>\mu=\sum P_i</M>.

(b) The problem is basically minimising <M>\sum P_i^2</M> subject
to <M>\sum P_i=\mu</M> being fixed. Cauchy-Scwartz might help.  

(c) Here we have to maximise <M>\sum P_i^2</M>  subject to <M>\sum P_i=\mu.</M>  First notice that
 in any maximising <M>(P_1,...,P_n)</M>  al but at most one <M>P_i</M>  must be either <M>0</M> 
 or <M>1</M>. Because if  <M>P_i,P_j\in(0,1)</M>  for <M>i\neq j</M>, then take the smaller one
 closer to zero and the larger one closer to 1 by the same amount. Then <M>\sum P_i</M>  remains unaltered, but <M>\sum P_i^2</M> 
 increases. 

So the only canidates are where there are exactly <M>[\mu]</M>  many <M>P_i</M>'s equal to <M>1</M>, and (if <M>\mu</M> 
 is not an integer) exactly one <M>P_i</M>  equal to <M>\{\mu\}</M>, the fractional part of
 <M>\mu</M>, and the rest equal to 0. 
For example, if <M>n=5</M>  and <M>\mu=2.3</M>, then one candidate is <M>(1,1,0.3,0,0)</M>. 

Clearly,
 for any such candidate the value of <M>\sum P_i^2</M>  is the same <M>([\mu]+\{\mu\}^2)</M>. Also since we are maximising
 a continuous function over a compact set, maximum exists. So these must be the maximising choices. 

[Thanks to Samyak for providing the answer to
</ANS>
<HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond4.png"></EIMG>
<ANS>
Let the black balls be labelled <M>b_1,...,b_m.</M>  

Let <M>X_i=<CASES>1<IF>\mbox{no white drawn before }b_i</IF>
0<ELSE/></CASES>.</M>

Then <M>X= 1+\sum_1^m X_i.</M>

Also, <M>E(X_i) = [[1][n+1]]</M>. To see this consider the <M>n</M>  white balls plus <M>b_i.</M>  Out of these <M>n+1</M> 
 balls <M>b_i</M>  has the chance <M>[[1][n+1]]</M>  to come first. 

(a) <M>V(X_i) = [[n][(n+1)^2]].</M>  

Also for <M>i\neq j</M>  we have <M>E(X_iX_j) = [[2][(n+2)(n+1)]]</M>    (because out of the <M>n</M>  white balls plus <M>b_i</M> 
 and <M>b_j</M>  any of the <M>\binom{n+2}{2}</M>  pairs can come first with equal probability).

(b) Let <M>Y_i</M>  be as given in the hint. Let's take an example to 
understand how <M>Y_i</M>'s are defined. Suppose that we have <M>m=n=2.</M>
Here is a list of all possible ways these may turn up:
<PRE>
W W B B
W B W B
W B B W
B W B W
B B W W
B W W B
</PRE>
All these are equally likely as the draws are random and do not care about the colours. 
Now consider the events <M>\{Y_1=1\}</M>  and <M>\{Y_2=1\}</M>. These are 
<D>\{Y_1 = 1\} = \{BWBW, BWWB\}\mbox{ and } \{Y_2 = 1\} = \{WBWB, BWBW\}.</D>
As both these have the same size, they also have the same probability. Similarly,
<D>\{Y_1 = 0\} = \{WWBB, WBWB, WBBW\}\mbox{ and } \{Y_2 = 0\} = \{WWBB, BWWB, BBWW\}</D>
also have the same size, and hence the same probability.

We claim that for any values of <M>m</M>  and <M>n</M>  and any <M>i< j</M> and any <M>k</M>,   the events
 <M>\{Y_i=k\}</M>  and <M>\{Y_j=k\}</M> 
 must have the same size (and hence the same probability). 

We show this by demontraiting a bijection <M>f:\{Y_i=k\}\to\{Y_j=k\}</M>  defined as <M>f(\omega)
 = </M>  the sequence of <M>W</M>'s and <M>B</M>'s obtained by swapping the <M>i</M>-th <M>W</M> 
 and its preceding <M>B</M>'s with the <M>j</M>-th <M>W</M>  and its preceding <M>B</M>'s. For
 instance, if <M>m=10, n=5, i=1, j=4, k=2</M>
 then here is an <M>\omega\in\{Y_i=k\}:</M>
<Q><RED>BBW</RED>BWBBW<BLUE>BBBW</BLUE>BBW</Q>
Then <M>f(\omega)</M>  is 
<Q><BLUE>BBBW</BLUE>BWBBW<RED>BBW</RED>BBW</Q>
All swaps are one-one (repeating a swap restores original order). Also <M>\omega\in\{Y_i=k\}\iff
 f(\omega)\in\{Y_j=k\}</M>. Hence <M>|\{Y_i=k\}|=|\{Y_j=k\}|</M>, as required. 
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="morecond5.png"></EIMG>
<ANS>
Let <M>T = \lambda X_1+ (1-\lambda) X_2.</M>  

Then <M>V(T) = \lambda^2 V(X_1) + (1-\lambda)^2 V(X_2),</M>  since <M>X_1,X_2</M>  are independent.

Thus, <M>V(T) = \lambda^2 \sigma_1^2 + (1-\lambda)^2 \sigma_2^2 = f(\lambda),</M>  say. 

Then <M>f'(\lambda) = 2 \sigma^2_1 \lambda - 2 \sigma^2_2(1-\lambda).</M>

Solving <M>f'(\lambda) = 0</M>  we get <M>\lambda = [[\sigma^2_2][\sigma^2_1+\sigma^2_2]].</M>

This is desirable because we are giving more weight to the <M>X_i</M>  that has less variance (i.e., is more stable). 
</ANS>
<HR/></EXR>
::<EXR>(Easy) <EIMG web="morecond6.png"></EIMG>
<ANS>
Just like <M>(a+b)(a-b) = a^2-b^2.</M>  
</ANS>
<HR/></EXR>
::<EXR>(Easy) <EIMG web="morecond7.png"></EIMG>

Do this only for discrete <M>X.</M>
<ANS>
<M>E(X|Y=y) = \sum_x x P(X=x|Y=y) = \sum_x x P(X=x),</M>
since <M>X,Y</M>  independent. 

Hence the result.
</ANS>
<HR/></EXR>
::<EXR>(Easy) <EIMG web="morecond8.png"></EIMG>

Do this for discrete <M>X, Y</M>  only. If <M>X</M>  can take values <M>x_1,x_2,x_3,...</M>  with
 positive probabilities, then
 you are prove
<D>\forall i~~E(g(X)Y|X=x_i] = g(x_i)E(Y|X=x_i).</D>
<ANS>
Take any <M>i.</M>

Then <M>E(g(X)Y|X=x_i) = \sum_y g(x_i) y P(Y=y|X=x_i) = g(x_i) \sum_y y P(Y=y|X=x_i) = g(x_i) E(Y|X=x_i),</M>  as required.
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="morecond9.png"></EIMG><HR/></EXR>
::<EXR>(Medium) <EIMG web="morecond10.png"></EIMG><ANS>
<MULTILINE>cov(X,E(Y|X)) 
& = & E(X\, E(Y|X))-E(X)E(E(Y|X))\\
& = & E(E(XY|X))-E(X)E(E(Y|X))<SINCE>substitution</SINCE> \\
& = & E(XY)-E(X)E(Y)<SINCE>tower</SINCE> \\
& = & cov(X,Y),
  </MULTILINE>
as required.
</ANS><HR/></EXR>
::<EXR>(Medium) <EIMG web="morecond11.png"></EIMG>

Will the result hold in general if the <M>X_i</M>'s are not independent?
<ANS>No, the result may not hold if the <M>X_i</M>'s have a dependence structure that is
 asymetric. A counterexample is as follows. 

<M>X_1 = </M>  outcome of a roll of a fair die. <M>X_2</M>  is obtained from <M>X_1</M>  by
 swapping 1 and 2. <M>X_3</M>  is obtained from <M>X_1</M>  by swapping 1 and 3. If the definitions
 of <M>X_2,X_3</M>  are not clear, then see their values below for all the possible values of <M>X_1:</M>
<TABLE>
<TR><TD><M>X_1</M></TD><TD><M>X_2</M></TD><TD><M>X_3</M></TD></TR>
<TR><TD>1</TD><TD>2</TD><TD>3</TD></TR>
<TR><TD>2</TD><TD>1</TD><TD>same as <M>X_1</M></TD></TR>
<TR><TD>3</TD><TD>same as <M>X_1</M></TD><TD>1</TD></TR>
<TR><TD>4</TD><TD>same as <M>X_1</M></TD><TD>same as <M>X_1</M></TD></TR>
<TR><TD>5</TD><TD>same as <M>X_1</M></TD><TD>same as <M>X_1</M></TD></TR>
<TR><TD>6</TD><TD>same as <M>X_1</M></TD><TD>same as <M>X_1</M></TD></TR>
</TABLE>
Then
 <M>E(X_1|X_1+X_2+X_3=6)=1\neq [[63]].</M> 

Here <M>(X_1,X_2,X_3)</M>  are jointly distributed with identical marginal distributions. But that does not mean that the
 joint distribution of <M>(X_1,X_2,X_3)</M>  and <M>(X_2,X_1,X_3)</M>  is the same. So in
 particular, <M>E(X_1|X_1+X_2+X_3)</M>  need not equal <M>E(X_2|X_1+X_2+X_3)</M>. 

But in the original problem the <M>X_i</M>'s were also given to be independent. So there indeed for any permutation <M>\pi</M> 
 of <M>(1,2,...,n)</M>  the joint distribution of <M>(X_1,...,X_n)</M>  was the same as that of <M>(X_{\pi(1)},...,X_{\pi(n)})</M>.
 So <M>E(X_i|X_1+\cdots+X_n)</M>  was free of <M>i</M>.
</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="morecond12.png"></EIMG> 

Here the random variable of interest is the number of white balls <I>remaining in the urn</I> 
 after stage <M>t</M>. In particular, for <M>=0</M>, this numer is <M>w</M>.
<ANS>Use induction on <M>t</M>. Basically, solve the problem only for <M>t=1</M>, and use the
 argument for the induction step.</ANS>
<HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond13.png"></EIMG><ANS>
Let's take an example with <M>r=4</M>. Consider the tosses until  the first tail: <M>T, HT, HHT, HHHT</M>  or <M>HHHH\cdots T</M> 
 (i.e., the first tail occurs after toss 4).  

Let us encode these cases using a random variable <M>K</M>  defined as 
<D>K = <CASES>i <IF>i\mbox{th toss gives first tail and } i\leq 4</IF> 5<ELSE/></CASES></D>
 In the last case, <M>X = 4</M>. 

In the other cases the search starts all over again after the first tail. 

So, for <M>i=1,2,3,4</M>,  we have  <M>E(X|K=i) = i+E(X)</M>.

Now use tower property.
</ANS><HR/></EXR>
::<EXR>(Easy) <EIMG web="morecond14.png"></EIMG>
<ANS>Let <M>Y = <CASES>0<IF>X<a</IF> 1<ELSE/></CASES>.</M>  Now use the tower property.</ANS>
<HR/></EXR>
::<EXR>(Medium) <EIMG web="morecond15.png"></EIMG><ANS>
Let <M>X</M>  be the number of balls remaining at the end. Let <M>Y</M>  be the indicator variable that the the first selected
 ball is white. Then <M>E(X) = M_{a,b}</M>  (given)  and <M>E(X|Y=1) = M_{a-1,b}</M>  and <M>E(X|Y=0) = M_{a,b-1}</M>. 

Now use tower property to get a recursion.  
</ANS><HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond16.png"></EIMG><ANS>
You may expand the expectation as a quadratic expression in <M>a,b,c</M>  and partially differentiate it wrt <M>a,b,c</M> 
 and equate to zero to get a set of equations. Then you need to justify that this indeed corresponds to the global minimum.
 
Or, you may use linear algebra as follows.

Consider the space <M>span\{1,X_1,X_2\}</M>. We are trying to find the vector in it that is the closest to <M>Y</M>  where
 the distance between two random variables <M>W,Z</M>  is being measured by <M>E(W-Z)^2</M>. 

The minimiser must be the foot <M>\hat Y = a+bX_1+cX_2</M>  of the perpendicular dropped from
 <M>Y</M>  onto <M>span\{1,X_1,X_2\}</M>. 

So we need <M>Y-\hat Y</M>  to be orthogonal to <M>1, X_1</M>  and <M>X_2</M>, i.e., <M>E(1(Y-\hat Y) = 0</M>  and <M>E(X_1(Y-\hat Y) = 0</M> 
 and <M>E(X_2(Y-\hat Y) = 0</M>. This gives rise to the <M>3\times 3</M>  system
<D><MAT>1 & E(X_1) & E(X_2)\\E(X_1) & E(X_1^2) & E(X_1X_2)\\E(X_2) & E(X_1X_2) & E(X_2^2)</MAT><MAT>a\\b\\c</MAT> =<MAT>E(Y)\\E(X_1Y)\\E(X_2Y)</MAT>.</D>

</ANS><HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond17.png"></EIMG><ANS>
Similar to the last exercise.
</ANS><HR/></EXR>
::<EXR>(Medium) <EIMG web="morecond18.png"></EIMG><ANS>
(a) Tower property.

(b) Induction.

(c) Use the theorem for computing unconditional variance from conditional quantities.

(d) Induction.
</ANS><HR/></EXR>

::<EXR>(Medium) <EIMG web="morecond19.png"></EIMG><ANS>Use indicator variables.</ANS><HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond20.png"></EIMG>

Assume that the die is fair.
<ANS>
Condition on the number of times you have rolled the die. If you have rolled it <M>k</M> times, then the <M>k</M>-th outcome
 could not have occured earlier also. 
</ANS>
<HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond21.png"></EIMG>
<ANS>
Let <M>X_i</M>  be the indicator for <M>i</M>-th red ball being a win. 

There are <M>\binom{2n}{n}</M>  sequences of <M>n</M>  R's and <M>n</M>  B's in all. Let us count
 how many of these lead to  <M>\{X_i=1\}.</M> 

Split each such sequence into two parts, the part before the <M>i</M>-th R, and the part after.
 For instance, for <M>n=4</M>  and <M>i=3</M>   the sequence RRBRBRBB is split as <RED>RRB</RED>R<BLUE>BRBB</BLUE>. 

For general <M>n</M>  and <M>i,</M>  the red part must consist of exactly <M>i-1</M>  R's and at
 most <M>i-1</M>  B's. The blue part consists of exactly <M>n-i</M>  R's and the remaining B's. 

Let <M>N_{r,b} = </M>  number of sequences with exactly <M>r</M>  R's and <M>b</M>  B's. In other words, <M>N_{r,b} =\binom{r+b}{r} = \binom{r+b}{b}. </M>

Then, for any sequence in <M>\{X_i=1\}</M>  the red part may be selected in 
<D>\sum_{j=0}^{i-1} \binom{i+j-1}{j}</D>
ways. Here <M>j</M>  denotes the number of B's in the red part. Once we also count the matching number
 of blue parts for each value of <M>j</M>, we get the size of <M>\{X_i=1\}</M>  as
<D>\sum_{j=0}^{i-1} \binom{i+j-1}{j}\binom{2n-i-j}{n-j}.</D>
Now you should be able to complete the rest.     

</ANS>
<HR/></EXR>
::<EXR>(Hard) <EIMG web="morecond22.png"></EIMG>
<ANS>
(a) Let's take an example with <M>n=10</M>  and <M>k=3.</M>  We are showing the selected balls in red:
<Q>
1 <RED>2 3</RED> 4 5 <RED>6</RED> 7 8 9 10
</Q>
Here <M>X = 6</M>  and <M>R = 4.</M>  

You should be able to see directly that in general <M>X+R=n.</M>
</ANS>
<HR/></EXR>
</NOTE>@}
