<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE0592.woff) format("opentype");</style>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {
      Macros: {
        h: ["{\\hat #1}",1],
        b: ["{\\overline #1}", 1],
        row: "{\\mathcal R}",
        col: "{\\mathcal C}",
        nul: "{\\mathcal N}"
      }
    }
  });
</script><script type="text/javascript" src="https://www.isical.ac.in/~arnabc/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head>
<body>
<a href="http://www.isical.ac.in/~arnabc/">[Home]</a>
<h3>Table of contents</h3>
<ul>
<li>
<a href="#Matrix algorithms">Matrix algorithms</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Gauss-Jordan elimination">Gauss-Jordan elimination</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Pivoting">Pivoting</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Inversion using Gauss-Jordan elimination">Inversion using Gauss-Jordan elimination</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Determinant using Gaussian elimination">Determinant using Gaussian elimination</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#$QR$ decomposition">$QR$ decomposition</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Householder transformation">Householder transformation</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Using Householder for $QR$">Using Householder for $QR$</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Efficient implementation">Efficient implementation</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Application to least squares">Application to least squares</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#What if $A$ is not full column rank?">What if $A$ is not full column rank?</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Eigenanalysis: power method">Eigenanalysis: power method</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#When does it work?">When does it work?</a>
</li>
</ul>
<hr/>

$\newcommand{\bx}{{\bf x}}
\newcommand{\by}{{\bf y}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bv}{{\bf v}}
\renewcommand{\bc}{{\bf c}}
\newcommand{\bz}{{\bf 0}}
\newcommand{\ba}{{\bf a}}
\newcommand{\bq}{{\bf q}}
\newcommand{\bp}{{\bf p}}
$

<title>Matrix algorithms</title>

<table width="100%">
<tr>
<td align="right"><i>Last
updated on:
TUE MAY 04 IST 2021</i></td>
</tr>
</table>

<h1><a
name="Matrix algorithms">Matrix algorithms</a></h1>

<h2><a
name="Gauss-Jordan elimination">Gauss-Jordan elimination</a></h2>
We shall start with a few concepts already familiar to you. 

<p>
<b>EXAMPLE:</b>&nbsp;
Consider the following three equations:
$$\begin{eqnarray*}
2x-3y+z&amp; =&amp; 5\\
3x+y+2z&amp; =&amp; 15\\
2x + 3z&amp; =&amp; -2
\end{eqnarray*}$$
This is an example of a <i> system of linear equations.</i>
<img src="../image/box.png"></p>

We can write a system of $m$ linear equations in $n$ unknowns
using matrix notation:
$$
A \bx = \bb,
$$
where $A_{m\times n}$ is called the <i> coefficient matrix,</i>
$\bx_{n\times 1}$ is called the vector of unknowns, and
$\bb_{m\times 1}$ is called the rhs vector.

<p>
<b>EXAMPLE:</b>&nbsp;
The system in the last example can be written as
$$
\left[\begin{array}{ccccccccccc}2&amp; -3&amp; 1\\3&amp; 1&amp; 2\\2&amp; 0&amp; 3
\end{array}\right] \left[\begin{array}{ccccccccccc}x\\y\\z
\end{array}\right] = \left[\begin{array}{ccccccccccc}5\\15\\-2
\end{array}\right].
$$
Thus, here
$$
A = \left[\begin{array}{ccccccccccc}2&amp; -3&amp; 1\\3&amp; 1&amp; 2\\2&amp; 0&amp; 3
\end{array}\right],~~~\bx = \left[\begin{array}{ccccccccccc}x\\y\\z
\end{array}\right],~~~
\bb = \left[\begin{array}{ccccccccccc}5\\15\\-2
\end{array}\right].
$$
<img src="../image/box.png"></p>

 If the rhs vector $\bb$ is $\bz$, then we call the system <b> homogeneous</b>, else we call it <b> non-homogeneous</b>. 

<p></p>

Any value of $\bx$ for which $A\bx = \bb,$ is called a <i>
solution</i>  of the system. A system of linear equations has either exactly
one solution, or infinitely many solutions or no solution at all. In the
last case we call the system <b> inconsistent</b>. Otherwise it is called
<b> consistent</b>.


The next example shows the method for solving a system of linear
equations that we learn at high school.

<p>
<b>EXAMPLE:</b>&nbsp;
Consider the following system of three linear equations, which we call
$\alpha_1,\beta_1$ and $\gamma_1.$


$$\begin{array}{lrrrrrrrrrrrr}
\alpha_1 :~~~&amp; x&amp; -y&amp; +z&amp; =&amp; 2 \\
\beta_1 :~~~&amp; 2x&amp; +5y&amp; -z&amp; =&amp; 9 \\
\gamma_1 :~~~&amp; x&amp; +2y&amp; -3z&amp; =&amp;  -4
\end{array}$$
In high school we used to solve this by eliminating the unknowns one by
one until only one remained. Here we shall do this for all the unknowns simultaneously. 


Let us first eliminate $x$ from the last two equations by subtracting
multiples of the first equation from them. Here are the resulting 3
equations, which we call
$\alpha_2,\beta_2$ and $\gamma_2.$

$$\begin{array}{lrrrrrrrrrrrr}
\alpha_2=&amp; \alpha_1 :~~~&amp; x&amp; -y&amp; +z&amp; =&amp; 2 \\
\beta_2=&amp; \beta_1-2\alpha_1 :~~~&amp;  &amp; 7y&amp; -3z&amp; =&amp; 5 \\
\gamma_2=&amp; \gamma_1-\alpha_1 :~~~&amp;  &amp; 3y&amp; -4z&amp; =&amp;  -6
\end{array}$$ 
We want the coefficient of $y$ in the second equation to be $1:$
$$\begin{array}{lrrrrrrrrrrrr}
\alpha_3=&amp; \alpha_2 :~~~&amp; x&amp; -y &amp; +z&amp; =&amp; 2 \\
\beta_3=&amp; \frac 17\beta_2 :~~~&amp;  &amp; y&amp; -\frac 37z&amp; =&amp; \frac 57 \\
\gamma_3=&amp;  \gamma_2 :~~~&amp;  &amp; 3y &amp; -4z&amp; =&amp;  -6
\end{array}$$
Now let us eliminate $y$ from the all the equations except
the second one:
$$\begin{array}{lrrrrrrrrrrrr}
\alpha_4=&amp; \alpha_3+\beta_2 :~~~&amp; x&amp; &amp; +\frac 47z&amp; =&amp; \frac{19}{7} \\
\beta_4=&amp; \beta_3 :~~~&amp;  &amp; y&amp; -\frac 37z&amp; =&amp; \frac 57 \\
\gamma_4=&amp; \gamma_3-3\beta_2 :~~~&amp;  &amp;  &amp; -\frac{19}{7}z&amp; =&amp;  -\frac{57}{7}
\end{array}$$
Next, we want the coefficient of $z$ in the third equation
to be $1:$
$$\begin{array}{lrrrrrrrrrrrr}
\alpha_5=&amp; \alpha_4 :~~~&amp; x&amp; &amp; +\frac 47z&amp; =&amp; \frac{19}{7} \\
\beta_5=&amp; \beta_4 :~~~&amp;  &amp; y&amp; -\frac 37z&amp; =&amp; \frac 57 \\
\gamma_5=&amp; -\frac{7}{19}\gamma_4 :~~~&amp;  &amp;  &amp; z&amp; =&amp;  3
\end{array}$$
Finally, eliminate $z$ from all but the last equation:
$$\begin{array}{lrrrrrrrrrrrr}
\alpha_6=&amp; \alpha_5-\frac 74\gamma_5 :~~~&amp; x&amp; &amp; &amp; =&amp; 1 \\
\beta_6=&amp; \beta_5+\frac 73 \gamma_5 :~~~&amp;  &amp; y&amp; &amp; =&amp; 2 \\
\gamma_6=&amp; \gamma_5 :~~~&amp;  &amp;  &amp; z&amp; =&amp;  3
\end{array}$$
This gives us the final solution.
<p></p>

<img src="../image/box.png"></p>

This is what is called Gauss-Jordan elimination 
in computational matrix theory. While doing
Gauss-Jordan elimination it is customary to write the system at each step in
the <b> augmented matrix form</b>. This is done in the example below.
 
<p>
<b>EXAMPLE:</b>&nbsp; The augmented matrix form of the given system is as follows.
$$\left[\begin{array}{rrr|r}
1&amp; -1&amp; 1&amp; 2 \\
2&amp; 5&amp; -1&amp; 9 \\
1&amp; 2&amp; -3&amp;  -4
\end{array}\right]$$
It is obtained by appending the rhs after the matrix. We draw a vertical
line to keep the rhs separate. 
<p></p>
Here is a sequence of augmented matrices that we encountered
during the process:
$$
\left[\begin{array}{rrr|r}
\fbox1 &amp; -1 &amp; 1 &amp; 2\\
2 &amp; 5 &amp; -1 &amp; 7\\
1 &amp; 2 &amp; -3 &amp; -4
\end{array}\right]
\stackrel{SP}{\longrightarrow}
\left[\begin{array}{rrr|r}
1 &amp; -1 &amp; 1 &amp; 2\\
0 &amp; \fbox7 &amp; -3 &amp; 5\\
0 &amp; 3 &amp; -4 &amp; -6
\end{array}\right]
\stackrel{M}{\longrightarrow}
\left[\begin{array}{rrr|r}
1 &amp; -1 &amp; 1 &amp; 2\\
0 &amp; \fbox1 &amp; -\frac 37 &amp; \frac 57\\
0 &amp; 3 &amp; -4 &amp; -6
\end{array}\right]
\stackrel{SP}{\longrightarrow}
\left[\begin{array}{rrr|r}
1 &amp; 0 &amp; \frac 47 &amp; 2\\
0 &amp; 1 &amp; -\frac 37 &amp; \frac 57\\
0 &amp; 0 &amp; \fbox{$-\frac{19}{7}$} &amp; -\frac{57}{7}
\end{array}\right]
\stackrel{M}{\longrightarrow}
\left[\begin{array}{rrr|r}
1 &amp; 0 &amp; \frac 47 &amp; 2\\
0 &amp; 1 &amp; -\frac 37 &amp; \frac 57\\
0 &amp; 0 &amp; \fbox1 &amp; 3
\end{array}\right]
\stackrel{SP}{\longrightarrow}
\left[\begin{array}{rrr|r}
1 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 1 &amp; 0 &amp; 2\\
0 &amp; 0 &amp; 1 &amp; 3
\end{array}\right]
$$
Here we have used 3 symbols $S$, $M$
and $P$. Let us understand them. Before each step we
choose an entry in the lhs of the augmented matrix (framed inside rectangles above). This
is called the <b>pivot</b>, and its row and column are called the <b>pivotal
row</b>  and <b>pivotal column</b>. Initially the top left hand entry is chosen as the
pivot.
<ul>
<li>An $M$-step divides the pivotal row by the pivot
(so that the pivot becomes $1$). </li>

<li>An $S$-step subtracts suitable multiples of the pivotal
row from the other rows to make the all the entries in the pivotal
column zero (except the pivot itself).</li>

<li>The $P$ step moves the pivot one step downwards
and to the right.</li>
</ul>

<img src="../image/box.png"></p>




<fieldset>
<legend><b><i>Gauss-Jordan elimination (without pivoting)</i></b></legend>
In Gauss-Jordan elimination of a $n\times n$ system
we start with the pivot at the $(1,1)$-th position. Then we 
perform the following operations.
$$
\underbrace{(M,S,P),\cdots,(M,S,P)}
_{n-1\mbox{ times}},M,S.
$$
</fieldset>
If the current position of the pivot is $(p,p)$, then the
following R code achieves the steps:
<font color="red">
<pre>
M = function() {mat[p,] &lt;&lt;- mat[p,]/mat[p,p]; print(mat)}
S = function(i) { mat[i,] &lt;&lt;- mat[i,] - mat[p,]*mat[i,p]; print(mat)}
P = function() { p &lt;&lt;- p + 1; print(p) }
</pre>
</font>
Here we have used the less known <code><<-</code>
operator. If <code>x</code> is a variable defined outside a
function, and you write <code>x = 5</code> inside the function,
then this assignment has no effect outside the function:
<font color="red">
<pre>
x = 3
f = function() {x = 5}
f()
x
</pre>
</font>
Try these to see that <code>x</code> is still 3. However, if you
use the <code><<-</code> operator, then the assignment is
visible outside the function:
<font color="red">
<pre>
x = 3
f = function() {x &lt;&lt;- 5}
f()
x
</pre>
</font>
Now <code>x</code> is 5.
<p></p>
To try these out let us create a random system:
<font color="red">
<pre>
A = matrix(sample(100, 16, rep=TRUE),4,4)
b = sample(100, 4, rep=T)
</pre>
</font>
The <code>sample</code> function draws an SRSWOR or SRSWR. The
first argument specifies the population, the second is the sample
size. The <code>rep</code> argument specifies whether replacement
is allowed.

<p></p>
Let's make the augmented matrix:
<font color="red">
<pre>
mat = cbind(A,b)
p = 1
</pre>
</font>
Now let's apply our steps:
<font color="red">
<pre>
M()
S(2)
S(3)
S(4)
</pre>
</font>
That finishes the first pass. The second pass starts by updating
the pivot:
<font color="red">
<pre>
P()
M()
S(1)
S(3)
S(4)
</pre>
</font>



<h3><a
name="Pivoting">Pivoting</a></h3>
The $M$- and $S$-steps require the pivot to 
be nonzero. However, in the above algorithm the pivot may become
zero. 
<p>
<b>EXAMPLE:</b>&nbsp;
If the system is 
$$\begin{array}{lrrrrrrrrrrrr}
 &amp; 3 y &amp; - 5z &amp; = &amp; -1\\
4x &amp; + y &amp; + z &amp; = &amp; 6\\
-x &amp; - 8y &amp; - z &amp; = &amp; -4
\end{array}$$
then the $(1,1)$-th entry is 0. 

<p></p>

Yet it is hardly a problem, because we just have to use some
other equation to eliminate $x$ with. For this we may choose
just any equation that has $x$ in it (<i>i.e.</i>, the coefficient
of $x$ is nonzero). For example, we may choose
the $3$rd equation:
equations to rewrite the system as
$$\begin{array}{lrrrrrrrrrrrr}
 &amp; 3 y &amp; - 5z &amp; = &amp; -1\\
4x &amp; + y &amp; + z &amp; = &amp; 6\\
\fbox{$-x$} &amp; - 8y &amp; - z &amp; = &amp; -4
\end{array}$$
Or we could have eliminated some other variable instead
of $x$ using the first equation:
$$\begin{array}{lrrrrrrrrrrrr}
 &amp; \fbox{$3 y$} &amp; - 5z &amp; = &amp; -1\\
4x &amp; + y &amp; + z &amp; = &amp; 6\\
-x &amp; - 8y &amp; - z &amp; = &amp; -4
\end{array}$$
<img src="../image/box.png"></p>
These are called <b>pivoting</b>.
It is useful
even when the pivot is nonzero, but is very small in absolute value. 
This is because
division by numbers near zero in a computer introduces large errors in the output. 


<p></p>

In terms of augmented matrices, pivoting means choosing the pivot
position appropriately at the start of each step. There are just
two rules to keep in mind: 
<ul>

<li>
the pivot must not be zero (<i>i.e.</i>, if
you plan to eliminate a variable using some equation, then that
variable should be present in that equation!).</li>

<li>if you have already used the $(i,j)$-th entry as the
pivot in some step, then no future pivot should be from
the $i$-th row or the $j$-th column. In other words,
never eliminate the same variable twice, and never use the same
equation to eliminate multiple variables (else, the first
variable may come back!).</li>

</ul>
Also we follow another <i>desirable</i> property:

<ul>

<li>Among all available pivot positions (<i>i.e.</i>, all $(i,j)$'s where
the $i$-th row and the $j$-th column are still
unused), choose the pivot as the entry farthest from $0.$</li>

</ul>
Most textbooks present pivoting in terms of swapping rows and
 columns of the augmented matrix. For example, if at the very
 first step you'd like to use the $(2,3)$-th entry as the
 pivot, then they would say: swap rows 1 and 2, and swap columns
 1 and 3. Thus, for them the pivot position at the $k$-th
 step is always $(k,k).$ If you want anything else at that
 position, you manually swap the rows and columns to bring it at
 the $(k,k)$-th position.

<p></p>

 In our example we
swap row 2 with row 3, and column 2 with column 3 to get
$$
\left[\begin{array}{rrr|r}
\fbox0 &amp;  3 &amp;  -5 &amp;  -1\\
4 &amp;  1 &amp;  1 &amp;  6\\
-1 &amp;  -8 &amp;  -1 &amp;  -4
\end{array}\right]
\stackrel{P}{\longrightarrow}
\left[\begin{array}{rrr|r}
\fbox{-8} &amp;  -1 &amp;  -1 &amp;  -4\\
1 &amp;  4 &amp;  1 &amp;  6\\
3 &amp;  0 &amp;  -5 &amp;  -1
\end{array}\right]
$$
Here $P$ is our symbol for pivoting.
<p></p>
Recall that the columns of the matrix correspond to the variables of the
equations. So swapping the columns also involves reordering the variables. 
A simple
way to keep track of the order of the variables is to write the variables
above the 
columns. If we call the variables as $x,y,z$ in the last example then
we shall write:

$$
\left[\begin{array}{rrr|r}
x &amp;  y &amp; z\\
\hline
\fbox0 &amp;  3 &amp;  -5 &amp;  -1\\
4 &amp;  1 &amp;  1 &amp;  6\\
-1 &amp;  -8 &amp;  -1 &amp;  -4
\end{array}\right]
\stackrel{P}{\longrightarrow}
\left[\begin{array}{rrr|r}
y &amp;  x &amp; z\\
\hline
\fbox{-8} &amp;  -1 &amp;  -1 &amp;  -4\\
1 &amp;  4 &amp;  1 &amp;  6\\
3 &amp;  0 &amp;  -5 &amp;  -1
\end{array}\right]
$$
To do this in R we just need to change our <code>P</code>
function. 
<font color="red">
<pre>
P = function() {
  p &lt;&lt;- p + 1
  n = nrow(mat)
  candidates = mat[p:n,p:n]
  argmax = which.max(abs(candidates))
  #cat('argmax=',argmax)
  newRow = p + (argmax-1) %% nrow(candidates)
  newCol = p + floor((argmax-1) / nrow(candidates))
  cat('Best candidate at', newRow, newCol,'\n')

<font color="#0000ff"># Now we shall swap the rows p and newRow.</font>
  tmp = mat[newRow,]
  mat[newRow,] &lt;&lt;- mat[p,]
  mat[p,] &lt;&lt;- tmp

<font color="#0000ff"># We also need to swap the row heads accordingly.
# First we extract the current row heads.</font>
  rheads = rownames(mat)
<font color="#0000ff"># Next we perform the swap.</font>
  tmp = rheads[newRow]
  rheads[newRow] = rheads[p]
  rheads[p] = tmp
<font color="#0000ff"># Finally we assign the swapped array of row heads to the matrix.</font>
  rownames(mat) &lt;&lt;- rheads

<font color="#0000ff"># Similarly, for the column swap.</font>
  tmp = mat[,newCol]
  mat[,newCol] &lt;&lt;- mat[,p]
  mat[,p] &lt;&lt;- tmp
  cheads = colnames(mat)
  tmp = cheads[newCol]
  cheads[newCol] = cheads[p]
  cheads[p] = tmp
  colnames(mat) &lt;&lt;- cheads

  print(mat)
}
</pre>
</font>
Of course, we need to keep track of our swaps. For this we shall
employ the ability of R to attach names to each row and column:
<font color="red">
<pre>
rownames(A) = 1:4
colnames(A) = paste('x',1:4,sep='')
A
</pre>
</font>
Now proceed as before:
<font color="red">
<pre>
( mat = cbind(A, b) )
p = 0
</pre>
</font>
Notice we are starting with pivot position 0. 
The only differecen is that we start with a pivoting step.
<font color="red">
<pre>
P()
M()
S(2)
S(3)
S(4)
</pre>
</font>
The next pass is similar:
<font color="red">
<pre>
P()
M()
S(1)
S(3)
S(4)
</pre>
</font>


Here you end up with 
<pre>
0 0 1 | 0.371951
1 0 0 | 1.33537
0 1 0 | 0.286585
</pre>
The left hand side is a permuation matrix, which tells us how to
interpret the right hand side: $z = 0.371951$, $x =
1.33537$ and $y = 0.286585$.

<fieldset>
<legend><b><i>Gauss-Jordan elimination (with pivoting)</i></b></legend>
We perform the following steps:
$$
\underbrace{P,M,S,\cdots,P,M,S}
_{n\mbox{ times}}.
$$
</fieldset>


<h3><a
name="Inversion using Gauss-Jordan elimination">Inversion using Gauss-Jordan elimination</a></h3>
Gauss-Jordan elimination (with pivoting) may  be used to find inverse of a
given nonsingular square matrix, since finding inverse is the same as
solving the system 
$$
AX = I.
$$


<p>
<b>EXAMPLE:</b>&nbsp;
Suppose that we want to find the inverse of the matrix 
$\left[\begin{array}{ccccccccccc}1&amp; -1&amp; 1 \\
2&amp; 5&amp; -1 \\
1&amp; 2&amp; -3
\end{array}\right].$
Then we append an identity matrix of the same size to the right of this
matrix to get the 
augmented matrix 
$$\left[\begin{array}{rrr|rrr}
1&amp; -1&amp; 1&amp; 1 &amp; 0 &amp; 0\\
2&amp; 5&amp; -1&amp; 0 &amp; 1&amp; 0\\
1&amp; 2&amp; -3&amp; 0&amp; 0&amp; 1
\end{array}\right].$$
Now go on applying Gauss-Jordan elimination until the
left hand matrix is reduced to a permuation matrix. The right hand part at the final step
will give the required inverse (after the permutation).
$$
\left[\begin{array}{rrr|rrr}
\fbox1&amp; -1&amp; 1&amp; 1 &amp; 0 &amp; 0\\
2&amp; 5&amp; -1&amp; 0 &amp; 1&amp; 0\\
1&amp; 2&amp; -3&amp; 0&amp; 0&amp; 1
\end{array}\right] 
\stackrel{M,S,P}{\longrightarrow}
\left[\begin{array}{rrr|rrr}
1&amp; -1&amp;  1&amp;  1&amp; 0&amp; 0 \\
0&amp; \fbox7 &amp; -3&amp; -2&amp; 1&amp; 0 \\
0&amp; 3 &amp; -4&amp; -1&amp; 0&amp; 1
\end{array}\right]
\stackrel{M,S,P}{\longrightarrow}
\left[\begin{array}{rrr|rrr}
1&amp; 0&amp;  \frac{4}{7}&amp; \frac{5}{7}&amp; \frac{1}{7}&amp; 0 \\
0&amp; 1&amp; \frac{-3}{7}&amp; \frac{-2}{7}&amp; \frac{1}{7}&amp; 0 \\
0&amp; 0 &amp; \fbox{$\frac{-19}{7}$}&amp; \frac{-1}{7}&amp; \frac{-3}{7}&amp; 1
\end{array}\right]
\stackrel{M,S,P}{\longrightarrow}
\left[\begin{array}{rrr|rrr}
1&amp; 0&amp; 0&amp;  \frac{91}{133}&amp; \frac{7}{133}&amp; \frac{4}{19} \\
0&amp; 1&amp; 0&amp; \frac{-35}{133}&amp; \frac{28}{133}&amp; \frac{-3}{19} \\
0&amp; 0&amp; 1&amp; \frac{1}{19}&amp; \frac{3}{19}&amp; \frac{-7}{19}
\end{array}\right]$$ 

Thus, the required inverse is 
$$\left[\begin{array}{ccccccccccc} \frac{91}{133}&amp; \frac{7}{133}&amp; \frac{4}{19}
\\\frac{-35}{133}&amp; \frac{28}{133}&amp; \frac{-3}{19}
\\\frac{1}{19}&amp; \frac{3}{19}&amp; \frac{-7}{19}
\end{array}\right].$$ 
<img src="../image/box.png"></p>


<b>Why this works:</b> It is based on the simple fact from linear
algebra that if we form the matrix product $AB,$ then
the $j$-th column of $AB$ is actually $A\bb_j,$
where $\bb_j$ is the $j$-th column of $B.$
<p></p>
From this it follows that the row operations are effectively
multiplication by a matrix from the left. For example, 
<ul>

<li>multiplying a row by a  scalar is like this:
$$
\left[\begin{array}{ccccccccccc}1 &amp; 0 &amp; 0\\0 &amp; 2 &amp; 0\\0&amp; 0 &amp; 1
\end{array}\right] 
\left[\begin{array}{ccccccccccc}
b_{11} &amp; b_{12} &amp; b_{13} &amp; b{14}\\
b_{21} &amp; b_{22} &amp; b_{23} &amp; b{24}\\
b_{31} &amp; b_{32} &amp; b_{33} &amp; b{34}

\end{array}\right]
 = 
\left[\begin{array}{ccccccccccc}
b_{11} &amp; b_{12} &amp; b_{13} &amp; b{14}\\
2b_{21} &amp; 2b_{22} &amp; 2b_{23} &amp; 2b{24}\\
b_{31} &amp; b_{32} &amp; b_{33} &amp; b{34}

\end{array}\right].
$$
</li>

<li>Adding a multiple of a row to another is like:
$$
\left[\begin{array}{ccccccccccc}1 &amp; 0 &amp; 3\\0 &amp; 1 &amp; 0\\0&amp; 0 &amp; 1
\end{array}\right] 
\left[\begin{array}{ccccccccccc}
b_{11} &amp; b_{12} &amp; b_{13} &amp; b{14}\\
b_{21} &amp; b_{22} &amp; b_{23} &amp; b{24}\\
b_{31} &amp; b_{32} &amp; b_{33} &amp; b{34}

\end{array}\right]
 = 
\left[\begin{array}{ccccccccccc}
b_{11}+3b_{31} &amp; b_{12}+3b_{32} &amp; b_{13}+2b_{33} &amp; b{14}+2b_{34}\\
2b_{21} &amp; 2b_{22} &amp; 2b_{23} &amp; 2b{24}\\
b_{31} &amp; b_{32} &amp; b_{33} &amp; b{34}

\end{array}\right].
$$
</li>

</ul>
If a row operation is invertible (<i>i.e.</i>, it is possible for you to
recover the original matrix from the transformed matrix), then
the corresponding premultiplir matrix must be nonsingular. Both
the above operations are invertible (assuming the scalar
multiplier in the first is nonzero), and so the corresponding
premultiplier matrices are nonsingular. These premultipliers are
called <b>elementary matrices</b>. They may be obtained by
applying the row operations to the identity matrix. For example,
to obtain the premultiplier for "subtracting 5 times the 4th row
to the 2nd in a $5\times n$ matrix" start with
the $5\times5$ identity matrix
$$
\left[\begin{array}{ccccccccccc}
1&amp;0&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0&amp;0\\
0&amp;0&amp;1&amp;0&amp;0\\
0&amp;0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;1

\end{array}\right].
$$
Then apply the operation on this to get
$$
\left[\begin{array}{ccccccccccc}
1&amp;0&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;-5&amp;0\\
0&amp;0&amp;1&amp;0&amp;0\\
0&amp;0&amp;0&amp;1&amp;0\\
0&amp;0&amp;0&amp;0&amp;1

\end{array}\right].
$$
This is the reqired premultiplier (why?).
<p></p>

Thus, each step in the GJ algorithm is a left multiplication by a nonsingular
matrix. Suppose that we start with $(A~|~I)$ and end
with $(P~|~ B),$ where $P$ is a permutation
matrix. This means 
$$
(P~|~B) = N(A~|~I),
$$
where $N$ is nonsingular matrix representing the combined
effect of all the GJ steps. Then 
$$
P = NA \mbox{ and } B = N.
$$
Since $P$ is a permutation matrix, hence $P'P=I.$
So $P'NA = I.$ Thus, $A ^{-1}  = P'N = P'B.$ In other
words, you permute the <i>rows</i> of $B$ according
to $P'$ to get $A ^{-1}.$

<h3><a
name="Determinant using Gaussian elimination">Determinant using Gaussian elimination</a></h3>

<p>
<b>EXERCISE:</b>&nbsp;Find out how one may use the  Gauss-Jordan
elimination algorithm to compute the determinant. <img src="../image/box.png"></p>



<p>
<a name="gj"><b>
<div id="gj">PROJECT:</div>
</b></a>&nbsp;Write a program that will apply Gauss-Jordan elimination to an arbitrary matrix of order
 $m\times n$  over ${\mathbb R}.$ It should perform
 the row swaps only when the pivot is zero.  The resulting form is called the <b>reduced row echelon
 form (RREF)</b>  of the matrix. This form has the interesting property
 that any two matrices with the same size and same row space must
 have the same RREF. The program should also find  bases of the
 column space, row space and null space of the matrix using the
 computed  RREF.<img src="../image/box.png"></p>


<h2><a
name="$QR$ decomposition">$QR$ decomposition</a></h2>

<fieldset>
<legend><b><i>Theorem</i></b></legend>
For any $n$ by $p$ matrix $A$ with $n\geq p$ we have
an $n$ by $n$ orthogonal matrix $Q$ and an $n$ by
$p$ upper triangular matrix $R$ such that
$$A = QR.$$
</fieldset>

This decomposition is the matrix version of the Gram-Schmidt
Orthogonalization (GSO). <b>[If you do not know GSO, then skip to
the next heading: <a href="#Householder%20transformation">Householder 
transformation</a>.]</b>To see this we first consider the case where $A$
is full column rank (<i> <i>i.e.</i>,</i> the columns are independent.) Call the
columns 
$$
\ba_1,...,\ba_p.
$$
Apply GSO to get an orthonormal set of vectors 
$$
\bq_1,...,\bq_p
$$
given by
$$
\bq_i  =  unit(\ba_i-\sum_{j=1}^{i-1}(\bq_j'\ba_i) \bq_j)
$$
computed in the order $i=1,...,p.$ Here $unit(\bv)$ is defined as
$$
unit(\bv) = \bv/\|\bv\|,\quad \bv\neq \bz.
$$

Notice that $span\{\ba_1,...,\ba_i\}=span\{\bq_1,...,\bq_i\}.$ 
So we can write 
$$\ba_j = r_{1j} \bq_1 + r_{2j} \bq_2 + \cdots + r_{jj} \bq_i.$$
Indeed, we have 
$$r_{ij}  = \bq_i' \ba_j \mbox{ for } i \leq j.$$
Define the matrix $R$ using the $r_{ij}$'s, and
form $Q$ with  $\bq_i$'s as its columns.
<p></p>


If $A$ is not full column rank, then some
$(\ba_i-\sum_{j=1}^{i-1}(\bq_j'\ba_i)\bq_j)$ will be zero, hence we cannot apply
$unit$ to it. But then we can take $\bq_i$ equal to any unit
norm vector orthogonal to $\bq_1,...,\bq_{i-1}$ and set $r_{ii}=0.$ 
<p></p>
However, GSO is not the best way to compute $QR$ decomposition of a
matrix. This is because in the $unit$ steps you have to divide
by the norms which may be too small. Division by small numbers in a computer may lead to numerical instability. The standard
 way to implement it is
using Householder's transformation, that we discuss next.

<h2><a
name="Householder transformation">Householder transformation</a></h2>
If $A$ is any orthogonal matrix, then we now that $\|Ax\| = \|x\|.$
In other words an orthogonal matrix does not change shape or size of an object. It can
only rotate and reflect it. We want to ask if the reverse is true:
<blockquote> If 
$x\neq y$ are two vectors of same length, then does there
exist an orthogonal $A$ that takes $x$ to $y$ and vice
versa? That is, we are looking for an orthogonal $A$ (possibly
depending on $x,y$) such that
$$
Ax = y \mbox{ and } Ay=x?
$$</blockquote>
The answer is ''Yes.'' In fact, there may be many. Householder's transform
is one such:
$$
A = I-2uu', \mbox{ where } u = unit(x-y).
$$

<p>
<b>EXERCISE:</b>&nbsp;
Show that this $A$ is orthogonal and it sends $x$ to $y$
and vice versa.
<img src="../image/box.png"></p>


<p>
<b>EXAMPLE:</b>&nbsp;
In general you need $n^2$ scalar multiplications to multiply
an $n\times n$ matrix with an $n\times 1$ vector.
However, show that if the matrix is a Householder matrix then only
$2n+1$ scalar multiplications are needed.
<p></p>
<b>SOLUTION:</b>
$$
A\bv = (I-2\bu\bu')\bv = \bv-2\bu\bu'\bv.
$$
Now $\bu'\bv$ is a scalar that requires $n$
multiplications to compute. Multiply that with $2$ (one more
multiplication) to get the scalar $2\bu'\bv = \lambda,$
say. Finally, multiply each entry of $\bu$
with $\lambda $ (requiring $n$ more multiplications).
<img src="../image/box.png"></p>
Let's play with the idea using R:
<font color="red">
<pre>
unit = function(v) {v/sqrt(sum(v*v))}
hmult = function(u, x) {x - 2* sum(u*x)*u}
( x = unit(c(1,2,3)) )
( y = unit(c(4,2,5)) )
( u = unit(x-y) )
hmult(u,x)
</pre>
</font>


<p>
<b>EXERCISE:</b>&nbsp;
Show that in 2 dimensions Householder's transform is the only such
transform. Show that this uniqueness does not hold for higher dimensions.
<img src="../image/box.png"></p>



<h3><a
name="Using Householder for $QR$">Using Householder for $QR$</a></h3>
The idea is to shave the columns of $X$ one by one by multiplying
with Householder matrices. For any non zero vector $u$ define
$H_u$ as the Householder matrix that reduces $u$ to 
$$
v = \left[\begin{array}{ccccccccccc}\|u\|^2\\0\\\vdots\\0
\end{array}\right].
$$

<p>
<b>EXERCISE:</b>&nbsp;
Let us partition a vector $\bu$ as
$$
\bu_{n\times 1} = \left[\begin{array}{ccccccccccc}\bu_1\\\bu_2
\end{array}\right],
$$
where both $\bu_1,\bu_2$ are vectors ($\bu_2$ being 
$k\times 1).$ Consider the $n\times n$
matrix
$$
H = \left[\begin{array}{ccccccccccc}I &amp;  0\\0 &amp;  H_{\bu_2}
\end{array}\right].
$$
Show that $H$ is orthogonal and compute $Hu.$ We shall say that
$H$ shaves the last $k-1$ elements of $u.$
<img src="../image/box.png"></p>

Let the first column of $X$ be $a.$ Let $H_1$ shave its
lower $n-1$ entries.  Consider  the second column  $b$ of
$H_1A.$ Let $H_2$ shave off its lower $n-2$ entries.
Next let $c$ denote the third column of $H_2H_1X,$ and so on.
Proceeding in this way, we get $H_1,H_2,...,H_p$ all of which are
orthogonal Householder matrices. Define
$$
Q = (H_1H_2\cdots H_p)'
$$
and $R = Q'X$ to get a $QR$ decomposition.

<p>
<b>EXERCISE:</b>&nbsp;
Carry this out for the following $5\times 4$ case.
$$
\left[\begin{array}{ccccccccccc}
1&amp;  2 &amp;  3 &amp;  4\\
1&amp;  3 &amp;  2 &amp;  4\\
1&amp;  -2 &amp;  5 &amp;  0\\
7&amp;  2 &amp;  1 &amp;  3\\
-2&amp;  8 &amp;  5 &amp;  4

\end{array}\right]
$$
<img src="../image/box.png"></p>

We shall now apply it to a $4\times 3$ matrix. 

The steps are
shown in the diagram below (red is the entry computed in that
step, grey means already computed, black means 0):
<center>
<table width="100%">
<tr>
<th><img width="" src="image/qrst.png"></th>
</tr>
<tr>
<th>Steps in the computation of $R$ and
the $\bu$'s</th>
</tr>
</table>
</center>


<font color="red">
<pre>
shaver = function(x) {
   x[1] = x[1] - sqrt(sum(x*x))
   unit(x)
}
</pre>
</font>
Next, let's create a matrix:
<font color="red">
<pre>
H1 = sample(1000, 6)
H2 = sample(1000, 6)
H3 = sample(1000, 6)
H4 = sample(1000, 6)
H5 = sample(1000, 6)
</pre>
</font>
OK, ready for first pass:
<font color="red">
<pre>
(u=shaver(H1))
(H1 = hmult(u,H1)) #Just for checking!
(H2 = hmult(u,H2))
(H3 = hmult(u,H3))
(H4 = hmult(u,H4))
(H5 = hmult(u,H5))
</pre>
</font>
The second pass is similar:
<font color="red">
<pre>
(u=shaver(H2[2:6]))
(H2[2:6] = hmult(u,H2[2:6]))
(H3[2:6] = hmult(u,H3[2:6]))
(H4[2:6] = hmult(u,H4[2:6]))
(H5[2:6] = hmult(u,H5[2:6]))
</pre>
</font>
and so on.

<h3><a
name="Efficient implementation">Efficient implementation</a></h3>
Notice that though the Householder matrix 
$$
I - 2\bu\bu'
$$
is an $n\times n$ matrix, it is actually determined by only $n$
numbers. Thus, we can effectively store the matrix in linear space. In
particular, the matrix $H_1$ needs only $n$ spaces, $H_2$
needs only $n-1$ spaces and so on. 

So we shall try to store these in the ``shaved'' parts of $X.$
Let 
$H_1 = 1-2\bu_1\bu_1'$ and $H_1X$ be partitioned as
$$
\left[\begin{array}{ccccccccccc}\alpha &amp;  v'\\0 &amp;  X_1
\end{array}\right].
$$
Then we shall try to store  $\bu_1$ in place of the
0's. But $\bu_1$ is an $n\times 1$ vector, while we
have only $n-1$ zeroes. So the standard practice is to
store $\alpha$ (which is the squared norm  of the first
column) in a separate array, and store $\bu_1$ in place of
the first column of $A.$

The final output will be a $n\times p$ matrix and
a $p$-dimensional vector (which is like an extra row
in $A).$ The matrix is packed with the
$u$'s and the strictly upper triangular part of $R:$
<center>
<table width="100%">
<tr>
<th><img width="" src="image/qrpack.png"></th>
</tr>
<tr>
<th>Output of efficient $QR$
  decomposition</th>
</tr>
</table>
</center>
The ``extra row'' stores the diagonal entries of $R.$
It is possible to ``unpack'' $Q$ from the $u$'s. However, if we
need $Q$ only to multiply some $x$ to get $Qx,$ then even
this unpacking is not necessary. 

<p>
<b>EXERCISE:</b>&nbsp;
Write a program that performs the above multiplication without explicitly
computing $Q.$
<img src="../image/box.png"></p>

 


<h3><a
name="Application to least squares">Application to least squares</a></h3>
An important use of the $QR$ decomposition is in solving least squares problems. Here we are given a
(possibly inconsistent) system
$$
A\bx = \bb,
$$
where $A$ is full column rank (need not be square.) Then we have the
following theorem.

<fieldset>
<legend><b><i>Theorem</i></b></legend>
The above system has unique least square solution $\bx$ given by
$$
\bx = (A'A)^{-1}A'\bb.
$$
Note that the full column rankness of $A$ guarantees the existence
of the inverse.
</fieldset>

However, computing the inverse directly and then performing matrix
multiplication is not an efficient algorithm. A better way (which is used
in standard software packages) is to first form a $QR$ decomposition
of $A$ as
$A = QR.$
The given system now looks like
$$
QR\bx = \bb.
$$
The lower part of $R$ is made of zeros:
$$
Q\left[\begin{array}{ccccccccccc}R_1\\O
\end{array}\right]\bx = \bb,
$$
or
$$
\left[\begin{array}{ccccccccccc}R_1\\O
\end{array}\right]\bx = Q'\bb,
$$
Partition $Q'\bb$ appropriately to get
$$
\left[\begin{array}{ccccccccccc}R_1\\O
\end{array}\right]\bx = \left[\begin{array}{ccccccccccc}\bc_1\\\bc_2
\end{array}\right],
$$
where $\bc_1$ is $p\times 1.$ This system is made of two systems:
$$
R_1 \bx = \bc_1
$$
and 
$$
O \bx = \bc_2.
$$
The first system is always consistent and can be solved by
back-substitution. The second system is trivial and inconsistent unless
$\bc_2=\bz.$ 



<p>
<b>EXERCISE:</b>&nbsp;
Show that $x=R_1^{-1}\bc_1$ is the least square solution of the original
system. Notice that you use back-substituting to find this $\bx$ and not
direct inversion of $R_1.$ 
<img src="../image/box.png"></p>


<p>
<b>EXERCISE:</b>&nbsp;
Find a use of $\bc_2$ to compute the residual sum of squares:
$$
\|\bb-A\bx\|^2.
$$
<img src="../image/box.png"></p>


<p>
<a name="qr"><b>
<div id="qr">PROJECT:</div>
</b></a>&nbsp;
Write a program to find a least  squares solution to the
system $A\bx=\bb$. Your program should first implement the above efficient version of $QR$
decomposition of $A.$ Your program
  should be able to detect if $A$ is not full column rank,
  in which case it should stop with an error message. If $A$
is full column rank, then the program should output the unique
least squares solution. Your program must never compute any
Householder matrix explicitly.
<img src="../image/box.png"></p>


<h3><a
name="What if $A$ is not full column rank?">What if $A$ is not full column rank?</a></h3>
You'll detect this during computation of the $QR$
decomposition: 
<blockquote>If $A$ is not full column rank, then for some $k$,
the $k$-th column of $A$ will be in the span of the
preceding columns. At that point, the norm of $\bu$ will be
zero. 
</blockquote>
There are two things you can do if you detect such a
situation. 
<ul>

<li>One is to take $I$ in place of the Householder
matrix for that step. This is natural, because if the entries are
already zero, then there is no need to "shave" them further!
</li>

<li>However, if you aim is to compute an ONB of column space
of $A$, or to compute least squares solution, then you
should proceed differently. You should "throw away" those columns
of $A$ that are in the spans of the preceding columns.</li>

</ul>
The following example illustrates both these approaches. 

<p>
<b>EXAMPLE:</b>&nbsp;
Let's take 
$$
A=\left[\begin{array}{ccccccccccc}3 &amp; 6 &amp; 1\\4 &amp; 8 &amp; 3\\0&amp;0&amp;4
\end{array}\right].
$$
Clearly, $r(A) = 2.$ The first step of the $QR$
algorithm will go smoothly, converting $A$ to 
$$
\left[\begin{array}{ccccccccccc}
5 &amp; 10 &amp; *\\
0 &amp; 0 &amp; *\\
0 &amp; 0 &amp; *

\end{array}\right].
$$
In the second step we run into trouble. We are supposed to
"shave" the two entries under the 10. But they are already
zeroes. So the first approach will simply move on to the third
step, and finally produce a $3\times 3$ upper
triangular $R.$ This $R$ will not help you much to get
any least squares solution of a system $A\bx = \bb.$
<p></p>
In the second approach, we shall throw away the second column to
get:
$$
\left[\begin{array}{ccccccccccc}
5 &amp;  *\\
0 &amp;  *\\
0 &amp;  *

\end{array}\right].
$$
Then it will proceed to perform the second step again on this new
matrix, <i>i.e.</i>, the lowest $*$ will get shaved. The output of
this version of the $QR$ algorithm is an $R$ matrix of
size  $3\times 2$. The top $2\times 2$ portion is
a <i>nonsingular</i>  upper triangular matrix, which will allow
you to compute a least squares solution. 
<img src="../image/box.png"></p>

<h2><a
name="Eigenanalysis: power method">Eigenanalysis: power method</a></h2>
Let us start with the definition of eigenvalues and eigenvectors:

<fieldset>
<legend><b>Definition: Eigenvalue and eigenvector</b></legend>
Let $A_{n\times n}$ be any real/complex
matrix. Then $\lambda\in{\mathbb C}$ is called an <b>eigenvalue</b>
of $A$ with
corresponding <b>eigenvector</b> $\bv\in{\mathbb C}^n$
if $\bv\neq\bz$ and
$$
A \bv = \lambda \bv.
$$
</fieldset>
The importance of these concepts is by no means  obvious from the
definition. However,
finding the eigenvalues and eigenvectors of matrices is a fundamental
problem of numerical linear algebra. 

The subject in its entirety is well
beyond the scope of the present course. Here We shall try to find
only a single eigenvector and a corresponding eigenvalue of a
given square matrix.

<p></p>
Our algorithm is very simple:


We start with a vector $\bv$ and constructs the sequence 
$$
unit(A\bv), unit(A^2\bv), unit(A^3\bv), unit(A^4\bv),... 
$$
Here $unit(\bv)$ is the unit vector along a nonzero
vector $\bv$, <i>i.e.</i>, 
$$unit(\bv) = \bv/\|\bv\|.$$ 
Under some condition on $\bv,$ the sequence "converges" to an eigenvector.

<p></p>
We shall not explore the most general condition under which this
algorithm works. But here is the most commonly used sufficient
condition.


<fieldset>
<legend><b><i>Theorem</i></b></legend>Let $A_{n\times n}$ have all real
eigenvalues, $\lambda_1,...,\lambda_n$, with
$$
\lambda_1 &gt; |\lambda_2|\geq \cdot \geq |\lambda_n|.
$$
Let $\bv = \sum_i c_i \bv_i$, where $\bv_i$'s are
eigenvectors corresponding to $\lambda_i$'s, and $c_1\neq 0.$
<p></p>
Then the sequence
$$
unit(A\bv), unit(A^2\bv), unit(A^3\bv), unit(A^4\bv),... 
$$
converges to $\bv_1$ if $ c_1&gt;0$ and
to $-\bv_1$ if $ c_1 &lt; 0.$
</fieldset>

<p>
<b><i>Proof:</i></b>
Here 
$$
A^k \bv = \sum_i c_i \lambda_i^k \bv_i = \lambda_1^k \left(c_1 \bv_1 +
\sum_{i=2}^n c_i \left(\tfrac{\lambda_i}{\lambda_1}\right)^k \bv_i\right).
$$
So 
$$
unit(A^k \bv) = unit\left(c_1 \bv_1 +
\sum_{i=2}^n c_i \left(\tfrac{\lambda_i}{\lambda_1}\right)^k \bv_i\right).
$$
Now, if $(\bx_n)$ is a sequence of vectors with $\bx_n\rightarrow
\bx\neq \bz,$ then $unit(\bx_n)\rightarrow unit(\bx).$ 
<p></p>
Hence $unit(A^k\bv) \rightarrow unit(c_1 \bv_1) =
unit(\bv_1)$ if $c_1 &gt;0$ and $-unit(\bv)$
is $ c_1 &lt;0,$ as required.
<b><i>[QED]</i></b>
</p>
Let's take a computational example:
<font color="red">
<pre>
A  = matrix(runif(9),3,3)
(u = unit(1:3))
(u= unit(A%*% u))
</pre>
</font>
Just keep on running the last line repeatedly.




<h3>Comments</h3>
To post an anonymous comment, click on the "Name" field. This
will bring up an option saying "I'd rather post as a guest."
<p></p><!--
begin disqus code --> <div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://www.isical.ac.in/~arnabc/numana2021/mat1.html"; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "mat1"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://txtbk.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript><!-- end disqus code --> 

<hr/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/>
</body>
</html>
