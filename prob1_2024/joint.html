<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type"/>
<link rel="stylesheet" type="text/css" href="../tools/ctut.css"/>
<link type="text/css" rel="stylesheet" href="../tools/style.css"/>
<style type="text/css">@font-face {font-family: SHREE_BAN_OTF_0592;src: local("../tools/SHREE_BAN_OTF_0592"),url(../tools/SHREE0592.woff) format("opentype");</style>
<script src="../tools/jquery-1.10.2.min.js"></script>

<script>
aha = function(code) {
  window.open("https://rdrr.io/snippets/embed/?code="+code)
}

togglePhoto = function(photoId) {
   var me = document.getElementById("pic_"+photoId)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

hideShow = function(lb) {
   var me = document.getElementById(lb)
   if(me.style.display=="block"){
     me.style.display="none";
   }
   else if (me.style.display=="none"){
     me.style.display="block";
   }
}

grabData = function(data){
  return "https://farm"+data.photo.farm+".staticflickr.com/"+data.photo.server+"/"+data.photo.id+"_"+
            data.photo.secret+".jpg"
}

fromFlickr = function(photoId) {

$.getJSON("https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=23a138c73bdbe1e68601aa7866924e62&user_id=109924623@N07&photo_id="+photoId+"&lang=en-us&format=json&jsoncallback=?",
  function(data) {
    imgURL = grabData(data)
    var l = document.getElementById("lnk_"+photoId)
    l.href = "https://www.flickr.com/photos/109924623@N07/"+photoId
    var i = document.getElementById("pic_"+photoId)
    i.src=imgURL
    i.onload = function() {
      document.getElementById("status_"+photoId).innerHTML="[Image loaded. Click to show/hide.]"
    }
  })
}
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js","color.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });
</script><script type="text/javascript" src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="../tools/htmlwidgets.js"></script>
<link href="../tools/rgl.css" rel="stylesheet"></link>
<script src="../tools/rglClass.src.js"></script>
<script src="../tools/CanvasMatrix.src.js"></script>
<script src="../tools/rglWebGL.js"></script>
</head>
<body>
<a href="http://www.isical.ac.in/~arnabc/">[Home]</a>
<h3>Table of contents</h3>
<ul>
<li>
<a href="#Joint distribution">Joint distribution</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Marginal distributions">Marginal distributions</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Expectation">Expectation</a>
</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#Indicator trick">Indicator trick</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Independent random variables">Independent random variables</a>
</li>
<li>&nbsp;&nbsp;&nbsp;
<a href="#Covariance">Covariance</a>
</li>
<li>
<a href="#Problems for practice">Problems for practice</a>
</li>
</ul>
<hr/>

<h1><a
name="Joint distribution">Joint distribution</a></h1>


<fieldset>
<legend><b>Definition: Jointly distributed random variables</b></legend>
When we say that some random variables are <b>jointly
distributed</b>, we mean that they are all defined on the same
probability space. 
</fieldset>
If we want to combine values of different random variables (<i>e.g.</i>,
by addition, subtraction etc or comparison like $\leq$), then
they must be jointly distributed. If we have $n$ jointly
distributed real-valued random variables, then you may consider
them as components of an ${\mathbb R}^n$-valued random
variable. Sometimes we call such a random variable a <b>multivariate</b>
random variable, as opposed to a <b>univariate</b> one.

<p></p>
We shall now extend the various familiar concepts about ${\mathbb R}$-valued  random
variables to ${\mathbb R}^n$-valued random variables. 
<p></p>

<fieldset>
<legend><b>Definition: Joint CDF</b></legend>
Let $X = (X_1,...,X_n)$ be an ${\mathbb R}^n$-valued random
variable. Its joint CDF is defined as $F:{\mathbb R}^n\rightarrow{\mathbb R}$ where for
all $(x_1,...,x_n)\in{\mathbb R}^n$
$$
F(x_1,...,x_n) = P(X_1\leq x_1~\&amp;~\cdots~\&amp;~X_n\leq x_n).
$$
</fieldset>

The extension of the concept of discreteness is straightforward.

<fieldset>
<legend><b>Definition: Discrete</b></legend>
An ${\mathbb R}^n$-valued random variable $X$ is called <b>discrete</b>
if there is a countable set $A\subseteq{\mathbb R}^n$ such that $P(X\in A)=1.$
</fieldset>
The definition of continuous random variable is slightly more
confusing. For ${\mathbb R}$-valued random variables we had two
equivalent definitions:
<ul>

<li>ever singleton set has probability zero,</li>

<li>CDF is continuous.</li>

</ul>
For an ${\mathbb R}^n$-valued random variable, these two conditions
are not equivalent (the latter is stronger). We use the stronger
condition as the defintion of continuity of
an ${\mathbb R}^n$-valued random variable.

<table align="right" width="20%" border="1">
<tr>
<td bgcolor="pink">
<b>Caution:</b> Most books take a much stronger definition of
continuity for joint distribution. More precisely, that
definition should be called <b>absolute continuity</b>, which we
shall learn later.
</td>
</tr>
</table>

<fieldset>
<legend><b>Definition: Continuous</b></legend>
An ${\mathbb R}^n$-valued random variable $X$ is
called <b>continuous</b> if its joint CDF is continuous.
</fieldset>

The following example shows that the first condition is indeed
weaker than the second.

<p>
<b>EXAMPLE:</b>
Consider the function with the following graph:
<center>
<table width="100%">
<tr>
<th><img width="" src="image/unifcdf.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
Clearly it satisfies the 4 conditions of being a CDF. Hence we
know that there is a random variable $X$ with this CDF (by
the fundamental theorem).
<p></p>
Define a ${\mathbb R}^2$-valued random variable 
as $Y=(X,1).$  Show that for any $(a,b)\in{\mathbb R}^2$ we have $P(Y=(a,b))=0.$
Also show that the CDF of $Y$ is not continuous.
<p></p>
<b>SOLUTION:</b>
$P(Y=(a,b))= P(X=a~\&amp;~1=b)\leq P(X=a)=0,$ since $X$ is
a continuous random variable.
<p></p>
Also, the joint CDF is 
$$
F(a,b) = P(X\leq a~\&amp;~1\leq b) = \left\{\begin{array}{ll}0&\text{if }b &lt;
1\\F(a)&\text{if }b\geq 1.\\\end{array}\right.
$$
If we take $(a_n,b_n) =\left( \frac 12, 1-\frac 1n\right),$
then $(a_n,b_n)\rightarrow \left(\frac 12,1\right).$ 

<p></p>
Now $F(a_n,b_n)\equiv 0,$ and so $F(a_n,b_n)\rightarrow 0.$

<p></p>
But $F\left(\frac 12,1\right) = \frac 12\neq 0.$
<img src="../image/box.png"></p>


<fieldset>
<legend><b>Definition: Joint PMF</b></legend>
Let $X$ be an ${\mathbb R}^n$-valued discrete random
variable. Then its <b>joint PMF</b> is the
function $p:{\mathbb R}^n\rightarrow{\mathbb R}$ defined as 
$$
p(x_1,...,x_n)= P(X_1=x_1~\&amp;~\cdots~\&amp;~X_n=x_n).
$$
</fieldset>


<h2><a
name="Marginal distributions">Marginal distributions</a></h2>
If you are given two jointly distributed random
variables $X,Y$ and you know their joint distribution,
<i>i.e.</i> given any $A\subseteq{\mathbb R}^2$ you know $P((X,Y)\in A),$
then you can work out the probability distribution of $X$
and $Y$ separately from this, <i>i.e.</i>, for any
fiven $B\subseteq{\mathbb R}$ you can find $P(X\in B)$ and $P(Y\in
B)$ as follows:
<blockquote>
$P(X\in B) = P(X\in B~\&amp;~ Y\in{\mathbb R}) = P((X,Y)\in A),$
where $A = B\times{\mathbb R}.$ Similarly, for $Y.$
</blockquote>


<fieldset>
<legend><b>Definition: Marginal distribution</b></legend>
Let $X=(X_1,...,X_n)$ be an ${\mathbb R}^n$-valued
random variable. For any $\{i_1,...,i_k\}\subseteq\{1,2,...,n\}$ the joint
distribution of $(X_{i_1},...,X_{i_k})$ is called
a $k$-dimensional <b>marginal</b> for the joint distribution
of $X.$
</fieldset>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $(X,Y)$ be an ${\mathbb R}^2$-valued random variable with
joint CDF $F(x,y).$ Then the marginal CDF of $X$ is 
$$
F_X(x) = P(X\leq x) = \lim_{y\rightarrow \infty} F(x,y)
$$
and the marginal CDF of $X$ is 
$$
F_Y(y) = P(Y\leq y) = \lim_{x\rightarrow \infty} F(x,y).
$$
</fieldset>


<h2><a
name="Expectation">Expectation</a></h2>

The definition of expectation is  straightforward extension of
the univariate case. 

<fieldset>
<legend><b>Definition: Expectation</b></legend>
Let $X$ be an ${\mathbb R}^n$-valued discrete random
variable with PMF $p(x)$. Let $f:{\mathbb R}^n\rightarrow {\mathbb R}$ be any
function. Then $E(h(X))$ is defined as follows.
<ul>

<li>If $\sum_x |h(x)| p(x) &lt; \infty,$ then 
$$
E(h(X)) = \sum_x h(x) p(x).
$$
</li>

<li>If $\sum_x |h(x)| p(x) = \infty,$ then 
<ol>

<li>if all but finitely many terms in the sum are positive, we
define $E(h(X))=\infty.$</li>

<li>if all but finitely many terms in the sum are negative, we
define $E(h(X))=-\infty.$</li>

<li>if there are infinitely many positive and negative terms,
then $E(h(X))$ is undefined.</li>

</ol>

</li>

</ul>


</fieldset>

If $X$ is an ${\mathbb R}^n$-valued random variable,
and $h:{\mathbb R}^n\rightarrow {\mathbb R}^m$ is any function, 
then $E(h(X))$ is defined component by component, and is said
to exists finitely iff all the component expectations exist finitely.
<p></p>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X, Y$ are jointly distributed real-valued random
variables, each with finite expectation, then $X+Y$ also has
finite expectation
$$
E(X+Y) = E(X)+E(Y).
$$
</fieldset>

<p>
<b><i>Proof:</i></b>
In this course we shall prove this  only when $X,Y$ are both
discrete random
variables.
<p></p>
First, notice that $X+Y$ is again discrete. 
<blockquote>
<b>Because:</b>
If $X$ takes values in the countable
set $\{x_1,x_2,...\}$ and $Y$ take values in the
countable set $\{y_1,y_2,...\},$ then each possible value
of $X+Y$ must be of the form $x_i+y_j.$ There are only
countably many such values.
</blockquote>
Let $p_{ij} = P(X=x_i~\&amp;~ Y=y_j).$
<p></p>
Then $P(X=x_i) = \sum_j p_{ij}$ and $P(Y=y_j) = \sum_i p_{ij}.$
<p></p>
So $E(X) = \sum_i x_i P(X=x_i) =  \sum_i x_i \sum_j p_{ij},$
and $E(Y) = \sum_j y_j P(Y=y_j) =  \sum_j y_j \sum_i p_{ij} .$
<p></p>
By the given condition both these series converges absolutely,
and may be grouped and arranged in any way without changing the
sum. 
<p></p>
So $\sum_i\sum_j |x_i p_{ij}|&lt; \infty,$ and $\sum_j\sum_i |y_j p_{ij}|&lt; \infty.$
<p></p>
Now $|x_i+y_j|\leq |x_i|+|y_j|$ by triangle
inequality.

<p></p>
Hence $\sum_{i,j} |(x_i+y_j)p_{ij}| &lt;\infty$ and
so $E(X+Y)$ exists finitely. Also 
$$
E(X+Y) = \sum_{i,j} (x_i+y_j)p_{ij} = \sum_i\sum_j x_ip_{ij} +
\sum_j\sum_i y_jp_{ij}  = E(X)+E(Y),
$$
as required.
<b><i>[QED]</i></b>
</p>

This result leads to simple trick that we discuss next.

<h3><a
name="Indicator trick">Indicator trick</a></h3>
Suppose that you are to find expected number of something. For
example, $n$ letters are randomly put into $n$
addressed envelops, and you are to find $E(X),$
where $X$ is the number of correctly placed letters. 
would you count $X$ In any given situation like the 
following, you can find $X$ by first putting a check mark
for each correctly placed letter and then counting the total
number of check marks. 
<p></p>
Mathematically each ckec mark is an <b>indicator</b>. For
example, the indicator for the $i$-th letter is 
$$
I_i = \left\{\begin{array}{ll}1&\text{if }i\mbox{-th letter is placed correctly}\\0&\text{otherwise.}\end{array}\right..
$$
Counting the number of check marks amounts to
summing $I_i$'.s Thus, $X = \sum I_i.$ 
<p></p>
Notice that each $I_i$ is a random variable, and $E(X) = \sum E(I_i).$
<p></p>
Since each $I_i$ takes only the values $1$
and $0,$ hence $E(I_i) = P(I_i=1).$
<p></p>
Now $I_i=1$ means $i$-th letter has been placed
correctly. This is has probability $\frac{(n-1)!}{n!} = \frac 1n.$
<p></p>
So $E(X) = n\times \frac 1n = 1.$ 
<p></p>
It's a bit surprising that $E(X)$ does not depend on $n.$

<h2><a
name="Independent random variables">Independent random variables</a></h2>
An important special case of jointly distributed random variables
is that of independent random variables. To state the definition
we shall intriduce a new terminology: If $X:\Omega\rightarrow S$ is
a random variable, then by "an event in terms of $X$" we
shall mean $\{w\in\Omega~:~ X(w)\in A\}$ for some $A\in
S.$ Similarly, if $X:\Omega\rightarrow S$ and $Y:\Omega\rightarrow
T$ are jointly distributed random
variables, then "an event in terms of $X,Y$" means 
$\{w\in\Omega~:~ (X(w),Y(w))\in A\},$ where $A\subseteq S\times T.$

<fieldset>
<legend><b>Definition: Indepdendent random variables</b></legend>
Let $X_1,...,X_n$ be jointly distributed random variables.
We say that they are <b>independent</b> if for all  disjoint
subsets $A,B\subseteq\{1,...,n\}$ any event in terms
of $\{X_i~:~i\in A\}$ is independent of any event in terms
of $\{X_i~:~i\in B\}.$
</fieldset>


<p>
<b>EXAMPLE:</b>
If $X,Y,Z$ are independent random variables, then 
$$
P(X^2+Y^2 \leq 4~\&amp;~ Z\neq 5) = P(X^2+Y^2 \leq 4)P(Z\neq 5).
$$
<img src="../image/box.png"></p>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X_1,...,X_n$ are independent random variables, then any
function of some of the $X$'s is independent of any
function of the remaining $X$'s.
</fieldset>

<p>
<b><i>Proof:</i></b>
Split $\{1,...,n\}$ into two disjoint
subsets $\{i_1,...,i_k\}$ and $\{j_1,...,j_{n-k}\}.$
<p></p>

Let $Y = f(X_{i_1,...,i_k})$ and $Z =
g(X_{j_1,...,j_{n-k}}),$ where $f,g$ are any two
functions. 
<p></p>
Take any two sets $A,B.$ Then 
$$P(Y\in A~\&amp;~Z\in B) = 
P(f(X_{i_1,...,i_k})\in
A~\&amp;~g(X_{j_1,...,j_{n-k}})\in B) = 
P(f(X_{i_1,...,i_k})\in A)P(g(X_{j_1,...,j_{n-k}})\in B) = P(Y\in
A)P(Z\in B).
$$
<b><i>[QED]</i></b>
</p>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X,Y$ be jointly distributed discrete random variables, with
PMFs $p(x)$ and $q(x).$ If they are independent, then
their joint PMF is $h(x,y) = p(x)q(y).$
</fieldset>

<p>
<b><i>Proof:</i></b>Immediate from the definition of independence.<b><i>[QED]</i></b>
</p>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
Let $X,Y$ be jointly distributed random variables, with
CDFs $F(x)$ and $G(x).$ If they are independent, then
their joint CDF is $H(x,y) = F(x)G(y).$
</fieldset>

<p>
<b><i>Proof:</i></b>Immediate from the definition of independence.<b><i>[QED]</i></b>
</p>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X,Y$ are independent random variables with finite
expectations, then $E(XY) = E(X)E(Y).$
</fieldset>

<p>
<b><i>Proof:</i></b>
We shall prove this for the case where $X,Y$ are both
discrete (hence so is $(X,Y)$). 

<p></p>
Let $p(x,y), p_X(x)$ and $p_Y(y)$ be the joint and
marginal PMFs, respectively.
<p></p>
Then 
$$
E(XY) = \sum_{x,y} xy p(x,y) = \sum_{x,y} xy p_X(x)p_Y(y) =
\sum_x x p_X(x)\times \sum _y yp_Y(y) = E(X)E(Y).
$$
The grouping and rearranging were justified since the series were
absolutely convergent. 
<b><i>[QED]</i></b>
</p>


<h2><a
name="Covariance">Covariance</a></h2>

<fieldset>
<legend><b>Definition: Covariance</b></legend>
If $X,Y$ are jointly distributed random variables, then
their <b>covariance</b> is defined as
$$
cov(X,Y) = E[(X-E(X))(Y-E(Y))].
$$
</fieldset>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
$cov(X,Y) = E(XY)-E(X)E(Y).$
</fieldset>

<p>
<b><i>Proof:</i></b>
By direct algebraic expansion.
<b><i>[QED]</i></b>
</p>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X,Y$ are independent and  $E(X^2),
E(Y^2) &lt; \infty,$then $cov(X,Y)=0.$ The
converse is not true.
</fieldset>

<p>
<b><i>Proof:</i></b>
The first part follows immediately from the fact that $E(XY)=E(X)E(Y).$
<p></p>
A counter example for the second part is as follows.
<p></p>
$X$ takes values $-1,0,1$ with equal
probabilities. $Y = |X|.$ Direct computation
shows $E(X)=E(XY)=0$ and so $cov(X,Y)=0.$
<p></p>
But $P(X=0~\&amp;~Y=1) = 0 \neq P(X=0)P(Y=1).$
<b><i>[QED]</i></b>
</p>

The $cov(\cdot,\cdot)$ function behaves much like ordinary
multiplication. The following theorems show this.

<fieldset>
<legend><b><i>Theorem</i></b></legend>
$cov(X,Y)=cov(Y,X).$
</fieldset>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
$cov(\sum a_i X_i, \sum b_j Y_j) = \sum_{i,j} a_ib_jcov(X_i,Y_j).$
</fieldset>

Also we have 
<fieldset>
<legend><b><i>Theorem</i></b></legend>
$cov(X,X) = V(X).$
</fieldset>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
$cov(aX+b,cY+d) = ac cov(X,Y).$
</fieldset>



<p>
<b>EXAMPLE:</b>
The analog of $(a+b)^2 = a^2+2ab+b^2$ here is $V(X+Y) =
V(X)+2 cov(X,Y) +V(Y).$ This also shows that if $X,Y$ are
independent, then $V(X+Y) = V(X)+V(Y).$
<img src="../image/box.png"></p>


<fieldset>
<legend><b><i>Theorem</i></b></legend>
If $X$ or $Y$ is a degenerate random variable, then $cov(X,Y)=0.$
</fieldset>


<fieldset>
<legend><b><i>Cauchy-Scwartz inequality</i></b></legend>
$cov(X,Y)^2 \leq V(X)V(Y).$
Equality holds iff $\exists a,b,c\in{\mathbb R}~~P(aX+bY=c)=1.$
</fieldset>

<p>
<b><i>Proof:</i></b>
The result is obvious if $X$ is degenerate. So let's
consider the case where $X$ is not degenerate. Then $V(X)&gt;0.$
<p></p>
Define $Z = Y-\underbrace{\frac{cov(X,Y)}{V(X)}}_\beta  X.$ 
<p></p>
We know that $V(Z)\geq 0.$
<p></p>
Now, 
$$
V(Z) = V(Y) + V(\beta X) - 2cov(Y,\beta X) = V(Y) + \beta^2 V(X)
- 2 \beta cov(X,Y).
$$
Since $\beta = \frac{cov(X,Y)}{V(X)},$ this reduces to 
$$
V(Y) - \frac{cov(X,Y)^2}{V(X)}.
$$
Since this is $\geq0,$ the inequality follows immediately.
<p></p>
Also equality holds iff $V(Z)=0$, <i>i.e.</i>, $Z$ is degenerate.
<p></p>
So we have $V(X) X - cov(X,Y) Y = kV(X)$ for some $k\in{\mathbb R}.$

<p></p>
This completes the proof.
<b><i>[QED]</i></b>
</p>


<fieldset>
<legend><b>Definition: Correlation</b></legend>
If $X,Y$ are jointly distributed random variables
with $V(X), V(Y)&gt;0,$ then their <b>correlation</b> is defined
as 
$$
\rho(X,Y)= \frac{ cov(X,Y) }{ \sqrt{V(X)V(Y)} }.
$$
</fieldset>
By  Cauchy-Scwartz inequality, $rho(X,Y) \in [-1,1].$ Also,
$\rho(X,Y)=-1$ or $\rho(X,Y)=1$ if and only
if $X,Y$ are linearly linearly related with probability 1,
<i>i.e.</i>, $\exists a,b,c\in{\mathbb R}$ such that $P(aX+bY=c)=1.$
<h1><a
name="Problems for practice">Problems for practice</a></h1>

<ol>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/most45.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt1.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt2.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt3.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt4.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt5.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt6.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt7.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt8.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt9.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt10.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt11.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt12.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt13.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt14.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt15.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt16.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt17.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>

<li>
<center>
<table width="100%">
<tr>
<th><img width="" src="image/jt18.png"></th>
</tr>
<tr>
<th></th>
</tr>
</table>
</center>
</li>


</ol>

<h3>Comments</h3>
To post an anonymous comment, click on the "Name" field. This
will bring up an option saying "I'd rather post as a guest."
<p></p><!--
begin disqus code --> <div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "http://www.isical.ac.in/~arnabc/prob1/joint.html"; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "joint"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://txtbk.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript><!-- end disqus code --> 
<hr/>
<table width="100%" border="0">
<tr>
<td align="left"/>
<td align="right"/>
</tr>
</table>
<hr/>
</body>
</html>
